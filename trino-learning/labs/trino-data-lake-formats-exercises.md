# Trino æ•°æ®æ¹–è¡¨æ ¼å¼å®æˆ˜å®éªŒ

## ğŸ¯ å®éªŒæ¦‚è¿°

è¿™å¥—æ•°æ®æ¹–è¡¨æ ¼å¼å®éªŒä¸“é—¨é…åˆã€ŠTrino æ•°æ®æ¹–è¡¨æ ¼å¼æ·±åº¦å‰–æã€‹æ–‡æ¡£ï¼Œé€šè¿‡å®é™…æ“ä½œApache Icebergã€Delta Lakeç­‰ç°ä»£è¡¨æ ¼å¼ï¼Œå¸®åŠ©ä½ æŒæ¡æ¹–ä»“ä¸€ä½“åŒ–æ¶æ„çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

**å‰ç½®æ¡ä»¶**: 
- å®ŒæˆTrinoåŸºç¡€å­¦ä¹ 
- ç†Ÿæ‚‰Parquet/ORCç­‰åˆ—å¼å­˜å‚¨æ ¼å¼
- æœ‰æƒé™è®¿é—®S3æˆ–HDFSå­˜å‚¨ç³»ç»Ÿ
- åŸºæœ¬çš„Sparkç¯å¢ƒ (éƒ¨åˆ†å®éªŒéœ€è¦)

---

## ğŸ“š å®éªŒç›®å½•

1. [å®éªŒ1: Icebergè¡¨åˆ›å»ºä¸åŸºç¡€æ“ä½œ](#å®éªŒ1-icebergè¡¨åˆ›å»ºä¸åŸºç¡€æ“ä½œ)
2. [å®éªŒ2: Delta Lakeäº‹åŠ¡æœºåˆ¶éªŒè¯](#å®éªŒ2-delta-lakeäº‹åŠ¡æœºåˆ¶éªŒè¯)
3. [å®éªŒ3: è¡¨æ ¼å¼æ€§èƒ½å¯¹æ¯”æµ‹è¯•](#å®éªŒ3-è¡¨æ ¼å¼æ€§èƒ½å¯¹æ¯”æµ‹è¯•)
4. [å®éªŒ4: æ—¶é—´æ—…è¡ŒåŠŸèƒ½å®æˆ˜](#å®éªŒ4-æ—¶é—´æ—…è¡ŒåŠŸèƒ½å®æˆ˜)
5. [å®éªŒ5: æ¨¡å¼æ¼”è¿›ä¸å…¼å®¹æ€§](#å®éªŒ5-æ¨¡å¼æ¼”è¿›ä¸å…¼å®¹æ€§)
6. [å®éªŒ6: ACIDäº‹åŠ¡ä¸å¹¶å‘æ§åˆ¶](#å®éªŒ6-acidäº‹åŠ¡ä¸å¹¶å‘æ§åˆ¶)
7. [å®éªŒ7: è¡¨ç»´æŠ¤ä¸ä¼˜åŒ–ç­–ç•¥](#å®éªŒ7-è¡¨ç»´æŠ¤ä¸ä¼˜åŒ–ç­–ç•¥)
8. [å®éªŒ8: æ¹–ä»“ä¸€ä½“åŒ–æ¶æ„è®¾è®¡](#å®éªŒ8-æ¹–ä»“ä¸€ä½“åŒ–æ¶æ„è®¾è®¡)

---

## å®éªŒ1: Icebergè¡¨åˆ›å»ºä¸åŸºç¡€æ“ä½œ

### ğŸ¯ å­¦ä¹ ç›®æ ‡
- ç†è§£Icebergè¡¨çš„åˆ›å»ºå’Œé…ç½®è¿‡ç¨‹
- æŒæ¡Icebergè¡¨çš„å…ƒæ•°æ®ç»“æ„
- éªŒè¯Trinoä¸Icebergçš„é›†æˆ
- åˆ†æIcebergçš„å­˜å‚¨å¸ƒå±€

### ğŸ”¬ å®éªŒæ­¥éª¤

#### æ­¥éª¤1: é…ç½®Icebergè¿æ¥å™¨
```properties
# config/catalog/iceberg.properties
connector.name=iceberg
iceberg.catalog.type=hive_metastore
hive.metastore.uri=thrift://localhost:9083

# S3å­˜å‚¨é…ç½®
iceberg.catalog.warehouse=s3://trino-iceberg-test/warehouse/
hive.s3.aws-access-key=YOUR_ACCESS_KEY
hive.s3.aws-secret-key=YOUR_SECRET_KEY
hive.s3.region=us-west-2

# Icebergç‰¹æ€§é…ç½®
iceberg.delete-as-join-rewrite-enabled=true
iceberg.target-max-file-size=512MB
iceberg.split-size=128MB
iceberg.file-format=PARQUET
iceberg.compression-codec=ZSTD
```

#### æ­¥éª¤2: åˆ›å»ºIcebergè¡¨å¹¶åˆ†æå…ƒæ•°æ®
```sql
-- åˆ›å»ºç¬¬ä¸€ä¸ªIcebergè¡¨
CREATE TABLE iceberg.test_schema.orders_iceberg (
    orderkey BIGINT,
    custkey BIGINT,
    orderstatus VARCHAR(1),
    totalprice DOUBLE,
    orderdate DATE,
    orderpriority VARCHAR(15),
    clerk VARCHAR(15),
    shippriority INTEGER,
    comment VARCHAR(79)
)
WITH (
    partitioning = ARRAY['orderdate'],
    format = 'PARQUET',
    format_version = 2
);

-- æ’å…¥æµ‹è¯•æ•°æ®
INSERT INTO iceberg.test_schema.orders_iceberg
SELECT 
    orderkey, custkey, orderstatus, totalprice, orderdate,
    orderpriority, clerk, shippriority, comment
FROM hive.tpch_sf1.orders
WHERE orderdate >= DATE '1995-01-01';

-- æŸ¥çœ‹è¡¨å±æ€§å’Œå…ƒæ•°æ®
SHOW CREATE TABLE iceberg.test_schema.orders_iceberg;

-- æŸ¥çœ‹Icebergç‰¹æœ‰çš„ç³»ç»Ÿè¡¨
SELECT * FROM iceberg.test_schema."orders_iceberg$snapshots" LIMIT 5;
SELECT * FROM iceberg.test_schema."orders_iceberg$manifests" LIMIT 5;
SELECT * FROM iceberg.test_schema."orders_iceberg$files" LIMIT 10;
```

#### æ­¥éª¤3: åˆ†æIcebergå­˜å‚¨ç»“æ„
```bash
# æ£€æŸ¥S3ä¸Šçš„Icebergè¡¨ç»“æ„
aws s3 ls s3://trino-iceberg-test/warehouse/test_schema/orders_iceberg/ --recursive

# é¢„æœŸçœ‹åˆ°ç±»ä¼¼ç»“æ„:
# metadata/
#   v1.metadata.json
#   v2.metadata.json  
#   snap-123456789.avro
# data/
#   orderdate=1995-01-01/
#     data-001.parquet
#     data-002.parquet
```

#### æ­¥éª¤4: å…ƒæ•°æ®æ–‡ä»¶å†…å®¹åˆ†æ
```python
#!/usr/bin/env python3
import boto3
import json
import avro.schema
import avro.io
import io

def analyze_iceberg_metadata(bucket_name, table_path):
    """åˆ†æIcebergè¡¨çš„å…ƒæ•°æ®æ–‡ä»¶"""
    s3 = boto3.client('s3')
    
    # æŸ¥æ‰¾æœ€æ–°çš„å…ƒæ•°æ®æ–‡ä»¶
    metadata_prefix = f"{table_path}/metadata/"
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=metadata_prefix)
    
    if 'Contents' not in response:
        print("No metadata files found")
        return
    
    # æ‰¾åˆ°æœ€æ–°çš„metadata.jsonæ–‡ä»¶
    metadata_files = [obj for obj in response['Contents'] if obj['Key'].endswith('.metadata.json')]
    latest_metadata = max(metadata_files, key=lambda x: x['LastModified'])
    
    print(f"Analyzing metadata file: {latest_metadata['Key']}")
    
    # è¯»å–å…ƒæ•°æ®å†…å®¹
    obj = s3.get_object(Bucket=bucket_name, Key=latest_metadata['Key'])
    metadata_content = obj['Body'].read().decode('utf-8')
    metadata = json.loads(metadata_content)
    
    # åˆ†æå…ƒæ•°æ®ç»“æ„
    print("\n=== Table Metadata Analysis ===")
    print(f"Format version: {metadata.get('format-version')}")
    print(f"Table UUID: {metadata.get('table-uuid')}")
    print(f"Current snapshot ID: {metadata.get('current-snapshot-id')}")
    print(f"Schema ID: {metadata.get('current-schema-id')}")
    print(f"Partition spec ID: {metadata.get('default-spec-id')}")
    
    # åˆ†ææ¨¡å¼ä¿¡æ¯
    schema = metadata.get('schemas', [{}])[0]
    print(f"\nSchema fields count: {len(schema.get('fields', []))}")
    for field in schema.get('fields', [])[:5]:  # æ˜¾ç¤ºå‰5ä¸ªå­—æ®µ
        print(f"  {field.get('name')}: {field.get('type')} (ID: {field.get('id')})")
    
    # åˆ†æåˆ†åŒºè§„æ ¼
    specs = metadata.get('partition-specs', [])
    if specs:
        current_spec = specs[-1]  # æœ€æ–°åˆ†åŒºè§„æ ¼
        print(f"\nPartition spec (ID: {current_spec.get('spec-id')}):")
        for field in current_spec.get('fields', []):
            print(f"  {field}")
    
    # åˆ†æå¿«ç…§ä¿¡æ¯
    snapshots = metadata.get('snapshots', [])
    print(f"\nSnapshot count: {len(snapshots)}")
    for snapshot in snapshots[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3ä¸ªå¿«ç…§
        print(f"  Snapshot {snapshot.get('snapshot-id')}:")
        print(f"    Timestamp: {snapshot.get('timestamp-ms')}")
        print(f"    Operation: {snapshot.get('operation')}")
        print(f"    Summary: {snapshot.get('summary', {})}")

if __name__ == "__main__":
    analyze_iceberg_metadata("trino-iceberg-test", "warehouse/test_schema/orders_iceberg")
```

### ğŸ“Š Icebergå…ƒæ•°æ®åˆ†æç»“æœ

| å…ƒæ•°æ®ç»„ä»¶ | æ•°é‡ | å¤§å° | å¤‡æ³¨ |
|-----------|------|------|------|
| å…ƒæ•°æ®æ–‡ä»¶ |  |  |  |
| å¿«ç…§æ•°é‡ |  |  |  |
| Manifestæ–‡ä»¶ |  |  |  |
| æ•°æ®æ–‡ä»¶ |  |  |  |

---

## å®éªŒ2: Delta Lakeäº‹åŠ¡æœºåˆ¶éªŒè¯

### ğŸ¯ å­¦ä¹ ç›®æ ‡
- éªŒè¯Delta Lakeçš„ACIDäº‹åŠ¡ç‰¹æ€§
- ç†è§£äº‹åŠ¡æ—¥å¿—çš„å·¥ä½œæœºåˆ¶
- æµ‹è¯•å¹¶å‘å†™å…¥çš„å†²çªæ£€æµ‹
- åˆ†æDeltaè¡¨çš„ç‰ˆæœ¬ç®¡ç†

### ğŸ”¬ å®éªŒæ­¥éª¤

#### æ­¥éª¤1: åˆ›å»ºDeltaè¡¨
```sql
-- åœ¨Trinoä¸­åˆ›å»ºDeltaè¡¨ (éœ€è¦Deltaè¿æ¥å™¨)
CREATE TABLE delta.test_schema.customer_delta (
    custkey BIGINT,
    name VARCHAR(25),
    address VARCHAR(40),
    nationkey BIGINT,
    phone VARCHAR(15),
    acctbal DOUBLE,
    mktsegment VARCHAR(10),
    comment VARCHAR(117),
    last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
)
WITH (
    location = 's3://trino-delta-test/tables/customer_delta/',
    checkpoint_interval = 10
);

-- æ’å…¥åˆå§‹æ•°æ®
INSERT INTO delta.test_schema.customer_delta
SELECT 
    custkey, name, address, nationkey, phone, acctbal, mktsegment, comment,
    CURRENT_TIMESTAMP as last_updated
FROM hive.tpch_sf1.customer
WHERE custkey <= 50000;
```

#### æ­¥éª¤2: åˆ†æDeltaäº‹åŠ¡æ—¥å¿—
```python
#!/usr/bin/env python3
import boto3
import json
import pandas as pd
from datetime import datetime

def analyze_delta_log(bucket_name, table_path):
    """åˆ†æDelta Lakeçš„äº‹åŠ¡æ—¥å¿—"""
    s3 = boto3.client('s3')
    
    # åˆ—å‡º_delta_logç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    log_prefix = f"{table_path}/_delta_log/"
    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=log_prefix)
    
    if 'Contents' not in response:
        print("No delta log files found")
        return
    
    print("=== Delta Log Analysis ===")
    
    log_files = []
    checkpoint_files = []
    
    for obj in response['Contents']:
        key = obj['Key']
        filename = key.split('/')[-1]
        
        if filename.endswith('.json'):
            # äº‹åŠ¡æ—¥å¿—æ–‡ä»¶
            version = int(filename.split('.')[0])
            log_files.append({
                'version': version,
                'key': key,
                'size': obj['Size'],
                'modified': obj['LastModified']
            })
        elif filename.endswith('.checkpoint.parquet'):
            # æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_files.append({
                'version': int(filename.split('.')[0]),
                'key': key,
                'size': obj['Size']
            })
    
    print(f"Transaction log files: {len(log_files)}")
    print(f"Checkpoint files: {len(checkpoint_files)}")
    
    # åˆ†ææœ€è¿‘çš„å‡ ä¸ªäº‹åŠ¡
    recent_logs = sorted(log_files, key=lambda x: x['version'])[-5:]
    
    print("\n=== Recent Transactions ===")
    for log_file in recent_logs:
        print(f"\nVersion {log_file['version']}:")
        print(f"  Size: {log_file['size']} bytes")
        print(f"  Modified: {log_file['modified']}")
        
        # è¯»å–äº‹åŠ¡æ—¥å¿—å†…å®¹
        try:
            obj = s3.get_object(Bucket=bucket_name, Key=log_file['key'])
            log_content = obj['Body'].read().decode('utf-8')
            
            # åˆ†ææ¯è¡Œæ“ä½œ
            operations = []
            for line in log_content.strip().split('\n'):
                if line.strip():
                    action = json.loads(line)
                    operations.append(action)
            
            print(f"  Operations count: {len(operations)}")
            
            # ç»Ÿè®¡æ“ä½œç±»å‹
            op_types = {}
            for op in operations:
                for key in op.keys():
                    if key not in ['commitInfo']:  # è·³è¿‡commitInfo
                        op_types[key] = op_types.get(key, 0) + 1
            
            print(f"  Operation types: {op_types}")
            
            # åˆ†æcommitInfo
            commit_info = next((op['commitInfo'] for op in operations if 'commitInfo' in op), None)
            if commit_info:
                print(f"  Operation: {commit_info.get('operation', 'Unknown')}")
                print(f"  User: {commit_info.get('userName', 'Unknown')}")
                print(f"  Timestamp: {commit_info.get('timestamp')}")
                
        except Exception as e:
            print(f"  Error reading log content: {e}")

if __name__ == "__main__":
    analyze_delta_log("trino-delta-test", "tables/customer_delta")
```

#### æ­¥éª¤3: äº‹åŠ¡å†²çªæµ‹è¯•
```sql
-- æ¨¡æ‹Ÿå¹¶å‘äº‹åŠ¡åœºæ™¯

-- ä¼šè¯1: æ›´æ–°é«˜ä»·å€¼å®¢æˆ·
UPDATE delta.test_schema.customer_delta 
SET mktsegment = 'VIP', last_updated = CURRENT_TIMESTAMP
WHERE acctbal > 8000;

-- åŒæ—¶åœ¨ä¼šè¯2: åˆ é™¤ä½æ´»è·ƒå®¢æˆ·
DELETE FROM delta.test_schema.customer_delta 
WHERE acctbal < 1000;

-- ä¼šè¯3: æ’å…¥æ–°å®¢æˆ·
INSERT INTO delta.test_schema.customer_delta VALUES
(150001, 'New Customer 1', 'New Address 1', 1, '555-0001', 5000.0, 'BUILDING', 'Test customer', CURRENT_TIMESTAMP),
(150002, 'New Customer 2', 'New Address 2', 2, '555-0002', 7500.0, 'AUTOMOBILE', 'Test customer', CURRENT_TIMESTAMP);

-- ğŸ¯ è§‚å¯Ÿå“ªäº›æ“ä½œæˆåŠŸï¼Œå“ªäº›å› ä¸ºå†²çªå¤±è´¥
-- ğŸ¯ åˆ†æDelta Lakeå¦‚ä½•å¤„ç†è¿™äº›å¹¶å‘æ“ä½œ
```

### ğŸ“Š äº‹åŠ¡æµ‹è¯•ç»“æœè®°å½•

| æ“ä½œç±»å‹ | æ‰§è¡Œæ—¶é—´ | æˆåŠŸ/å¤±è´¥ | å†²çªåŸå›  | ç‰ˆæœ¬å· | å¤‡æ³¨ |
|---------|----------|----------|----------|--------|------|
| UPDATEé«˜ä»·å€¼å®¢æˆ· |  |  |  |  |  |
| DELETEä½æ´»è·ƒå®¢æˆ· |  |  |  |  |  |
| INSERTæ–°å®¢æˆ· |  |  |  |  |  |

---

## å®éªŒ3: è¡¨æ ¼å¼æ€§èƒ½å¯¹æ¯”æµ‹è¯•

### ğŸ¯ å­¦ä¹ ç›®æ ‡
- å¯¹æ¯”Icebergã€Deltaã€Hiveè¡¨çš„æŸ¥è¯¢æ€§èƒ½
- ç†è§£ä¸åŒè¡¨æ ¼å¼çš„ä¼˜åŒ–ç‰¹å¾
- åˆ†æå­˜å‚¨å¼€é”€å’Œå‹ç¼©æ•ˆæœ
- éªŒè¯æŸ¥è¯¢ä¸‹æ¨çš„æ•ˆæœå·®å¼‚

### ğŸ”¬ å®éªŒæ­¥éª¤

#### æ­¥éª¤1: åˆ›å»ºç›¸åŒç»“æ„çš„ä¸åŒæ ¼å¼è¡¨
```sql
-- Hiveè¡¨ (ä¼ ç»Ÿæ ¼å¼)
CREATE TABLE hive.test_schema.lineitem_hive (
    orderkey BIGINT,
    partkey BIGINT,
    suppkey BIGINT,
    linenumber INTEGER,
    quantity DOUBLE,
    extendedprice DOUBLE,
    discount DOUBLE,
    tax DOUBLE,
    returnflag VARCHAR(1),
    linestatus VARCHAR(1),
    shipdate DATE,
    commitdate DATE,
    receiptdate DATE,
    shipinstruct VARCHAR(25),
    shipmode VARCHAR(10),
    comment VARCHAR(44)
)
WITH (
    partitioned_by = ARRAY['shipdate'],
    format = 'PARQUET'
);

-- Icebergè¡¨
CREATE TABLE iceberg.test_schema.lineitem_iceberg (
    orderkey BIGINT,
    partkey BIGINT, 
    suppkey BIGINT,
    linenumber INTEGER,
    quantity DOUBLE,
    extendedprice DOUBLE,
    discount DOUBLE,
    tax DOUBLE,
    returnflag VARCHAR(1),
    linestatus VARCHAR(1),
    shipdate DATE,
    commitdate DATE,
    receiptdate DATE,
    shipinstruct VARCHAR(25),
    shipmode VARCHAR(10),
    comment VARCHAR(44)
)
WITH (
    partitioning = ARRAY['bucket(orderkey, 16)', 'day(shipdate)'],
    format = 'PARQUET'
);

-- Deltaè¡¨ (å¦‚æœæœ‰Deltaè¿æ¥å™¨)
CREATE TABLE delta.test_schema.lineitem_delta (
    -- ç›¸åŒçš„åˆ—å®šä¹‰
)
WITH (
    location = 's3://trino-delta-test/tables/lineitem_delta/',
    partitioned_by = ARRAY['shipdate']
);
```

#### æ­¥éª¤2: æ’å…¥ç›¸åŒçš„æµ‹è¯•æ•°æ®
```sql
-- ä¸ºæ¯ä¸ªè¡¨æ ¼å¼æ’å…¥ç›¸åŒçš„æ•°æ®
INSERT INTO hive.test_schema.lineitem_hive
SELECT * FROM hive.tpch_sf10.lineitem WHERE shipdate >= DATE '1995-01-01';

INSERT INTO iceberg.test_schema.lineitem_iceberg  
SELECT * FROM hive.tpch_sf10.lineitem WHERE shipdate >= DATE '1995-01-01';

INSERT INTO delta.test_schema.lineitem_delta
SELECT * FROM hive.tpch_sf10.lineitem WHERE shipdate >= DATE '1995-01-01';

-- æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
ANALYZE TABLE hive.test_schema.lineitem_hive;
ANALYZE TABLE iceberg.test_schema.lineitem_iceberg;
ANALYZE TABLE delta.test_schema.lineitem_delta;
```

#### æ­¥éª¤3: æ€§èƒ½åŸºå‡†æµ‹è¯•
```sql
-- æµ‹è¯•æŸ¥è¯¢1: ç®€å•è¿‡æ»¤å’Œèšåˆ
-- Hiveè¡¨
EXPLAIN (TYPE DISTRIBUTED, FORMAT JSON)
SELECT 
    returnflag,
    linestatus,
    COUNT(*) as line_count,
    SUM(quantity) as total_qty,
    SUM(extendedprice) as total_price,
    AVG(extendedprice) as avg_price
FROM hive.test_schema.lineitem_hive
WHERE shipdate >= DATE '1996-01-01' AND shipdate < DATE '1996-04-01'
GROUP BY returnflag, linestatus
ORDER BY returnflag, linestatus;

-- Icebergè¡¨ (ç›¸åŒæŸ¥è¯¢)
EXPLAIN (TYPE DISTRIBUTED, FORMAT JSON)
SELECT 
    returnflag, linestatus, COUNT(*) as line_count,
    SUM(quantity) as total_qty, SUM(extendedprice) as total_price, AVG(extendedprice) as avg_price
FROM iceberg.test_schema.lineitem_iceberg
WHERE shipdate >= DATE '1996-01-01' AND shipdate < DATE '1996-04-01'
GROUP BY returnflag, linestatus
ORDER BY returnflag, linestatus;

-- ğŸ¯ å¯¹æ¯”æ‰§è¡Œè®¡åˆ’çš„å·®å¼‚:
-- - æ‰«æçš„æ–‡ä»¶æ•°é‡
-- - åˆ†åŒºè£å‰ªæ•ˆæœ
-- - ç»Ÿè®¡ä¿¡æ¯è´¨é‡
-- - æŸ¥è¯¢ä¸‹æ¨ç¨‹åº¦
```

#### æ­¥éª¤4: è‡ªåŠ¨åŒ–æ€§èƒ½æµ‹è¯•è„šæœ¬
```python
#!/usr/bin/env python3
import trino
import time
import statistics
from dataclasses import dataclass
from typing import List

@dataclass 
class BenchmarkResult:
    table_format: str
    query_name: str
    execution_time: float
    rows_scanned: int
    rows_returned: int
    peak_memory_mb: float
    files_scanned: int

class TableFormatBenchmark:
    
    def __init__(self):
        self.conn = trino.dbapi.connect(
            host='localhost',
            port=8080,
            user='benchmark'
        )
        
    def benchmark_query(self, query: str, table_format: str, query_name: str) -> BenchmarkResult:
        """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢åŸºå‡†æµ‹è¯•"""
        cursor = self.conn.cursor()
        
        start_time = time.time()
        cursor.execute(query)
        result = cursor.fetchall()
        execution_time = time.time() - start_time
        
        # è·å–æŸ¥è¯¢ç»Ÿè®¡
        stats_query = f"""
        SELECT 
            total_rows,
            processed_rows, 
            peak_user_memory_bytes,
            total_bytes
        FROM system.runtime.queries
        WHERE query_id = '{cursor.query_id}'
        """
        cursor.execute(stats_query)
        stats = cursor.fetchone()
        
        cursor.close()
        
        return BenchmarkResult(
            table_format=table_format,
            query_name=query_name,
            execution_time=execution_time,
            rows_scanned=stats[0] if stats else 0,
            rows_returned=len(result),
            peak_memory_mb=(stats[2] / 1024 / 1024) if stats else 0,
            files_scanned=0  # éœ€è¦ä»EXPLAINç»“æœä¸­è§£æ
        )
    
    def run_comprehensive_benchmark(self) -> List[BenchmarkResult]:
        """è¿è¡Œå…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        results = []
        
        # å®šä¹‰æµ‹è¯•æŸ¥è¯¢é›†
        queries = {
            'point_query': {
                'hive': "SELECT * FROM hive.test_schema.lineitem_hive WHERE orderkey = 12345",
                'iceberg': "SELECT * FROM iceberg.test_schema.lineitem_iceberg WHERE orderkey = 12345",
                'delta': "SELECT * FROM delta.test_schema.lineitem_delta WHERE orderkey = 12345"
            },
            'range_scan': {
                'hive': "SELECT COUNT(*) FROM hive.test_schema.lineitem_hive WHERE shipdate BETWEEN DATE '1996-01-01' AND DATE '1996-03-31'",
                'iceberg': "SELECT COUNT(*) FROM iceberg.test_schema.lineitem_iceberg WHERE shipdate BETWEEN DATE '1996-01-01' AND DATE '1996-03-31'",
                'delta': "SELECT COUNT(*) FROM delta.test_schema.lineitem_delta WHERE shipdate BETWEEN DATE '1996-01-01' AND DATE '1996-03-31'"
            },
            'aggregation': {
                'hive': "SELECT suppkey, SUM(extendedprice) as total FROM hive.test_schema.lineitem_hive GROUP BY suppkey",
                'iceberg': "SELECT suppkey, SUM(extendedprice) as total FROM iceberg.test_schema.lineitem_iceberg GROUP BY suppkey",
                'delta': "SELECT suppkey, SUM(extendedprice) as total FROM delta.test_schema.lineitem_delta GROUP BY suppkey"
            }
        }
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        for query_name, format_queries in queries.items():
            print(f"Running benchmark for: {query_name}")
            
            for table_format, query in format_queries.items():
                print(f"  Testing {table_format} format...")
                
                try:
                    result = self.benchmark_query(query, table_format, query_name)
                    results.append(result)
                    
                    print(f"    Execution time: {result.execution_time:.2f}s")
                    print(f"    Rows scanned: {result.rows_scanned:,}")
                    print(f"    Peak memory: {result.peak_memory_mb:.1f}MB")
                    
                except Exception as e:
                    print(f"    Error: {e}")
        
        return results
    
    def generate_performance_report(self, results: List[BenchmarkResult]):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("PERFORMANCE COMPARISON REPORT")
        print("="*60)
        
        # æŒ‰æŸ¥è¯¢ç±»å‹åˆ†ç»„
        by_query = {}
        for result in results:
            if result.query_name not in by_query:
                by_query[result.query_name] = {}
            by_query[result.query_name][result.table_format] = result
        
        # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
        for query_name, formats in by_query.items():
            print(f"\n{query_name.upper()} Performance:")
            print("-" * 40)
            
            baseline_time = formats.get('hive', {}).execution_time if 'hive' in formats else None
            
            for format_name, result in formats.items():
                improvement = ""
                if baseline_time and result.execution_time != baseline_time:
                    ratio = baseline_time / result.execution_time
                    improvement = f"({ratio:.1f}x {'faster' if ratio > 1 else 'slower'})"
                
                print(f"{format_name:>8}: {result.execution_time:6.2f}s  {improvement}")

if __name__ == "__main__":
    benchmark = TableFormatBenchmark()
    results = benchmark.run_comprehensive_benchmark()
    benchmark.generate_performance_report(results)
```

### ğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ

| æŸ¥è¯¢ç±»å‹ | Hiveè€—æ—¶ | Icebergè€—æ—¶ | Deltaè€—æ—¶ | æœ€ä½³æ ¼å¼ | æ€§èƒ½æå‡ |
|---------|----------|-------------|----------|----------|----------|
| ç‚¹æŸ¥è¯¢ |  |  |  |  |  |
| èŒƒå›´æ‰«æ |  |  |  |  |  |
| èšåˆæŸ¥è¯¢ |  |  |  |  |  |
| JoinæŸ¥è¯¢ |  |  |  |  |  |

---

## å®éªŒ4: æ—¶é—´æ—…è¡ŒåŠŸèƒ½å®æˆ˜

### ğŸ¯ å­¦ä¹ ç›®æ ‡
- éªŒè¯Icebergå’ŒDeltaçš„æ—¶é—´æ—…è¡ŒåŠŸèƒ½
- ç†è§£å¿«ç…§å’Œç‰ˆæœ¬çš„æ¦‚å¿µå·®å¼‚
- æµ‹è¯•å†å²æ•°æ®æŸ¥è¯¢çš„æ€§èƒ½
- åˆ†ææ—¶é—´æ—…è¡Œçš„å­˜å‚¨å¼€é”€

### ğŸ”¬ å®éªŒæ­¥éª¤

#### æ­¥éª¤1: åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
```sql
-- åˆ›å»ºæ¨¡æ‹Ÿæ—¶é—´åºåˆ—æ›´æ–°çš„åœºæ™¯
-- é¦–æ¬¡æ’å…¥
INSERT INTO iceberg.test_schema.customer_delta
SELECT 
    custkey, name, address, nationkey, phone, acctbal, mktsegment, comment,
    TIMESTAMP '2023-01-01 00:00:00' as last_updated
FROM hive.tpch_sf1.customer WHERE custkey <= 1000;

-- æ¨¡æ‹Ÿåç»­æ›´æ–°æ“ä½œ (åˆ›å»ºå¤šä¸ªç‰ˆæœ¬)
UPDATE iceberg.test_schema.customer_delta 
SET acctbal = acctbal * 1.1, last_updated = TIMESTAMP '2023-02-01 00:00:00'
WHERE mktsegment = 'BUILDING';

UPDATE iceberg.test_schema.customer_delta
SET acctbal = acctbal * 0.95, last_updated = TIMESTAMP '2023-03-01 00:00:00' 
WHERE mktsegment = 'AUTOMOBILE';

-- æ·»åŠ æ–°å®¢æˆ·
INSERT INTO iceberg.test_schema.customer_delta VALUES
(1001, 'Time Travel Customer', 'Test Address', 1, '555-1001', 10000.0, 'MACHINERY', 'Test', TIMESTAMP '2023-04-01 00:00:00');

-- åˆ é™¤ä¸€äº›å®¢æˆ·
DELETE FROM iceberg.test_schema.customer_delta 
WHERE acctbal < 0 OR custkey IN (999, 998, 997);
```

#### æ­¥éª¤2: Icebergæ—¶é—´æ—…è¡ŒæŸ¥è¯¢
```sql
-- æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„å¿«ç…§
SELECT * FROM iceberg.test_schema."customer_delta$snapshots" 
ORDER BY committed_at;

-- åŸºäºå¿«ç…§IDçš„æ—¶é—´æ—…è¡Œ
SELECT snapshot_id FROM iceberg.test_schema."customer_delta$snapshots" 
ORDER BY committed_at LIMIT 3;

-- ä½¿ç”¨ç¬¬ä¸€ä¸ªå¿«ç…§çš„æ•°æ®
SELECT COUNT(*), AVG(acctbal) as avg_balance
FROM iceberg.test_schema.customer_delta FOR VERSION AS OF 12345678901234;  -- æ›¿æ¢ä¸ºå®é™…snapshot_id

-- åŸºäºæ—¶é—´æˆ³çš„æ—¶é—´æ—…è¡Œ  
SELECT COUNT(*), AVG(acctbal) as avg_balance
FROM iceberg.test_schema.customer_delta FOR TIMESTAMP AS OF TIMESTAMP '2023-02-15 00:00:00';

-- å¯¹æ¯”å½“å‰ç‰ˆæœ¬
SELECT COUNT(*), AVG(acctbal) as avg_balance  
FROM iceberg.test_schema.customer_delta;

-- ğŸ¯ åˆ†æç»“æœå·®å¼‚ï¼ŒéªŒè¯æ—¶é—´æ—…è¡Œçš„æ­£ç¡®æ€§
```

#### æ­¥éª¤3: å¢é‡æŸ¥è¯¢å®æˆ˜
```sql
-- Icebergå¢é‡æŸ¥è¯¢ (æŸ¥çœ‹ä¸¤ä¸ªå¿«ç…§ä¹‹é—´çš„å˜åŒ–)
SELECT * FROM TABLE(iceberg.system.table_changes(
    'test_schema', 'customer_delta',
    12345678901234,  -- from_snapshot_id
    23456789012345   -- to_snapshot_id  
));

-- åˆ†æå¢é‡æ•°æ®çš„ç‰¹å¾
WITH incremental_changes AS (
  SELECT * FROM TABLE(iceberg.system.table_changes(
    'test_schema', 'customer_delta', 
    12345678901234, 23456789012345
  ))
)
SELECT 
  change_type,
  COUNT(*) as change_count,
  COUNT(DISTINCT custkey) as affected_customers,
  SUM(CASE WHEN change_type = 'INSERT' THEN 1 ELSE 0 END) as inserts,
  SUM(CASE WHEN change_type = 'DELETE' THEN 1 ELSE 0 END) as deletes,
  SUM(CASE WHEN change_type = 'UPDATE_BEFORE' THEN 1 ELSE 0 END) as updates
FROM incremental_changes
GROUP BY change_type;
```

### ğŸ“Š æ—¶é—´æ—…è¡Œæ€§èƒ½åˆ†æ

| æŸ¥è¯¢ç±»å‹ | å½“å‰ç‰ˆæœ¬è€—æ—¶ | å†å²ç‰ˆæœ¬è€—æ—¶ | æ€§èƒ½å·®å¼‚ | å­˜å‚¨å¼€é”€ | å¤‡æ³¨ |
|---------|-------------|-------------|----------|----------|------|
| ç‚¹æŸ¥è¯¢ |  |  |  |  |  |
| èšåˆæŸ¥è¯¢ |  |  |  |  |  |
| å¢é‡æŸ¥è¯¢ |  |  |  |  |  |

---

## å®éªŒ5: æ¨¡å¼æ¼”è¿›ä¸å…¼å®¹æ€§

### ğŸ¯ å­¦ä¹ ç›®æ ‡
- éªŒè¯ä¸åŒè¡¨æ ¼å¼çš„æ¨¡å¼æ¼”è¿›èƒ½åŠ›
- ç†è§£å‘å‰å’Œå‘åå…¼å®¹æ€§
- æµ‹è¯•å¤æ‚æ¨¡å¼å˜æ›´çš„å¤„ç†
- åˆ†ææ¨¡å¼æ¼”è¿›å¯¹æŸ¥è¯¢çš„å½±å“

### ğŸ”¬ å®éªŒæ­¥éª¤

#### æ­¥éª¤1: Icebergæ¨¡å¼æ¼”è¿›æµ‹è¯•
```sql
-- åˆå§‹è¡¨ç»“æ„
CREATE TABLE iceberg.test_schema.evolving_table (
    id BIGINT,
    name VARCHAR(100),
    created_date DATE
);

-- æ’å…¥åˆå§‹æ•°æ®
INSERT INTO iceberg.test_schema.evolving_table VALUES
(1, 'Record 1', DATE '2023-01-01'),
(2, 'Record 2', DATE '2023-01-02'),
(3, 'Record 3', DATE '2023-01-03');

-- æ¨¡å¼æ¼”è¿›1: æ·»åŠ æ–°åˆ—
ALTER TABLE iceberg.test_schema.evolving_table 
ADD COLUMN email VARCHAR(200);

-- éªŒè¯æ–°åˆ—çš„é»˜è®¤å€¼å¤„ç†
SELECT * FROM iceberg.test_schema.evolving_table;

-- æ’å…¥åŒ…å«æ–°åˆ—çš„æ•°æ®
INSERT INTO iceberg.test_schema.evolving_table VALUES
(4, 'Record 4', DATE '2023-01-04', 'test4@example.com'),
(5, 'Record 5', DATE '2023-01-05', 'test5@example.com');

-- æ¨¡å¼æ¼”è¿›2: ä¿®æ”¹åˆ—ç±»å‹ (æ”¯æŒçš„ç±»å‹æå‡)
ALTER TABLE iceberg.test_schema.evolving_table 
ALTER COLUMN id SET DATA TYPE BIGINT;  -- int â†’ bigint

-- æ¨¡å¼æ¼”è¿›3: é‡å‘½ååˆ—
ALTER TABLE iceberg.test_schema.evolving_table 
RENAME COLUMN created_date TO creation_date;

-- éªŒè¯æ¨¡å¼æ¼”è¿›åçš„æŸ¥è¯¢å…¼å®¹æ€§
SELECT id, name, creation_date, email FROM iceberg.test_schema.evolving_table;
```

#### æ­¥éª¤2: è·¨ç‰ˆæœ¬å…¼å®¹æ€§æµ‹è¯•
```sql
-- ä½¿ç”¨æ—§æ¨¡å¼æŸ¥è¯¢æ–°æ•°æ® (å‘åå…¼å®¹)
SELECT id, name, created_date  -- ä½¿ç”¨æ—§åˆ—å
FROM iceberg.test_schema.evolving_table FOR VERSION AS OF <old_snapshot_id>;

-- ä½¿ç”¨æ–°æ¨¡å¼æŸ¥è¯¢æ‰€æœ‰æ•°æ® (å‘å‰å…¼å®¹)
SELECT id, name, creation_date, email
FROM iceberg.test_schema.evolving_table;

-- ğŸ¯ éªŒè¯å…¼å®¹æ€§:
-- - æ—§æŸ¥è¯¢æ˜¯å¦ä»èƒ½æ­£å¸¸å·¥ä½œï¼Ÿ
-- - æ–°åˆ—åœ¨æ—§æ•°æ®ä¸­å¦‚ä½•å¤„ç†ï¼Ÿ
-- - ç±»å‹æå‡æ˜¯å¦é€æ˜ï¼Ÿ
```

### ğŸ“Š æ¨¡å¼æ¼”è¿›å…¼å®¹æ€§æµ‹è¯•ç»“æœ

| æ¼”è¿›æ“ä½œ | Iceberg | Delta | Hive | å…¼å®¹æ€§å½±å“ |
|---------|---------|-------|------|------------|
| ADD COLUMN |  |  |  |  |
| DROP COLUMN |  |  |  |  |
| RENAME COLUMN |  |  |  |  |
| ç±»å‹æå‡(intâ†’bigint) |  |  |  |  |
| åµŒå¥—ç»“æ„å˜æ›´ |  |  |  |  |

---

## å®éªŒ6-8: å…¶ä»–é«˜çº§å®éªŒ

ç”±äºç¯‡å¹…é™åˆ¶ï¼Œæä¾›å…¶ä½™å®éªŒçš„å¤§çº²ï¼š

### å®éªŒ6: ACIDäº‹åŠ¡ä¸å¹¶å‘æ§åˆ¶
- æ¨¡æ‹Ÿå¤šå®¢æˆ·ç«¯å¹¶å‘å†™å…¥åœºæ™¯
- æµ‹è¯•äº‹åŠ¡éš”ç¦»çº§åˆ«
- éªŒè¯ä¹è§‚å¹¶å‘æ§åˆ¶æœºåˆ¶
- åˆ†æå†²çªè§£å†³ç­–ç•¥

### å®éªŒ7: è¡¨ç»´æŠ¤ä¸ä¼˜åŒ–ç­–ç•¥
- å®æ–½è‡ªåŠ¨åŒ–è¡¨ç»´æŠ¤æµç¨‹
- æµ‹è¯•å°æ–‡ä»¶å‹ç¼©æ•ˆæœ  
- éªŒè¯è¿‡æœŸå¿«ç…§æ¸…ç†æœºåˆ¶
- åˆ†æå­˜å‚¨æˆæœ¬ä¼˜åŒ–

### å®éªŒ8: æ¹–ä»“ä¸€ä½“åŒ–æ¶æ„è®¾è®¡
- è®¾è®¡ç«¯åˆ°ç«¯çš„æ¹–ä»“æ¶æ„
- é›†æˆæµå¼å’Œæ‰¹é‡å¤„ç†
- å®ç°ç»Ÿä¸€çš„å…ƒæ•°æ®ç®¡ç†
- å»ºç«‹æ•°æ®æ²»ç†ä½“ç³»

---

## ğŸ¯ å®éªŒæ€»ç»“ä¸è¿›é˜¶

å®Œæˆè¿™äº›æ•°æ®æ¹–è¡¨æ ¼å¼å®éªŒåï¼Œä½ å°†ï¼š

### âœ… **æŒæ¡ç°ä»£æ•°æ®æ¹–æŠ€æœ¯**
- æ·±å…¥ç†è§£Icebergã€Deltaã€Hudiçš„æŠ€æœ¯åŸç†
- å…·å¤‡é€‰æ‹©å’Œé…ç½®åˆé€‚è¡¨æ ¼å¼çš„èƒ½åŠ›
- æŒæ¡è¡¨æ ¼å¼çš„æ€§èƒ½è°ƒä¼˜æŠ€å·§

### âœ… **å»ºç«‹æ¹–ä»“ä¸€ä½“åŒ–è§†é‡**  
- ç†è§£æ¹–ä»“ä¸€ä½“åŒ–çš„æŠ€æœ¯è·¯å¾„
- å…·å¤‡è®¾è®¡ç°ä»£æ•°æ®æ¶æ„çš„èƒ½åŠ›
- æŒæ¡è·¨å¼•æ“æ•°æ®è®¿é—®çš„æŠ€æœ¯æ–¹æ¡ˆ

### âœ… **è·å¾—å‰æ²¿æŠ€æœ¯ä¼˜åŠ¿**
- æ•°æ®æ¹–è¡¨æ ¼å¼æ˜¯æ•°æ®é¢†åŸŸçš„å‰æ²¿æŠ€æœ¯
- ACIDäº‹åŠ¡èƒ½åŠ›æ˜¯ç°ä»£æ•°æ®å¹³å°çš„æ ‡é…
- æ—¶é—´æ—…è¡Œå’Œå¢é‡å¤„ç†æ˜¯å·®å¼‚åŒ–èƒ½åŠ›

### ğŸš€ **èŒä¸šå‘å±•ä»·å€¼**
1. **æ•°æ®æ¹–æ¶æ„å¸ˆ**: è®¾è®¡ä¼ä¸šçº§æ•°æ®æ¹–å¹³å°
2. **æ¹–ä»“æŠ€æœ¯ä¸“å®¶**: å¼•é¢†ç»„ç»‡çš„æŠ€æœ¯æ¼”è¿›  
3. **å¼€æºæŠ€æœ¯è´¡çŒ®è€…**: å‚ä¸å‰æ²¿å¼€æºé¡¹ç›®
4. **æŠ€æœ¯å’¨è¯¢é¡¾é—®**: å¸®åŠ©ä¼ä¸šé€‰æ‹©æœ€ä¼˜æŠ€æœ¯æ–¹æ¡ˆ

é€šè¿‡æŒæ¡æ•°æ®æ¹–è¡¨æ ¼å¼æŠ€æœ¯ï¼Œä½ å·²ç»ç«™åœ¨äº†æ•°æ®å­˜å‚¨æŠ€æœ¯çš„æœ€å‰æ²¿ï¼ğŸŒŸ
