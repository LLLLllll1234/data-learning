#!/usr/bin/env python3
"""
Trino Performance Analyzer

è¿™ä¸ªè„šæœ¬æä¾›äº†ä¸€å¥—å®Œæ•´çš„Trinoæ€§èƒ½åˆ†æå’Œç›‘æ§å·¥å…·ï¼ŒåŒ…æ‹¬ï¼š
- è‡ªåŠ¨åŒ–æ€§èƒ½æŠ¥å‘Šç”Ÿæˆ
- æŸ¥è¯¢æ€§èƒ½è¶‹åŠ¿åˆ†æ  
- ç»Ÿè®¡ä¿¡æ¯å¥åº·åº¦æ£€æŸ¥
- ä¼˜åŒ–å»ºè®®ç”Ÿæˆ
- æ€§èƒ½é¢„è­¦ç³»ç»Ÿ

ä½¿ç”¨æ–¹æ³•:
    python trino_performance_analyzer.py --config config.yaml
    python trino_performance_analyzer.py --report --days 7
    python trino_performance_analyzer.py --analyze-slow-queries --threshold 300
"""

import argparse
import json
import logging
import sys
import time
import yaml
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import trino
from trino.auth import BasicAuthentication

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('trino_analyzer.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class TrinoPerformanceAnalyzer:
    """Trinoæ€§èƒ½åˆ†æå™¨ä¸»ç±»"""
    
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.config = self._load_config(config_path)
        self.conn = self._create_connection()
        
    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        default_config = {
            'trino': {
                'host': 'localhost',
                'port': 8080,
                'user': 'admin',
                'catalog': 'system',
                'schema': 'runtime'
            },
            'analysis': {
                'slow_query_threshold': 300,  # 5åˆ†é’Ÿ
                'memory_threshold_gb': 10,
                'queue_time_threshold': 30,
                'failure_rate_threshold': 0.05
            },
            'reporting': {
                'output_dir': './reports',
                'days_lookback': 7
            }
        }
        
        if config_path:
            try:
                with open(config_path, 'r') as f:
                    user_config = yaml.safe_load(f)
                    default_config.update(user_config)
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶ {config_path}: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                
        return default_config
        
    def _create_connection(self):
        """åˆ›å»ºTrinoè¿æ¥"""
        config = self.config['trino']
        
        auth = None
        if config.get('username') and config.get('password'):
            auth = BasicAuthentication(config['username'], config['password'])
            
        return trino.dbapi.connect(
            host=config['host'],
            port=config['port'],
            user=config['user'],
            catalog=config['catalog'],
            schema=config['schema'],
            auth=auth
        )
    
    def execute_query(self, query: str) -> pd.DataFrame:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›DataFrame"""
        try:
            return pd.read_sql(query, self.conn)
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
            logger.error(f"æŸ¥è¯¢å†…å®¹: {query[:200]}...")
            raise
    
    def generate_performance_report(self, days: int = 7) -> Dict:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        logger.info(f"ç”Ÿæˆè¿‡å» {days} å¤©çš„æ€§èƒ½æŠ¥å‘Š...")
        
        report = {
            'report_time': datetime.now().isoformat(),
            'analysis_period_days': days,
            'summary': {},
            'detailed_analysis': {}
        }
        
        # åŸºç¡€ç»Ÿè®¡
        report['summary'] = self._get_basic_stats(days)
        
        # æ…¢æŸ¥è¯¢åˆ†æ
        report['detailed_analysis']['slow_queries'] = self._analyze_slow_queries(days)
        
        # èµ„æºä½¿ç”¨åˆ†æ
        report['detailed_analysis']['resource_usage'] = self._analyze_resource_usage(days)
        
        # ç»Ÿè®¡ä¿¡æ¯å¥åº·åº¦
        report['detailed_analysis']['stats_health'] = self._check_stats_health()
        
        # ä¼˜åŒ–å»ºè®®
        report['recommendations'] = self._generate_recommendations(report)
        
        return report
    
    def _get_basic_stats(self, days: int) -> Dict:
        """è·å–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯"""
        query = f"""
        SELECT 
            COUNT(*) as total_queries,
            COUNT(CASE WHEN state = 'FINISHED' THEN 1 END) as successful_queries,
            COUNT(CASE WHEN state = 'FAILED' THEN 1 END) as failed_queries,
            AVG(elapsed_time.seconds) as avg_runtime_seconds,
            PERCENTILE(elapsed_time.seconds, 0.95) as p95_runtime_seconds,
            SUM(total_bytes) / 1024.0 / 1024.0 / 1024.0 / 1024.0 as total_processed_tb,
            AVG(peak_user_memory_bytes) / 1024.0 / 1024.0 / 1024.0 as avg_memory_gb,
            MAX(peak_user_memory_bytes) / 1024.0 / 1024.0 / 1024.0 as max_memory_gb
        FROM system.runtime.queries
        WHERE created >= current_timestamp - INTERVAL '{days}' DAY
        """
        
        result = self.execute_query(query)
        if not result.empty:
            stats = result.iloc[0].to_dict()
            stats['success_rate'] = (stats['successful_queries'] / 
                                   max(stats['total_queries'], 1))
            stats['failure_rate'] = (stats['failed_queries'] / 
                                   max(stats['total_queries'], 1))
            return stats
        return {}
    
    def _analyze_slow_queries(self, days: int) -> Dict:
        """åˆ†ææ…¢æŸ¥è¯¢"""
        threshold = self.config['analysis']['slow_query_threshold']
        
        query = f"""
        WITH slow_queries AS (
            SELECT 
                query_id,
                user_name,
                query,
                elapsed_time.seconds as total_seconds,
                queued_time.seconds as queued_seconds,
                planning_time.seconds as planning_seconds,
                execution_time.seconds as execution_seconds,
                total_bytes / 1024.0 / 1024.0 / 1024.0 as processed_gb,
                peak_user_memory_bytes / 1024.0 / 1024.0 / 1024.0 as peak_memory_gb,
                created
            FROM system.runtime.queries
            WHERE state = 'FINISHED'
              AND elapsed_time.seconds > {threshold}
              AND created >= current_timestamp - INTERVAL '{days}' DAY
        )
        SELECT 
            COUNT(*) as slow_query_count,
            AVG(total_seconds) as avg_slow_runtime,
            AVG(planning_seconds) as avg_planning_time,
            AVG(execution_seconds) as avg_execution_time,
            AVG(processed_gb) as avg_data_processed,
            AVG(peak_memory_gb) as avg_memory_usage,
            array_agg(DISTINCT user_name) as affected_users
        FROM slow_queries
        """
        
        result = self.execute_query(query)
        
        if not result.empty and result.iloc[0]['slow_query_count'] > 0:
            analysis = result.iloc[0].to_dict()
            
            # è·å–å…·ä½“çš„æ…¢æŸ¥è¯¢è¯¦æƒ…
            detail_query = f"""
            SELECT 
                query_id,
                user_name,
                left(query, 200) as query_preview,
                elapsed_time.seconds as total_seconds,
                total_bytes / 1024.0 / 1024.0 / 1024.0 as processed_gb,
                peak_user_memory_bytes / 1024.0 / 1024.0 / 1024.0 as peak_memory_gb
            FROM system.runtime.queries
            WHERE state = 'FINISHED'
              AND elapsed_time.seconds > {threshold}
              AND created >= current_timestamp - INTERVAL '{days}' DAY
            ORDER BY elapsed_time.seconds DESC
            LIMIT 10
            """
            
            details = self.execute_query(detail_query)
            analysis['top_slow_queries'] = details.to_dict('records')
            
            return analysis
        
        return {'slow_query_count': 0}
    
    def _analyze_resource_usage(self, days: int) -> Dict:
        """åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ"""
        query = f"""
        WITH hourly_stats AS (
            SELECT 
                date_trunc('hour', created) as hour,
                COUNT(*) as query_count,
                AVG(peak_user_memory_bytes / 1024.0 / 1024.0 / 1024.0) as avg_memory_gb,
                MAX(peak_user_memory_bytes / 1024.0 / 1024.0 / 1024.0) as max_memory_gb,
                AVG(queued_time.seconds) as avg_queue_time,
                COUNT(CASE WHEN error_code = 'EXCEEDED_MEMORY_LIMIT' THEN 1 END) as oom_count
            FROM system.runtime.queries
            WHERE created >= current_timestamp - INTERVAL '{days}' DAY
            GROUP BY 1
        )
        SELECT 
            AVG(query_count) as avg_queries_per_hour,
            MAX(query_count) as peak_queries_per_hour,
            AVG(avg_memory_gb) as avg_memory_usage_gb,
            MAX(max_memory_gb) as peak_memory_usage_gb,
            AVG(avg_queue_time) as avg_queue_time_seconds,
            SUM(oom_count) as total_oom_failures
        FROM hourly_stats
        """
        
        result = self.execute_query(query)
        if not result.empty:
            return result.iloc[0].to_dict()
        return {}
    
    def _check_stats_health(self) -> Dict:
        """æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯å¥åº·åº¦"""
        query = """
        WITH table_stats AS (
            SELECT 
                COUNT(*) as total_tables,
                COUNT(CASE WHEN row_count IS NULL THEN 1 END) as missing_row_stats,
                COUNT(CASE WHEN row_count = 0 THEN 1 END) as zero_row_stats
            FROM system.metadata.table_statistics
            WHERE table_schema NOT IN ('information_schema', 'system')
        ),
        column_stats AS (
            SELECT 
                COUNT(*) as total_columns,
                COUNT(CASE WHEN distinct_values_count IS NULL THEN 1 END) as missing_ndv_stats,
                COUNT(CASE WHEN nulls_fraction IS NULL THEN 1 END) as missing_null_stats
            FROM system.metadata.column_statistics
            WHERE table_schema NOT IN ('information_schema', 'system')
        )
        SELECT 
            t.total_tables,
            t.missing_row_stats,
            t.zero_row_stats,
            c.total_columns,
            c.missing_ndv_stats,
            c.missing_null_stats
        FROM table_stats t, column_stats c
        """
        
        result = self.execute_query(query)
        if not result.empty:
            stats = result.iloc[0].to_dict()
            
            # è®¡ç®—å¥åº·åº¦ç™¾åˆ†æ¯”
            stats['table_stats_health'] = (
                1 - (stats['missing_row_stats'] + stats['zero_row_stats']) / 
                max(stats['total_tables'], 1)
            ) * 100
            
            stats['column_stats_health'] = (
                1 - (stats['missing_ndv_stats'] + stats['missing_null_stats']) / 
                max(stats['total_columns'], 1)
            ) * 100
            
            return stats
        return {}
    
    def _generate_recommendations(self, report: Dict) -> List[Dict]:
        """åŸºäºåˆ†æç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        summary = report.get('summary', {})
        slow_queries = report.get('detailed_analysis', {}).get('slow_queries', {})
        resource_usage = report.get('detailed_analysis', {}).get('resource_usage', {})
        stats_health = report.get('detailed_analysis', {}).get('stats_health', {})
        
        # å¤±è´¥ç‡å»ºè®®
        failure_rate = summary.get('failure_rate', 0)
        if failure_rate > self.config['analysis']['failure_rate_threshold']:
            recommendations.append({
                'category': 'stability',
                'priority': 'high',
                'issue': f'æŸ¥è¯¢å¤±è´¥ç‡è¿‡é«˜: {failure_rate:.2%}',
                'recommendation': 'æ£€æŸ¥é›†ç¾¤èµ„æºé…ç½®ï¼Œåˆ†æå¤±è´¥åŸå› åˆ†å¸ƒ',
                'action': 'SELECT error_type, COUNT(*) FROM system.runtime.queries WHERE state = \'FAILED\' GROUP BY error_type'
            })
        
        # æ…¢æŸ¥è¯¢å»ºè®®
        slow_count = slow_queries.get('slow_query_count', 0)
        if slow_count > 0:
            recommendations.append({
                'category': 'performance',
                'priority': 'medium',
                'issue': f'å‘ç° {slow_count} ä¸ªæ…¢æŸ¥è¯¢',
                'recommendation': 'åˆ†ææ…¢æŸ¥è¯¢çš„æ‰§è¡Œè®¡åˆ’ï¼Œè€ƒè™‘å¯ç”¨åŠ¨æ€è¿‡æ»¤æˆ–è°ƒæ•´Joinç­–ç•¥',
                'action': 'SET SESSION enable_dynamic_filtering = true; SET SESSION join_distribution_type = \'AUTOMATIC\''
            })
        
        # å†…å­˜ä½¿ç”¨å»ºè®®
        avg_memory = resource_usage.get('avg_memory_usage_gb', 0)
        peak_memory = resource_usage.get('peak_memory_usage_gb', 0)
        if peak_memory > self.config['analysis']['memory_threshold_gb']:
            recommendations.append({
                'category': 'resource',
                'priority': 'medium', 
                'issue': f'å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œå³°å€¼: {peak_memory:.1f}GB',
                'recommendation': 'è€ƒè™‘è°ƒæ•´query_max_memoryå‚æ•°æˆ–ä¼˜åŒ–å†…å­˜å¯†é›†å‹æŸ¥è¯¢',
                'action': 'SET SESSION query_max_memory_per_node = \'8GB\''
            })
        
        # ç»Ÿè®¡ä¿¡æ¯å»ºè®®
        table_health = stats_health.get('table_stats_health', 100)
        if table_health < 80:
            recommendations.append({
                'category': 'statistics',
                'priority': 'high',
                'issue': f'è¡¨ç»Ÿè®¡ä¿¡æ¯ç¼ºå¤±ä¸¥é‡ï¼Œå¥åº·åº¦: {table_health:.1f}%',
                'recommendation': 'ç«‹å³æ”¶é›†å…³é”®è¡¨çš„ç»Ÿè®¡ä¿¡æ¯ä»¥æ”¹å–„CBOæ•ˆæœ',
                'action': 'æ‰§è¡Œ ANALYZE TABLE è¯­å¥æ”¶é›†ç»Ÿè®¡ä¿¡æ¯'
            })
        
        return recommendations
    
    def create_performance_dashboard(self, days: int = 7) -> str:
        """åˆ›å»ºæ€§èƒ½ä»ªè¡¨æ¿"""
        logger.info("åˆ›å»ºæ€§èƒ½ä»ªè¡¨æ¿...")
        
        # è·å–æ—¶é—´åºåˆ—æ•°æ®
        query = f"""
        SELECT 
            date_trunc('hour', created) as hour,
            COUNT(*) as query_count,
            AVG(elapsed_time.seconds) as avg_runtime,
            PERCENTILE(elapsed_time.seconds, 0.95) as p95_runtime,
            AVG(peak_user_memory_bytes / 1024.0 / 1024.0 / 1024.0) as avg_memory_gb,
            COUNT(CASE WHEN state = 'FAILED' THEN 1 END) as failed_count
        FROM system.runtime.queries
        WHERE created >= current_timestamp - INTERVAL '{days}' DAY
        GROUP BY 1
        ORDER BY 1
        """
        
        df = self.execute_query(query)
        
        if df.empty:
            logger.warning("æ²¡æœ‰æŸ¥è¯¢æ•°æ®å¯ç”¨äºç”Ÿæˆä»ªè¡¨æ¿")
            return ""
        
        # åˆ›å»ºå­å›¾
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=('æŸ¥è¯¢æ•°é‡è¶‹åŠ¿', 'è¿è¡Œæ—¶é—´è¶‹åŠ¿', 
                          'å†…å­˜ä½¿ç”¨è¶‹åŠ¿', 'å¤±è´¥ç‡è¶‹åŠ¿',
                          '95åˆ†ä½è¿è¡Œæ—¶é—´', 'æŸ¥è¯¢æ•°é‡åˆ†å¸ƒ'),
            specs=[[{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"secondary_y": False}],
                   [{"secondary_y": False}, {"type": "bar"}]]
        )
        
        # æŸ¥è¯¢æ•°é‡è¶‹åŠ¿
        fig.add_trace(
            go.Scatter(x=df['hour'], y=df['query_count'], 
                      mode='lines+markers', name='æŸ¥è¯¢æ•°é‡'),
            row=1, col=1
        )
        
        # è¿è¡Œæ—¶é—´è¶‹åŠ¿  
        fig.add_trace(
            go.Scatter(x=df['hour'], y=df['avg_runtime'], 
                      mode='lines+markers', name='å¹³å‡è¿è¡Œæ—¶é—´(ç§’)'),
            row=1, col=2
        )
        
        # å†…å­˜ä½¿ç”¨è¶‹åŠ¿
        fig.add_trace(
            go.Scatter(x=df['hour'], y=df['avg_memory_gb'], 
                      mode='lines+markers', name='å¹³å‡å†…å­˜ä½¿ç”¨(GB)'),
            row=2, col=1
        )
        
        # å¤±è´¥ç‡è¶‹åŠ¿
        df['failure_rate'] = df['failed_count'] / df['query_count'] * 100
        fig.add_trace(
            go.Scatter(x=df['hour'], y=df['failure_rate'], 
                      mode='lines+markers', name='å¤±è´¥ç‡(%)'),
            row=2, col=2  
        )
        
        # 95åˆ†ä½è¿è¡Œæ—¶é—´
        fig.add_trace(
            go.Scatter(x=df['hour'], y=df['p95_runtime'], 
                      mode='lines+markers', name='P95è¿è¡Œæ—¶é—´(ç§’)'),
            row=3, col=1
        )
        
        # æŸ¥è¯¢æ•°é‡åˆ†å¸ƒæŸ±çŠ¶å›¾
        fig.add_trace(
            go.Bar(x=df['hour'].dt.hour, y=df['query_count'], 
                   name='æŒ‰å°æ—¶åˆ†å¸ƒ'),
            row=3, col=2
        )
        
        fig.update_layout(
            height=1200, 
            title_text=f"Trinoæ€§èƒ½ä»ªè¡¨æ¿ (è¿‡å»{days}å¤©)",
            showlegend=False
        )
        
        # ä¿å­˜ä»ªè¡¨æ¿
        output_file = f"trino_dashboard_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        fig.write_html(output_file)
        
        logger.info(f"ä»ªè¡¨æ¿å·²ä¿å­˜åˆ°: {output_file}")
        return output_file
    
    def analyze_query_patterns(self, days: int = 7) -> Dict:
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        logger.info("åˆ†ææŸ¥è¯¢æ¨¡å¼...")
        
        query = f"""
        WITH query_patterns AS (
            SELECT 
                regexp_replace(
                    regexp_replace(query, '\\d+', 'N'),  -- æ•°å­—æ›¿æ¢
                    '''[^'']*''', '''X'''                -- å­—ç¬¦ä¸²æ›¿æ¢  
                ) as pattern,
                elapsed_time.seconds as seconds,
                user_name,
                created
            FROM system.runtime.queries
            WHERE state = 'FINISHED'
              AND created >= current_timestamp - INTERVAL '{days}' DAY
              AND length(query) > 50  -- è¿‡æ»¤ç®€å•æŸ¥è¯¢
        )
        SELECT 
            left(pattern, 200) as pattern_preview,
            COUNT(*) as execution_count,
            AVG(seconds) as avg_runtime,
            STDDEV(seconds) as stddev_runtime,
            MIN(seconds) as min_runtime,
            MAX(seconds) as max_runtime,
            array_agg(DISTINCT user_name) as users
        FROM query_patterns
        GROUP BY pattern
        HAVING COUNT(*) >= 3
        ORDER BY execution_count DESC, avg_runtime DESC
        LIMIT 20
        """
        
        result = self.execute_query(query)
        
        patterns = []
        for _, row in result.iterrows():
            pattern_info = {
                'pattern': row['pattern_preview'],
                'frequency': row['execution_count'],
                'avg_runtime': row['avg_runtime'],
                'runtime_variability': (row['stddev_runtime'] / max(row['avg_runtime'], 1)),
                'users': row['users'],
                'optimization_potential': 'high' if row['avg_runtime'] > 300 else 'medium' if row['avg_runtime'] > 60 else 'low'
            }
            patterns.append(pattern_info)
        
        return {'patterns': patterns, 'total_patterns': len(patterns)}
    
    def export_report(self, report: Dict, format: str = 'json') -> str:
        """å¯¼å‡ºæŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if format.lower() == 'json':
            filename = f"trino_performance_report_{timestamp}.json"
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
                
        elif format.lower() == 'html':
            filename = f"trino_performance_report_{timestamp}.html"
            html_content = self._generate_html_report(report)
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(html_content)
                
        logger.info(f"æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {filename}")
        return filename
    
    def _generate_html_report(self, report: Dict) -> str:
        """ç”ŸæˆHTMLæ ¼å¼çš„æŠ¥å‘Š"""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Trino Performance Report</title>
            <meta charset="UTF-8">
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background: #f5f5f5; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; }}
                .metric {{ background: #e8f4fd; padding: 10px; margin: 5px 0; border-left: 4px solid #2196F3; }}
                .recommendation {{ background: #fff3cd; padding: 10px; margin: 5px 0; border-left: 4px solid #ffc107; }}
                .high-priority {{ border-left-color: #dc3545 !important; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Trino æ€§èƒ½åˆ†ææŠ¥å‘Š</h1>
                <p>æŠ¥å‘Šæ—¶é—´: {report['report_time']}</p>
                <p>åˆ†æå‘¨æœŸ: {report['analysis_period_days']} å¤©</p>
            </div>
            
            <div class="section">
                <h2>ğŸ“Š åŸºç¡€ç»Ÿè®¡</h2>
        """
        
        summary = report.get('summary', {})
        for key, value in summary.items():
            if isinstance(value, float):
                if key.endswith('_rate'):
                    value = f"{value:.2%}"
                elif key.endswith('_gb') or key.endswith('_tb'):
                    value = f"{value:.2f}"
                else:
                    value = f"{value:.2f}"
            html += f'<div class="metric"><strong>{key}:</strong> {value}</div>\n'
        
        html += """
            </div>
            
            <div class="section">
                <h2>âš¡ ä¼˜åŒ–å»ºè®®</h2>
        """
        
        for rec in report.get('recommendations', []):
            priority_class = ' high-priority' if rec.get('priority') == 'high' else ''
            html += f"""
            <div class="recommendation{priority_class}">
                <h4>{rec.get('issue', 'Unknown Issue')}</h4>
                <p><strong>å»ºè®®:</strong> {rec.get('recommendation', '')}</p>
                <p><strong>æ“ä½œ:</strong> <code>{rec.get('action', '')}</code></p>
            </div>
            """
        
        html += """
            </div>
        </body>
        </html>
        """
        
        return html


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Trino Performance Analyzer')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--report', action='store_true', help='ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š')
    parser.add_argument('--dashboard', action='store_true', help='åˆ›å»ºæ€§èƒ½ä»ªè¡¨æ¿')
    parser.add_argument('--analyze-patterns', action='store_true', help='åˆ†ææŸ¥è¯¢æ¨¡å¼')
    parser.add_argument('--days', type=int, default=7, help='åˆ†æå¤©æ•°')
    parser.add_argument('--format', choices=['json', 'html'], default='json', help='æŠ¥å‘Šæ ¼å¼')
    parser.add_argument('--threshold', type=int, default=300, help='æ…¢æŸ¥è¯¢é˜ˆå€¼(ç§’)')
    
    args = parser.parse_args()
    
    try:
        analyzer = TrinoPerformanceAnalyzer(args.config)
        
        if args.report:
            logger.info("å¼€å§‹ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š...")
            report = analyzer.generate_performance_report(args.days)
            filename = analyzer.export_report(report, args.format)
            logger.info(f"æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {filename}")
            
        if args.dashboard:
            logger.info("å¼€å§‹åˆ›å»ºæ€§èƒ½ä»ªè¡¨æ¿...")
            dashboard_file = analyzer.create_performance_dashboard(args.days)
            logger.info(f"æ€§èƒ½ä»ªè¡¨æ¿åˆ›å»ºå®Œæˆ: {dashboard_file}")
            
        if args.analyze_patterns:
            logger.info("å¼€å§‹åˆ†ææŸ¥è¯¢æ¨¡å¼...")
            patterns = analyzer.analyze_query_patterns(args.days)
            logger.info(f"å‘ç° {patterns['total_patterns']} ä¸ªæŸ¥è¯¢æ¨¡å¼")
            
            # è¾“å‡ºé«˜é¢‘æŸ¥è¯¢æ¨¡å¼
            for pattern in patterns['patterns'][:5]:
                logger.info(f"æ¨¡å¼é¢‘ç‡: {pattern['frequency']}, å¹³å‡è¿è¡Œæ—¶é—´: {pattern['avg_runtime']:.1f}ç§’")
                logger.info(f"æŸ¥è¯¢æ¨¡å¼: {pattern['pattern'][:100]}...")
                
    except Exception as e:
        logger.error(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
