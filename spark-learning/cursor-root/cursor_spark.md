# æ·±å…¥å­¦ä¹  Spark çš„ç†è®ºçŸ¥è¯†ä¸åŸç†
_Exported on 2025/9/8 at GMT+8 11:20:34 from Cursor (1.5.11)_

---

**User**

æ·±å…¥å­¦ä¹  spark çš„ç†è®ºçŸ¥è¯†å’ŒåŸç†
ä¸€ã€è®¾è®¡æ€æƒ³ä¸å®šä½
å¤§æ•°æ®å¤„ç†ç—›ç‚¹ï¼šHadoop MapReduce ç£ç›˜ I/O è¿‡å¤šï¼Œå»¶è¿Ÿé«˜ï¼›éœ€è¦ä¸€ä¸ªæ›´å¿«ã€æ›´é€šç”¨çš„è®¡ç®—æ¡†æ¶ã€‚

Spark çš„æ ¸å¿ƒæ€è·¯ï¼š

å†…å­˜è®¡ç®—ï¼šå°½é‡åœ¨å†…å­˜ä¸­å®Œæˆè¿­ä»£ï¼Œå‡å°‘ç£ç›˜è¯»å†™ã€‚

ç»Ÿä¸€è®¡ç®—å¼•æ“ï¼šä¸€ä¸ªæ¡†æ¶æ”¯æŒæ‰¹å¤„ç†ã€äº¤äº’å¼ SQLã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—ã€‚

å¼¹æ€§ä¸å®¹é”™ï¼šé€šè¿‡ RDD è¡€ç¼˜æ¥ä¿è¯å®¹é”™èƒ½åŠ›ã€‚

äºŒã€æ•´ä½“æ¶æ„
1. æ¶æ„è§’è‰²
Driverï¼šåº”ç”¨ç¨‹åºä¸»æ§ï¼Œè´Ÿè´£ DAG è°ƒåº¦ã€ä»»åŠ¡åˆ’åˆ†ã€‚

Executorï¼šåˆ†å¸ƒå¼è®¡ç®—è¿›ç¨‹ï¼ŒçœŸæ­£æ‰§è¡Œ Task å¹¶æä¾›ç¼“å­˜ã€‚

Cluster Managerï¼šé›†ç¾¤èµ„æºè°ƒåº¦å™¨ï¼ˆStandaloneã€YARNã€Mesosã€K8sï¼‰ã€‚

2. ä½œä¸šæ‰§è¡Œæµç¨‹
ç”¨æˆ·ä»£ç è§¦å‘ Actionã€‚

Spark å°†é€»è¾‘è½¬ä¸º DAGï¼ˆæœ‰å‘æ— ç¯å›¾ï¼‰ã€‚

DAG åˆ‡åˆ†ä¸º Stageï¼ˆæ ¹æ®å®½/çª„ä¾èµ–ï¼‰ã€‚

Stage å†…éƒ¨å†åˆ†ä¸ºå¤šä¸ªå¹¶è¡Œçš„ Taskã€‚

Executor æ‰§è¡Œ Taskï¼Œç»“æœæ±‡æ€»ç»™ Driverã€‚

ä¸‰ã€æ ¸å¿ƒç†è®º
1. RDDï¼ˆResilient Distributed Datasetï¼‰
å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†ï¼šä¸å¯å˜ã€åˆ†åŒºåŒ–ï¼Œæ”¯æŒå¹¶è¡Œæ“ä½œã€‚

å®¹é”™æœºåˆ¶ï¼šé€šè¿‡ Lineageï¼ˆè¡€ç¼˜ï¼‰ è¿½è¸ªæ•°æ®æ¥æºï¼Œä¸¢å¤±åˆ†åŒºå¯è‡ªåŠ¨é‡ç®—ã€‚

ä¾èµ–å…³ç³»ï¼š

çª„ä¾èµ–ï¼šçˆ¶åˆ†åŒºåˆ°å­åˆ†åŒºæ˜¯ä¸€å¯¹ä¸€æˆ–å°‘å¯¹ä¸€ï¼ˆå¦‚ map/filterï¼‰ã€‚

å®½ä¾èµ–ï¼šå­åˆ†åŒºä¾èµ–å¤šä¸ªçˆ¶åˆ†åŒºï¼Œéœ€ Shuffleï¼ˆå¦‚ groupBy/joinï¼‰ã€‚

2. DataFrame & Dataset
æ›´é«˜çº§æŠ½è±¡ï¼Œå¸¦ schema çš„è¡¨æ ¼æ•°æ®ã€‚

æ”¯æŒ SQL æŸ¥è¯¢ï¼Œä¾èµ– Catalyst ä¼˜åŒ–å™¨è¿›è¡Œä¼˜åŒ–ã€‚

3. Spark SQL å†…æ ¸
Catalyst ä¼˜åŒ–å™¨ï¼š

è§£æ SQL â†’ é€»è¾‘è®¡åˆ’ â†’ ä¼˜åŒ–ï¼ˆå¸¸é‡æŠ˜å ã€è°“è¯ä¸‹æ¨ç­‰ï¼‰ â†’ ç‰©ç†è®¡åˆ’ã€‚

Tungsten å¼•æ“ï¼š

å†…å­˜ç®¡ç†ï¼šå †å¤–å†…å­˜ã€ç¼“å­˜å¸ƒå±€ä¼˜åŒ–ã€‚

ä»£ç ç”Ÿæˆï¼šWhole-Stage Codegenï¼Œå‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€ã€‚

4. Shuffle åŸç†
Shuffle æ˜¯æ•°æ®é‡åˆ†åŒºçš„è¿‡ç¨‹ï¼šMapper è¾“å‡º â†’ æœ¬åœ°ç£ç›˜å­˜å‚¨ â†’ é€šè¿‡ç½‘ç»œä¼ è¾“ â†’ Reducer æ‹‰å–ã€‚

ä¼˜åŒ–ç‚¹ï¼šæ’åº/å“ˆå¸Œã€ç¼“å†²åŒºã€å‹ç¼©ã€åˆå¹¶å°æ–‡ä»¶ã€‚

5. å†…å­˜ç®¡ç†æœºåˆ¶
Spark å†…å­˜åˆ’åˆ†ï¼šExecutionï¼ˆæ‰§è¡Œç®—å­ã€Shuffleï¼‰+ Storageï¼ˆç¼“å­˜æ•°æ®ï¼‰ã€‚

åŠ¨æ€å…±äº«ï¼šæœªç”¨çš„ Storage å†…å­˜å¯è¢« Execution å ç”¨ï¼Œåä¹‹äº¦ç„¶ã€‚

6. å®¹é”™æœºåˆ¶
è¡€ç¼˜é‡ç®—ï¼šå¤±è´¥åˆ†åŒºæŒ‰ lineage é‡ç®—ã€‚

Checkpointï¼šå°†æ•°æ®å­˜å‚¨åˆ° HDFSï¼Œé¿å… lineage è¿‡é•¿ã€‚

å››ã€æ¨¡å—ä¸åŠŸèƒ½
Spark Coreï¼šRDDã€è°ƒåº¦å™¨ã€å†…å­˜/å­˜å‚¨ç®¡ç†ã€‚

Spark SQLï¼šDataFrame/Datasetã€Catalyst ä¼˜åŒ–ã€Tungsten æ‰§è¡Œå¼•æ“ã€‚

Structured Streamingï¼šæµæ‰¹ä¸€ä½“ï¼Œæ”¯æŒäº‹ä»¶æ—¶é—´ä¸æ°´ä½çº¿ï¼ˆWatermarkï¼‰ã€‚

MLlibï¼šæœºå™¨å­¦ä¹ åº“ï¼Œæ”¯æŒç‰¹å¾å¤„ç†ã€è®­ç»ƒã€Pipelineã€‚

GraphX/GraphFramesï¼šå›¾è®¡ç®— APIã€‚

äº”ã€éƒ¨ç½²ä¸è¿è¡Œæ¨¡å¼
è¿è¡Œæ¨¡å¼ï¼šLocal / Standalone / YARN / Mesos / Kubernetesã€‚

æäº¤æ¨¡å¼ï¼š

Clientï¼šDriver è¿è¡Œåœ¨æäº¤ç«¯ã€‚

Clusterï¼šDriver è¿è¡Œåœ¨é›†ç¾¤ä¸­ã€‚

å…­ã€è°ƒä¼˜ç†è®º
ä½œä¸šçº§åˆ«ï¼šåˆç†è®¾ç½®å¹¶è¡Œåº¦ï¼ˆspark.sql.shuffle.partitionsï¼‰ã€‚

å†…å­˜çº§åˆ«ï¼šExecutor å†…å­˜å¤§å°ã€GC è°ƒä¼˜ã€åºåˆ—åŒ–æ–¹å¼ã€‚

SQL ä¼˜åŒ–ï¼šä½¿ç”¨å†…ç½®å‡½æ•°ã€å¹¿æ’­ joinã€åˆ†åŒºè£å‰ªã€‚

Shuffle ä¼˜åŒ–ï¼šå‡å°‘ shuffle æ¬¡æ•°ã€ä¼˜åŒ–æ•°æ®å€¾æ–œã€‚

ä¸ƒã€å¯¹æ¯”ä¸å®šä½
MapReduceï¼šåŸºäºç£ç›˜ï¼Œæ€§èƒ½æ…¢ï¼›Spark å†…å­˜ä¼˜å…ˆï¼Œæ€§èƒ½æ›´é«˜ã€‚

Flinkï¼šæ›´åå‘ä½å»¶è¿Ÿæµå¤„ç†ï¼›Spark åœ¨æ‰¹å¤„ç†ã€ç”Ÿæ€ä¸Šæ›´æˆç†Ÿã€‚

Trino/Prestoï¼šäº¤äº’å¼ SQL å¼ºï¼›Spark æ›´é€‚åˆå¤æ‚ ETL + MLã€‚

å…«ã€å­¦ä¹ ä¸ç ”ç©¶æ–¹å‘
ç†è®ºå±‚é¢ï¼š

è°ƒåº¦ç³»ç»Ÿï¼ˆDAG â†’ Stage â†’ Taskï¼‰ã€‚

Catalyst ä¼˜åŒ–å™¨ & Tungsten å¼•æ“ã€‚

Shuffle ä¸å†…å­˜ç®¡ç†ã€‚

å®è·µå±‚é¢ï¼š

æ•°æ®å€¾æ–œä¼˜åŒ–ã€‚

æµæ‰¹ä¸€ä½“åŒ–å®è·µã€‚

ä¸æ•°æ®æ¹–ï¼ˆDelta/Hudi/Icebergï¼‰ç»“åˆã€‚

---

**Cursor**

çœ‹åˆ°ä½ åˆ†äº«äº†ä¸€ä¸ªå¾ˆå®Œæ•´çš„Sparkç†è®ºå­¦ä¹ å¤§çº²ï¼è¿™æ˜¯ä¸€ä¸ªéå¸¸ç³»ç»Ÿçš„å­¦ä¹ æ¡†æ¶ï¼Œæ¶µç›–äº†ä»æ ¸å¿ƒç†è®ºåˆ°å®è·µåº”ç”¨çš„å„ä¸ªæ–¹é¢ã€‚

æˆ‘æ³¨æ„åˆ°ä½ çš„é¡¹ç›®æ˜¯ä¸€ä¸ª`data-studio`ï¼Œè®©æˆ‘å…ˆäº†è§£ä¸€ä¸‹ä½ çš„é¡¹ç›®ä¸­æ˜¯å¦æœ‰ç›¸å…³çš„Sparkä»£ç ï¼Œè¿™æ ·å¯ä»¥ç»“åˆå®é™…ä»£ç æ¥å¸®åŠ©ä½ æ›´æ·±å…¥åœ°ç†è§£è¿™äº›ç†è®ºçŸ¥è¯†ã€‚

å¾ˆæ£’çš„Sparkå­¦ä¹ å¤§çº²ï¼æˆ‘å‘ç°ä½ çš„`data-studio`é¡¹ç›®æ­£å¥½æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„Sparkå®è·µå¹³å°ï¼Œæ”¯æŒå¤šç§Sparkä»»åŠ¡ç±»å‹ï¼š

- **Spark JARä»»åŠ¡** - Java/Scala Sparkåº”ç”¨
- **Spark SQLä»»åŠ¡** - SQLæŸ¥è¯¢å¤„ç†  
- **PySparkä»»åŠ¡** - Python Sparkåº”ç”¨

é¡¹ç›®ä¸­æœ‰å®Œæ•´çš„å·¥ä½œæµæ‰§è¡Œå¼•æ“ï¼Œæ­£å¥½å¯¹åº”ä½ å­¦ä¹ å¤§çº²ä¸­çš„"ä½œä¸šæ‰§è¡Œæµç¨‹"éƒ¨åˆ†ã€‚

åŸºäºä½ æä¾›çš„è¿™ä¸ªå¾ˆå®Œæ•´çš„å­¦ä¹ æ¡†æ¶ï¼Œæˆ‘å¯ä»¥å¸®ä½ ï¼š

## æˆ‘èƒ½ä¸ºä½ åšä»€ä¹ˆï¼Ÿ

1. **ğŸ“š åˆ›å»ºè¯¦ç»†çš„å­¦ä¹ ç¬”è®°** - åŸºäºä½ çš„å¤§çº²ï¼Œåˆ¶ä½œæ›´è¯¦ç»†çš„ç†è®ºçŸ¥è¯†æ–‡æ¡£ï¼ŒåŒ…å«ä»£ç ç¤ºä¾‹å’Œå›¾è¡¨

2. **ğŸ”— ç†è®ºä¸å®è·µç»“åˆ** - ç»“åˆä½ é¡¹ç›®ä¸­çš„Sparkä»£ç ï¼Œè§£é‡Šç†è®ºæ¦‚å¿µå¦‚ä½•åœ¨å®é™…ç³»ç»Ÿä¸­å®ç°

3. **ğŸ“Š å¯è§†åŒ–å­¦ä¹ ** - åˆ›å»ºæ¶æ„å›¾ã€æ‰§è¡Œæµç¨‹å›¾ç­‰ï¼Œå¸®åŠ©ç†è§£å¤æ‚æ¦‚å¿µ

4. **ğŸ’¡ å®è·µç¤ºä¾‹** - åŸºäºä½ çš„å­¦ä¹ é‡ç‚¹ï¼Œåˆ›å»ºä¸€äº›Sparkä»£ç ç¤ºä¾‹æ¥æ¼”ç¤ºæ ¸å¿ƒåŸç†

5. **ğŸ¯ é’ˆå¯¹æ€§æ·±å…¥** - é€‰æ‹©ä½ æœ€æ„Ÿå…´è¶£çš„æŸä¸ªæ–¹é¢ï¼ˆå¦‚Catalystä¼˜åŒ–å™¨ã€Shuffleæœºåˆ¶ç­‰ï¼‰è¿›è¡Œæ·±å…¥å­¦ä¹ 

ä½ å¸Œæœ›æˆ‘é‡ç‚¹å¸®åŠ©ä½ å“ªä¸ªæ–¹é¢ï¼Ÿæˆ–è€…ä½ æœ‰ä»€ä¹ˆå…·ä½“çš„å­¦ä¹ ç›®æ ‡å—ï¼Ÿæ¯”å¦‚ï¼š
- æƒ³æ·±å…¥ç†è§£æŸä¸ªç‰¹å®šæ¨¡å—ï¼ˆå¦‚RDDã€DataFrameã€Catalystç­‰ï¼‰
- æƒ³äº†è§£å¦‚ä½•åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨è¿™äº›ç†è®º
- æƒ³åˆ›å»ºå­¦ä¹ èµ„æ–™ä¾›æ—¥åå¤ä¹ 

è¯·å‘Šè¯‰æˆ‘ä½ çš„å…·ä½“éœ€æ±‚ï¼

---

**User**

åˆ›å»ºè¯¦ç»†çš„å­¦ä¹ ç¬”è®°ä¾›å­¦ä¹ å’Œå›é¡¾ä½¿ç”¨ï¼Œé’ˆå¯¹å¤šæ–¹é¢å‡ ä¹æ‰€æœ‰æ¨¡å—çš„æ·±å…¥å­¦ä¹ äº†è§£

---

**Cursor**

æˆ‘æ¥ä¸ºä½ åˆ›å»ºä¸€å¥—å…¨é¢æ·±å…¥çš„Sparkå­¦ä¹ ç¬”è®°ï¼è¿™å°†æ˜¯ä¸€ä¸ªcomprehensiveçš„å­¦ä¹ èµ„æ–™ï¼Œæ¶µç›–Sparkçš„å„ä¸ªæ ¸å¿ƒæ¨¡å—ã€‚

```markdown
# Sparkæ ¸å¿ƒç†è®ºä¸æ¶æ„æ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [Sparkè®¾è®¡æ€æƒ³ä¸å®šä½](#1-sparkè®¾è®¡æ€æƒ³ä¸å®šä½)
2. [æ•´ä½“æ¶æ„è¯¦è§£](#2-æ•´ä½“æ¶æ„è¯¦è§£) 
3. [RDDæ ¸å¿ƒç†è®º](#3-rddæ ¸å¿ƒç†è®º)
4. [DataFrameä¸Dataset](#4-dataframeä¸dataset)
5. [ä»£ç å®ä¾‹åˆ†æ](#5-ä»£ç å®ä¾‹åˆ†æ)

---

## 1. Sparkè®¾è®¡æ€æƒ³ä¸å®šä½

### 1.1 å¤§æ•°æ®å¤„ç†ç—›ç‚¹åˆ†æ

**MapReduceçš„å±€é™æ€§ï¼š**
- **ç£ç›˜I/Oå¯†é›†**ï¼šæ¯ä¸ªMapReduceä½œä¸šéƒ½éœ€è¦å°†ä¸­é—´ç»“æœå†™å…¥HDFSï¼Œå¯¼è‡´å¤§é‡ç£ç›˜è¯»å†™
- **é«˜å»¶è¿Ÿ**ï¼šè¿­ä»£ç®—æ³•éœ€è¦å¯åŠ¨å¤šä¸ªMapReduceä½œä¸šï¼Œæ¯æ¬¡éƒ½è¦é‡æ–°è¯»å–æ•°æ®
- **ç¼–ç¨‹å¤æ‚æ€§**ï¼šéœ€è¦å°†é€»è¾‘æ‹†åˆ†ä¸ºMapå’ŒReduceé˜¶æ®µï¼Œè¡¨è¾¾å¤æ‚ç®—æ³•å›°éš¾
- **èµ„æºåˆ©ç”¨ç‡ä½**ï¼šæ¯ä¸ªä½œä¸šç‹¬ç«‹ç”³è¯·èµ„æºï¼Œæ— æ³•å¤ç”¨

### 1.2 Sparkæ ¸å¿ƒæ€è·¯

```
ä¼ ç»ŸMapReduce:
Input â†’ Map â†’ Write to Disk â†’ Reduce â†’ Output
         â†“
     ç£ç›˜I/Oç“¶é¢ˆ

Sparkå†…å­˜è®¡ç®—:
Input â†’ Transformation â†’ Transformation â†’ Action â†’ Output
               â†“              â†“
           å†…å­˜ç¼“å­˜       è¡€ç¼˜å…³ç³»è¿½è¸ª
```

**æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š**

1. **å†…å­˜ä¼˜å…ˆè®¡ç®—**
   - å°†æ•°æ®å°½é‡ä¿å­˜åœ¨å†…å­˜ä¸­ï¼Œé¿å…é¢‘ç¹çš„ç£ç›˜I/O
   - æ”¯æŒæ•°æ®åœ¨å†…å­˜ä¸­çš„å¤šæ¬¡å¤ç”¨

2. **ç»Ÿä¸€è®¡ç®—å¼•æ“**
   - ä¸€ä¸ªæ¡†æ¶æ”¯æŒæ‰¹å¤„ç†ã€äº¤äº’å¼æŸ¥è¯¢ã€æµå¤„ç†ã€æœºå™¨å­¦ä¹ ã€å›¾è®¡ç®—
   - é™ä½å­¦ä¹ æˆæœ¬å’Œè¿ç»´å¤æ‚åº¦

3. **å¼¹æ€§ä¸å®¹é”™**
   - é€šè¿‡RDDè¡€ç¼˜ï¼ˆLineageï¼‰å®ç°è‡ªåŠ¨å®¹é”™
   - æ— éœ€é¢å¤–çš„æ£€æŸ¥ç‚¹æœºåˆ¶

4. **æ‡’æƒ°æ‰§è¡Œï¼ˆLazy Evaluationï¼‰**
   - åªæœ‰é‡åˆ°Actionæ“ä½œæ‰çœŸæ­£æ‰§è¡Œè®¡ç®—
   - ä¼˜åŒ–å™¨å¯ä»¥åˆ†ææ•´ä¸ªè®¡ç®—å›¾è¿›è¡Œä¼˜åŒ–

---

## 2. æ•´ä½“æ¶æ„è¯¦è§£

### 2.1 æ¶æ„ç»„ä»¶

```mermaid
graph TB
    subgraph "Sparkåº”ç”¨æ¶æ„"
        Driver["Driver Program<br/>- SparkContext<br/>- DAGè°ƒåº¦<br/>- ä»»åŠ¡åˆ†å‘"]
        CM["Cluster Manager<br/>- Standalone<br/>- YARN<br/>- Mesos<br/>- Kubernetes"]
        
        subgraph "Worker Node 1"
            E1["Executor<br/>- ä»»åŠ¡æ‰§è¡Œ<br/>- æ•°æ®ç¼“å­˜<br/>- è®¡ç®—èµ„æº"]
        end
        
        subgraph "Worker Node 2" 
            E2["Executor<br/>- ä»»åŠ¡æ‰§è¡Œ<br/>- æ•°æ®ç¼“å­˜<br/>- è®¡ç®—èµ„æº"]
        end
        
        subgraph "Worker Node N"
            EN["Executor<br/>- ä»»åŠ¡æ‰§è¡Œ<br/>- æ•°æ®ç¼“å­˜<br/>- è®¡ç®—èµ„æº"]
        end
    end
    
    Driver --> CM
    CM --> E1
    CM --> E2
    CM --> EN
    Driver -.-> E1
    Driver -.-> E2
    Driver -.-> EN
```

### 2.2 æ ¸å¿ƒç»„ä»¶èŒè´£

#### Driverï¼ˆé©±åŠ¨å™¨ï¼‰
```scala
// Driverçš„æ ¸å¿ƒèŒè´£
class SparkDriver {
  val sparkContext: SparkContext // åº”ç”¨å…¥å£
  
  // 1. DAGæ„å»ºä¸è°ƒåº¦
  def buildDAG(rdd: RDD): DAG = {
    // åˆ†æRDDä¾èµ–å…³ç³»ï¼Œæ„å»ºæœ‰å‘æ— ç¯å›¾
  }
  
  // 2. Stageåˆ’åˆ†
  def createStages(dag: DAG): List[Stage] = {
    // æ ¹æ®å®½ä¾èµ–åˆ‡åˆ†Stage
  }
  
  // 3. ä»»åŠ¡åˆ†å‘
  def submitTasks(stage: Stage): Unit = {
    // å°†Stageå†…çš„ä»»åŠ¡å‘é€ç»™Executoræ‰§è¡Œ
  }
}
```

#### Executorï¼ˆæ‰§è¡Œå™¨ï¼‰
```scala
class SparkExecutor {
  // 1. ä»»åŠ¡æ‰§è¡Œ
  def runTask(task: Task): TaskResult = {
    // æ‰§è¡Œå…·ä½“çš„è®¡ç®—é€»è¾‘
  }
  
  // 2. æ•°æ®ç¼“å­˜ç®¡ç†
  val blockManager: BlockManager // å†…å­˜å’Œç£ç›˜å­˜å‚¨ç®¡ç†
  
  // 3. Shuffleæ•°æ®ç®¡ç†
  val shuffleManager: ShuffleManager // Shuffleè¯»å†™
}
```

### 2.3 ä½œä¸šæ‰§è¡Œæµç¨‹

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·ä»£ç 
    participant Driver as Driver
    participant CM as Cluster Manager
    participant Executor as Executor
    
    User->>Driver: æäº¤Sparkåº”ç”¨
    Driver->>CM: ç”³è¯·èµ„æº
    CM->>Executor: å¯åŠ¨Executorè¿›ç¨‹
    
    User->>Driver: æ‰§è¡ŒActionæ“ä½œ
    Driver->>Driver: æ„å»ºDAG
    Driver->>Driver: åˆ’åˆ†Stage
    Driver->>Driver: ç”ŸæˆTask
    
    loop æ¯ä¸ªStage
        Driver->>Executor: æäº¤TaskSet
        Executor->>Executor: æ‰§è¡ŒTask
        Executor->>Driver: è¿”å›ç»“æœ
    end
    
    Driver->>User: è¿”å›æœ€ç»ˆç»“æœ
```

**è¯¦ç»†æ‰§è¡Œæ­¥éª¤ï¼š**

1. **åº”ç”¨å¯åŠ¨**
   - ç”¨æˆ·æäº¤Sparkåº”ç”¨
   - Driverå¯åŠ¨ï¼Œåˆ›å»ºSparkContext
   - å‘Cluster Managerç”³è¯·èµ„æº

2. **èµ„æºåˆ†é…**
   - Cluster Manageråˆ†é…WorkerèŠ‚ç‚¹
   - å¯åŠ¨Executorè¿›ç¨‹
   - Driverä¸Executorå»ºç«‹é€šä¿¡

3. **DAGæ„å»º**ï¼ˆActionè§¦å‘ï¼‰
   - åˆ†æRDDçš„ä¾èµ–å…³ç³»
   - æ„å»ºæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰

4. **Stageåˆ’åˆ†**
   - æ ¹æ®å®½ä¾èµ–ï¼ˆShuffleï¼‰åˆ‡åˆ†Stage
   - æ¯ä¸ªStageåŒ…å«å¯å¹¶è¡Œæ‰§è¡Œçš„Task

5. **ä»»åŠ¡è°ƒåº¦**
   - å°†Stageå†…çš„Taskåˆ†å‘ç»™Executor
   - Executoræ‰§è¡ŒTaskå¹¶è¿”å›ç»“æœ

6. **ç»“æœæ”¶é›†**
   - Driveræ”¶é›†å„Taskçš„æ‰§è¡Œç»“æœ
   - åˆå¹¶åè¿”å›ç»™ç”¨æˆ·ç¨‹åº

---

## 3. RDDæ ¸å¿ƒç†è®º

### 3.1 RDDæ¦‚å¿µä¸ç‰¹æ€§

**RDDï¼ˆResilient Distributed Datasetï¼‰å¼¹æ€§åˆ†å¸ƒå¼æ•°æ®é›†**

```scala
// RDDçš„æœ¬è´¨æ˜¯ä¸€ä¸ªæŠ½è±¡ç±»
abstract class RDD[T] {
  // 1. åˆ†åŒºåˆ—è¡¨
  def getPartitions: Array[Partition]
  
  // 2. åˆ†åŒºè®¡ç®—å‡½æ•°  
  def compute(partition: Partition, context: TaskContext): Iterator[T]
  
  // 3. ä¾èµ–å…³ç³»
  def getDependencies: Seq[Dependency[_]]
  
  // 4. åˆ†åŒºå™¨ï¼ˆå¯é€‰ï¼‰
  def partitioner: Option[Partitioner] = None
  
  // 5. æœ€ä½³ä½ç½®ï¼ˆå¯é€‰ï¼‰
  def getPreferredLocations(partition: Partition): Seq[String] = Nil
}
```

### 3.2 RDDäº”å¤§ç‰¹æ€§

#### 1. **åˆ†åŒºåˆ—è¡¨ï¼ˆA list of partitionsï¼‰**
```scala
// æ•°æ®è¢«åˆ’åˆ†ä¸ºå¤šä¸ªåˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºå¯ä»¥åœ¨ä¸åŒèŠ‚ç‚¹ä¸Šå¹¶è¡Œå¤„ç†
val rdd = sparkContext.parallelize(List(1,2,3,4,5,6), numPartitions = 3)
// åˆ†åŒº0: [1,2]
// åˆ†åŒº1: [3,4] 
// åˆ†åŒº2: [5,6]
```

#### 2. **è®¡ç®—å‡½æ•°ï¼ˆA function for computing each splitï¼‰**
```scala
// æ¯ä¸ªåˆ†åŒºéƒ½æœ‰å¯¹åº”çš„è®¡ç®—å‡½æ•°
val mappedRDD = rdd.map(x => x * 2)
// åˆ†åŒº0è®¡ç®—: [1,2] => [2,4]
// åˆ†åŒº1è®¡ç®—: [3,4] => [6,8]
// åˆ†åŒº2è®¡ç®—: [5,6] => [10,12]
```

#### 3. **ä¾èµ–å…³ç³»ï¼ˆDependenciesï¼‰**
```scala
// RDDä¹‹é—´çš„ä¾èµ–å…³ç³»ç”¨äºå®¹é”™å’Œè°ƒåº¦ä¼˜åŒ–
val rdd1 = sparkContext.parallelize(List(1,2,3,4))
val rdd2 = rdd1.map(x => x * 2)  // çª„ä¾èµ–
val rdd3 = rdd2.groupBy(x => x % 2) // å®½ä¾èµ–
```

#### 4. **åˆ†åŒºå™¨ï¼ˆPartitioner for Key-Value RDDsï¼‰**
```scala
// ç”¨äºæ§åˆ¶Key-Value RDDçš„æ•°æ®åˆ†å¸ƒ
val kvRDD = sparkContext.parallelize(List(("a",1),("b",2),("c",3)))
val partitionedRDD = kvRDD.partitionBy(new HashPartitioner(2))
```

#### 5. **æœ€ä½³ä½ç½®ï¼ˆPreferred locationsï¼‰**
```scala
// æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–ï¼Œå°†è®¡ç®—è°ƒåº¦åˆ°æ•°æ®æ‰€åœ¨çš„èŠ‚ç‚¹
// HDFSæ–‡ä»¶çš„blockæ‰€åœ¨èŠ‚ç‚¹å°±æ˜¯è¯¥åˆ†åŒºçš„æœ€ä½³ä½ç½®
```

### 3.3 ä¾èµ–å…³ç³»è¯¦è§£

#### çª„ä¾èµ–ï¼ˆNarrow Dependenciesï¼‰
```mermaid
graph LR
    subgraph "çˆ¶RDD"
        P1[åˆ†åŒº1]
        P2[åˆ†åŒº2] 
        P3[åˆ†åŒº3]
    end
    
    subgraph "å­RDD"
        C1[åˆ†åŒº1]
        C2[åˆ†åŒº2]
        C3[åˆ†åŒº3]
    end
    
    P1 --> C1
    P2 --> C2
    P3 --> C3
```

**ç‰¹ç‚¹ï¼š**
- çˆ¶åˆ†åŒºåˆ°å­åˆ†åŒºæ˜¯ä¸€å¯¹ä¸€æˆ–å¤šå¯¹ä¸€
- ä¸éœ€è¦Shuffleï¼Œæ•°æ®åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹
- å¯ä»¥pipelineæ‰§è¡Œï¼Œæ€§èƒ½é«˜
- å¤±è´¥æ¢å¤ç®€å•ï¼Œåªéœ€é‡ç®—å¯¹åº”çˆ¶åˆ†åŒº

**å¸¸è§æ“ä½œï¼š** map, filter, union, mapPartitions

```scala
// çª„ä¾èµ–ç¤ºä¾‹
val rdd1 = sparkContext.parallelize(1 to 10, 4)
val rdd2 = rdd1.map(_ * 2)        // çª„ä¾èµ–ï¼šä¸€å¯¹ä¸€
val rdd3 = rdd1.filter(_ > 5)     // çª„ä¾èµ–ï¼šä¸€å¯¹ä¸€  
val rdd4 = rdd2.union(rdd3)       // çª„ä¾èµ–ï¼šå¤šå¯¹ä¸€
```

#### å®½ä¾èµ–ï¼ˆWide Dependencies/Shuffle Dependenciesï¼‰
```mermaid
graph LR
    subgraph "çˆ¶RDD"
        P1[åˆ†åŒº1]
        P2[åˆ†åŒº2]
        P3[åˆ†åŒº3]
    end
    
    subgraph "å­RDD"
        C1[åˆ†åŒº1]
        C2[åˆ†åŒº2]
    end
    
    P1 --> C1
    P1 --> C2
    P2 --> C1
    P2 --> C2
    P3 --> C1
    P3 --> C2
```

**ç‰¹ç‚¹ï¼š**
- å­åˆ†åŒºä¾èµ–å¤šä¸ªçˆ¶åˆ†åŒº
- éœ€è¦Shuffleï¼Œæ•°æ®éœ€è¦è·¨èŠ‚ç‚¹ä¼ è¾“
- æ€§èƒ½å¼€é”€å¤§ï¼Œæ˜¯ä¼˜åŒ–é‡ç‚¹
- å¤±è´¥æ¢å¤å¤æ‚ï¼Œéœ€è¦é‡ç®—å¤šä¸ªçˆ¶åˆ†åŒº

**å¸¸è§æ“ä½œï¼š** groupByKey, reduceByKey, join, distinct

```scala
// å®½ä¾èµ–ç¤ºä¾‹
val rdd1 = sparkContext.parallelize(List(("a",1),("b",2),("a",3),("b",4)), 4)
val rdd2 = rdd1.groupByKey()      // å®½ä¾èµ–ï¼šéœ€è¦Shuffle
val rdd3 = rdd1.reduceByKey(_ + _) // å®½ä¾èµ–ï¼šä½†æœ‰é¢„èšåˆä¼˜åŒ–
```

### 3.4 å®¹é”™æœºåˆ¶ï¼šè¡€ç¼˜å…³ç³»ï¼ˆLineageï¼‰

```scala
// RDDè¡€ç¼˜ç¤ºä¾‹
val textFile = sparkContext.textFile("hdfs://data.txt")  // RDD_1
val words = textFile.flatMap(_.split(" "))                // RDD_2 (ä¾èµ–RDD_1)
val pairs = words.map(word => (word, 1))                  // RDD_3 (ä¾èµ–RDD_2)  
val wordCounts = pairs.reduceByKey(_ + _)                 // RDD_4 (ä¾èµ–RDD_3)

// è¡€ç¼˜å…³ç³»é“¾ï¼šRDD_1 <- RDD_2 <- RDD_3 <- RDD_4
```

**å®¹é”™åŸç†ï¼š**
1. **è‡ªåŠ¨é‡ç®—**ï¼šå½“æŸä¸ªRDDåˆ†åŒºä¸¢å¤±æ—¶ï¼Œæ ¹æ®è¡€ç¼˜å…³ç³»è‡ªåŠ¨é‡æ–°è®¡ç®—
2. **æœ€å°æ¢å¤**ï¼šåªé‡ç®—ä¸¢å¤±çš„åˆ†åŒºï¼Œä¸å½±å“å…¶ä»–å¥åº·åˆ†åŒº  
3. **è¡€ç¼˜æˆªæ–­**ï¼šé€šè¿‡ç¼“å­˜æˆ–æ£€æŸ¥ç‚¹æˆªæ–­è¿‡é•¿çš„è¡€ç¼˜é“¾

```scala
// å®¹é”™ä¼˜åŒ–
val importantRDD = pairs.cache()  // ç¼“å­˜é‡è¦çš„ä¸­é—´ç»“æœ
wordCounts.checkpoint()           // è®¾ç½®æ£€æŸ¥ç‚¹æˆªæ–­è¡€ç¼˜
```

### 3.5 RDDæ“ä½œåˆ†ç±»

#### Transformationï¼ˆè½¬æ¢æ“ä½œï¼‰
**ç‰¹ç‚¹ï¼š** æ‡’æƒ°æ‰§è¡Œï¼Œè¿”å›æ–°çš„RDD

```scala
// å¸¸ç”¨Transformation
val rdd = sparkContext.parallelize(1 to 10)

// å…ƒç´ è½¬æ¢
val mapped = rdd.map(x => x * 2)
val filtered = rdd.filter(x => x > 5)
val flatMapped = rdd.flatMap(x => List(x, x*2))

// é‡‡æ ·
val sampled = rdd.sample(false, 0.5)

// é›†åˆè¿ç®—  
val rdd2 = sparkContext.parallelize(5 to 15)
val unioned = rdd.union(rdd2)
val intersected = rdd.intersection(rdd2)
val distinct = rdd.distinct()

// Key-Valueæ“ä½œ
val kvRDD = rdd.map(x => (x % 3, x))
val grouped = kvRDD.groupByKey()
val reduced = kvRDD.reduceByKey(_ + _)
```

#### Actionï¼ˆè¡ŒåŠ¨æ“ä½œï¼‰
**ç‰¹ç‚¹ï¼š** ç«‹å³æ‰§è¡Œï¼Œè§¦å‘å®é™…è®¡ç®—

```scala
// æ”¶é›†ç»“æœ
val result = rdd.collect()        // æ”¶é›†æ‰€æœ‰å…ƒç´ åˆ°Driver
val first = rdd.first()           // è·å–ç¬¬ä¸€ä¸ªå…ƒç´ 
val take = rdd.take(5)           // è·å–å‰5ä¸ªå…ƒç´ 

// èšåˆè®¡ç®—
val count = rdd.count()           // å…ƒç´ ä¸ªæ•°
val sum = rdd.reduce(_ + _)       // èšåˆæ“ä½œ
val aggregate = rdd.aggregate(0)(_ + _, _ + _) // è‡ªå®šä¹‰èšåˆ

// è¾“å‡ºæ“ä½œ
rdd.saveAsTextFile("hdfs://output") // ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
rdd.foreach(println)              // å¯¹æ¯ä¸ªå…ƒç´ æ‰§è¡Œæ“ä½œ
```

---

## 4. DataFrameä¸Dataset

### 4.1 ä»RDDåˆ°DataFrameçš„æ¼”è¿›

```mermaid
graph TB
    RDD["RDD<br/>â€¢ ç±»å‹å®‰å…¨<br/>â€¢ æ— Schema<br/>â€¢ æ— ä¼˜åŒ–å™¨<br/>â€¢ åºåˆ—åŒ–å¼€é”€å¤§"] 
    DF["DataFrame<br/>â€¢ æœ‰Schema<br/>â€¢ SQLæ”¯æŒ<br/>â€¢ Catalystä¼˜åŒ–<br/>â€¢ åˆ—å¼å­˜å‚¨ä¼˜åŒ–"]
    DS["Dataset<br/>â€¢ ç±»å‹å®‰å…¨+Schema<br/>â€¢ ç¼–è¯‘æ—¶æ£€æŸ¥<br/>â€¢ Catalystä¼˜åŒ–<br/>â€¢ ä»£ç ç”Ÿæˆ"]
    
    RDD --> DF
    DF --> DS
```

### 4.2 DataFrameè¯¦è§£

**DataFrame = RDD + Schema + Catalystä¼˜åŒ–å™¨**

```scala
// DataFrameåˆ›å»º
import org.apache.spark.sql.SparkSession

val spark = SparkSession.builder()
  .appName("DataFrame Example")
  .getOrCreate()

// ä»ç»“æ„åŒ–æ•°æ®åˆ›å»º
val df = spark.read
  .option("header", "true")
  .option("inferSchema", "true") 
  .csv("path/to/data.csv")

// ä»RDDåˆ›å»º
import spark.implicits._
case class Person(name: String, age: Int, city: String)
val peopleDF = rdd.map(row => Person(row(0), row(1).toInt, row(2))).toDF()
```

**DataFrame APIç¤ºä¾‹ï¼š**
```scala
// SQLé£æ ¼æ“ä½œ
df.select("name", "age")
  .where($"age" > 18)
  .groupBy("city")
  .count()
  .orderBy($"count".desc)

// ç­‰ä»·çš„SQLæŸ¥è¯¢
df.createOrReplaceTempView("people")
spark.sql("""
  SELECT city, count(*) as count
  FROM people 
  WHERE age > 18
  GROUP BY city
  ORDER BY count DESC
""")
```

### 4.3 Datasetè¯¦è§£

**Dataset = DataFrame + å¼ºç±»å‹**

```scala
// Datasetå®šä¹‰
case class Person(name: String, age: Int, salary: Double)

// Datasetåˆ›å»º
val ds: Dataset[Person] = spark.read
  .json("people.json")
  .as[Person]  // è½¬æ¢ä¸ºå¼ºç±»å‹Dataset

// ç±»å‹å®‰å…¨æ“ä½œ
val adults = ds.filter(_.age >= 18)  // ç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
val highEarners = ds.filter(_.salary > 50000)

// èšåˆæ“ä½œ
val avgSalary = ds
  .groupByKey(_.age / 10)  // æŒ‰å¹´é¾„æ®µåˆ†ç»„
  .agg(avg($"salary"))     // å¹³å‡å·¥èµ„
```

### 4.4 ä¸‰ç§APIå¯¹æ¯”

| ç‰¹æ€§ | RDD | DataFrame | Dataset |
|------|-----|-----------|---------|
| ç±»å‹å®‰å…¨ | ç¼–è¯‘æ—¶ | è¿è¡Œæ—¶ | ç¼–è¯‘æ—¶ |
| æ€§èƒ½ä¼˜åŒ– | æ—  | Catalystä¼˜åŒ– | Catalystä¼˜åŒ– |
| åºåˆ—åŒ– | Javaåºåˆ—åŒ– | åˆ—å¼å­˜å‚¨ | åˆ—å¼å­˜å‚¨ |
| ä»£ç ç”Ÿæˆ | æ—  | æ”¯æŒ | æ”¯æŒ |
| åƒåœ¾å›æ”¶ | å½±å“å¤§ | å †å¤–å†…å­˜ | å †å¤–å†…å­˜ |
| APIæ˜“ç”¨æ€§ | å‡½æ•°å¼ | SQL + DSL | ç±»å‹å®‰å…¨DSL |

---

## 5. ä»£ç å®ä¾‹åˆ†æ

åŸºäºé¡¹ç›®ä¸­çš„å®é™…ä»£ç ï¼Œæˆ‘ä»¬æ¥åˆ†æSparkçš„å®é™…åº”ç”¨ï¼š

### 5.1 é¡¹ç›®ä¸­çš„Sparké…ç½®ç»“æ„

```java
// æ¥è‡ªé¡¹ç›®ï¼šSparkJarConfig.java
@Data
public class SparkJarConfig {
    private String sparkVersion;           // Sparkç‰ˆæœ¬
    private ResourceConfig mainResource;   // ä¸»è¦èµ„æºï¼ˆjaråŒ…ï¼‰
    private String mainClass;              // å…¥å£ç±»
    private String sparkProperties;        // Sparké…ç½®å‚æ•°
    private String programArgument;        // ç¨‹åºå‚æ•°
    private List<ResourceConfig> otherResources; // å…¶ä»–ä¾èµ–èµ„æº
    private Boolean isStreamingTask;       // æ˜¯å¦ä¸ºæµå¤„ç†ä»»åŠ¡
    private YarnQueueInfo yarnQueueInfo;   // YARNé˜Ÿåˆ—ä¿¡æ¯
}
```

**å®é™…é…ç½®ç¤ºä¾‹ï¼š**
```json
{
  "sparkVersion": "3.2.0",
  "mainClass": "com.shopee.di.example.SparkWordCount",
  "sparkProperties": "--conf spark.executor.memory=4g --conf spark.executor.cores=2",
  "programArgument": "hdfs://input/data.txt hdfs://output/",
  "isStreamingTask": false,
  "yarnQueueInfo": {
    "queueName": "data-processing",
    "priority": "NORMAL"
  }
}
```

### 5.2 ä»»åŠ¡æ‰§è¡Œæµç¨‹åˆ†æ

```java
// æ¥è‡ªé¡¹ç›®ï¼šWorkflowExecutionServiceImpl.java (ç®€åŒ–ç‰ˆ)
public class WorkflowExecutionService {
    
    public Long submit(Long workflowId, String operator, Set<Long> selectedIds) {
        // 1. è·å–å·¥ä½œæµå®šä¹‰
        AssetDO assetDO = assetMapper.getById(workflowId);
        
        // 2. æ„å»ºæ‰§è¡Œè®¡åˆ’
        List<AssetDO> allNodes = assetSearchService.getAllWorkflowTaskNodes(workflowId);
        
        // 3. åˆ›å»ºæ‰§è¡Œè®°å½•
        ExecutionDO executionDO = buildExecutionDO(assetDO, operator);
        executionService.insert(executionDO);
        
        // 4. æ„å»ºä»»åŠ¡åˆ—è¡¨
        List<TaskDO> taskDOS = buildTaskDOList(allNodes, executionDO);
        
        // 5. æäº¤åˆå§‹ä»»åŠ¡ï¼ˆæ— ä¸Šæ¸¸ä¾èµ–çš„ä»»åŠ¡ï¼‰
        Set<Long> nodesWithoutUpStream = getNodesWithoutUpStream(allIds, relations);
        for (TaskDO taskDO : firstSubmitTasks) {
            String scheduleInstanceCode = adHocWrapper.submit(taskDO);
            updateTaskStatus(taskDO.getId(), ExecutionStatusEnum.WAIT);
        }
        
        return executionDO.getId();
    }
}
```

### 5.3 Sparkä»»åŠ¡é…ç½®æ„å»º

```java
// åŸºäºé¡¹ç›®ä»£ç çš„Sparkä»»åŠ¡é…ç½®æ„å»ºé€»è¾‘
switch (assetTypeEnum) {
    case SPARK_JAR:
        SparkJarConfig sparkJarConfig = JsonUtils.parseObject(
            assetDetailDO.getSpecificConfiguration(), 
            SparkJarConfig.class
        );
        
        // æ„å»ºSparké…ç½®
        String config = StringUtils.trim(sparkJarConfig.getSparkProperties()) 
                      + " --class " + sparkJarConfig.getMainClass();
        
        executionInfo.setConfig(config);
        executionInfo.setSparkVersion(sparkJarConfig.getSparkVersion());
        executionInfo.setMainResource(buildMainResource(sparkJarConfig.getMainResource()));
        executionInfo.setAttachedResources(buildAttachedResource(sparkJarConfig.getOtherResources()));
        
        // å¤„ç†ç¨‹åºå‚æ•°å’Œå˜é‡æ›¿æ¢
        if (StringUtils.isNotBlank(sparkJarConfig.getProgramArgument())) {
            Map<String, Object> taskVariableMap = fillTaskVariables(variableMap, assetDetailDO);
            executionInfo.setProgramArguments(
                StringReplaceUtils.replacedVariable(
                    sparkJarConfig.getProgramArgument(), 
                    taskVariableMap
                )
            );
        }
        break;
}
```

### 5.4 å®é™…Sparkç¨‹åºç¤ºä¾‹

åŸºäºé¡¹ç›®ç»“æ„ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªå…¸å‹çš„Sparkåº”ç”¨ç¤ºä¾‹ï¼š

```scala
// å…¸å‹çš„Spark Jaråº”ç”¨ç¨‹åºç»“æ„
package com.shopee.di.example

import org.apache.spark.sql.SparkSession

object SparkWordCount {
  def main(args: Array[String]): Unit = {
    // 1. åˆ›å»ºSparkSession
    val spark = SparkSession.builder()
      .appName("WordCount Example")
      .getOrCreate()
    
    val sc = spark.sparkContext
    val inputPath = args(0)
    val outputPath = args(1)
    
    // 2. è¯»å–æ•°æ® (RDDæ–¹å¼)
    val textFile = sc.textFile(inputPath)
    
    // 3. æ•°æ®å¤„ç†
    val wordCounts = textFile
      .flatMap(line => line.split("\\s+"))  // åˆ†è¯
      .map(word => (word, 1))               // æ˜ å°„ä¸º(word, 1)
      .reduceByKey(_ + _)                   // æŒ‰keyèšåˆ
    
    // 4. ä¿å­˜ç»“æœ
    wordCounts.saveAsTextFile(outputPath)
    
    // 5. åœæ­¢SparkContext
    spark.stop()
  }
}
```

```scala
// DataFrame/Datasetæ–¹å¼å®ç°
object SparkWordCountSQL {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("WordCount SQL Example")
      .getOrCreate()
    
    import spark.implicits._
    
    // 1. è¯»å–æ•°æ®ä¸ºDataFrame
    val textDF = spark.read.text(args(0))
    
    // 2. SQLæ–¹å¼å¤„ç†
    textDF.createOrReplaceTempView("text_table")
    
    val wordCountDF = spark.sql("""
      SELECT word, count(*) as count
      FROM (
        SELECT explode(split(value, '\\s+')) as word
        FROM text_table
      ) words
      WHERE word != ''
      GROUP BY word
      ORDER BY count DESC
    """)
    
    // 3. ä¿å­˜ç»“æœ
    wordCountDF.write.mode("overwrite").csv(args(1))
    
    spark.stop()
  }
}
```

---

## æ€»ç»“

æœ¬ç« æ·±å…¥è®²è§£äº†Sparkçš„æ ¸å¿ƒç†è®ºåŸºç¡€ï¼š

1. **è®¾è®¡æ€æƒ³**ï¼šå†…å­˜è®¡ç®—ã€ç»Ÿä¸€å¼•æ“ã€å¼¹æ€§å®¹é”™ã€æ‡’æƒ°æ‰§è¡Œ
2. **æ¶æ„ç»„ä»¶**ï¼šDriverã€Executorã€Cluster Managerçš„èŒè´£ä¸äº¤äº’
3. **RDDç†è®º**ï¼šäº”å¤§ç‰¹æ€§ã€ä¾èµ–å…³ç³»ã€è¡€ç¼˜å®¹é”™æœºåˆ¶
4. **é«˜çº§æŠ½è±¡**ï¼šDataFrameå’ŒDatasetçš„ä¼˜åŠ¿ä¸ä½¿ç”¨åœºæ™¯
5. **å®é™…åº”ç”¨**ï¼šç»“åˆé¡¹ç›®ä»£ç ç†è§£ç†è®ºåœ¨å®è·µä¸­çš„ä½“ç°

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ Sparkçš„æ‰§è¡Œå¼•æ“ï¼ŒåŒ…æ‹¬DAGè°ƒåº¦ã€Stageåˆ’åˆ†å’ŒTaskæ‰§è¡Œæœºåˆ¶ã€‚

---

## æ¨èé˜…è¯»
- [Sparkå®˜æ–¹æ–‡æ¡£ - Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)
- [Sparkæºç åˆ†æ - RDDå®ç°](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala)
- ã€ŠSparkæƒå¨æŒ‡å—ã€‹ç¬¬3-5ç« 
```

```markdown
# Sparkæ‰§è¡Œå¼•æ“æ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [æ‰§è¡Œå¼•æ“æ¦‚è¿°](#1-æ‰§è¡Œå¼•æ“æ¦‚è¿°)
2. [DAGè°ƒåº¦å™¨è¯¦è§£](#2-dagè°ƒåº¦å™¨è¯¦è§£)
3. [Stageåˆ’åˆ†æœºåˆ¶](#3-stageåˆ’åˆ†æœºåˆ¶)
4. [Taskè°ƒåº¦æ‰§è¡Œ](#4-taskè°ƒåº¦æ‰§è¡Œ)
5. [è°ƒåº¦å™¨ç»„ä»¶æ¶æ„](#5-è°ƒåº¦å™¨ç»„ä»¶æ¶æ„)
6. [å®ä¾‹åˆ†æ](#6-å®ä¾‹åˆ†æ)

---

## 1. æ‰§è¡Œå¼•æ“æ¦‚è¿°

### 1.1 Sparkæ‰§è¡Œå¼•æ“æ ¸å¿ƒèŒè´£

```mermaid
graph TB
    subgraph "Sparkæ‰§è¡Œå¼•æ“æ¶æ„"
        subgraph "Driverç«¯"
            SC[SparkContext] --> DS[DAGScheduler<br/>DAGè°ƒåº¦å™¨]
            DS --> TS[TaskScheduler<br/>ä»»åŠ¡è°ƒåº¦å™¨]
            TS --> SE[SchedulerBackend<br/>è°ƒåº¦åç«¯]
        end
        
        subgraph "Executorç«¯"
            SE --> TE[TaskExecutor<br/>ä»»åŠ¡æ‰§è¡Œå™¨]
            TE --> BM[BlockManager<br/>å­˜å‚¨ç®¡ç†å™¨]
            TE --> SM[ShuffleManager<br/>Shuffleç®¡ç†å™¨]
        end
        
        subgraph "å¤–éƒ¨ç³»ç»Ÿ"
            SE --> CM[ClusterManager<br/>é›†ç¾¤ç®¡ç†å™¨]
        end
    end
    
    style DS fill:#e3f2fd
    style TS fill:#f1f8e9
    style SE fill:#fff3e0
    style TE fill:#fce4ec
```

### 1.2 æ‰§è¡Œå¼•æ“å·¥ä½œæµç¨‹

```scala
// Sparkæ‰§è¡Œå¼•æ“çš„æ ¸å¿ƒå·¥ä½œæµç¨‹
class SparkExecutionEngine {
  
  // 1. Actionè§¦å‘ä½œä¸šæäº¤
  def triggerJob[T](rdd: RDD[T], func: Iterator[T] => Unit): Unit = {
    sparkContext.runJob(rdd, func)
  }
  
  // 2. DAGæ„å»ºä¸ä¼˜åŒ–
  def buildDAG(finalRDD: RDD[_]): DAG = {
    val visitedRDDs = new mutable.HashSet[RDD[_]]
    val dependencies = new ArrayBuffer[Dependency[_]]
    
    def visit(rdd: RDD[_]): Unit = {
      if (!visitedRDDs.contains(rdd)) {
        visitedRDDs += rdd
        dependencies ++= rdd.dependencies
        rdd.dependencies.foreach(dep => visit(dep.rdd))
      }
    }
    
    visit(finalRDD)
    new DAG(visitedRDDs.toSet, dependencies.toSeq)
  }
  
  // 3. Stageåˆ’åˆ†
  def createStages(dag: DAG): Array[Stage] = {
    // æ ¹æ®å®½ä¾èµ–åˆ‡åˆ†Stage
    val stages = new ArrayBuffer[Stage]
    val shuffleDeps = dag.dependencies.filter(_.isInstanceOf[ShuffleDependency])
    
    // åˆ›å»ºShuffleMapStageå’ŒResultStage
    stages.toArray
  }
}
```

---

## 2. DAGè°ƒåº¦å™¨è¯¦è§£

### 2.1 DAGScheduleræ ¸å¿ƒåŠŸèƒ½

**DAGScheduleræ˜¯Sparkè°ƒåº¦ç³»ç»Ÿçš„æ ¸å¿ƒ**ï¼Œè´Ÿè´£ï¼š

1. **DAGæ„å»º**ï¼šå°†RDDçš„ä¾èµ–å…³ç³»è½¬æ¢ä¸ºæœ‰å‘æ— ç¯å›¾
2. **Stageåˆ’åˆ†**ï¼šæ ¹æ®å®½ä¾èµ–å°†DAGåˆ‡åˆ†ä¸ºå¤šä¸ªStage
3. **Taskç”Ÿæˆ**ï¼šä¸ºæ¯ä¸ªStageç”Ÿæˆç›¸åº”çš„Taské›†åˆ
4. **å¤±è´¥é‡è¯•**ï¼šå¤„ç†Stageå’ŒTaskçš„å¤±è´¥é‡è¯•é€»è¾‘
5. **æ•°æ®æœ¬åœ°æ€§**ï¼šä¼˜åŒ–Taskçš„æ•°æ®æœ¬åœ°æ€§è°ƒåº¦

```scala
// DAGSchedulerçš„æ ¸å¿ƒå®ç°é€»è¾‘
class DAGScheduler(
    private[scheduler] val sc: SparkContext,
    private[scheduler] val taskScheduler: TaskScheduler
) {
  
  // æäº¤ä½œä¸šçš„å…¥å£æ–¹æ³•
  def submitJob[T, U](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      callSite: CallSite,
      resultHandler: (Int, U) => Unit,
      properties: Properties): JobWaiter[U] = {
    
    // 1. åˆ›å»ºJobå¯¹è±¡
    val jobId = nextJobId.getAndIncrement()
    val job = new Job(jobId, rdd, func, partitions, callSite, resultHandler, properties)
    
    // 2. æäº¤ç»™äº‹ä»¶å¤„ç†é˜Ÿåˆ—
    eventProcessLoop.post(JobSubmitted(job))
    
    job.waiter
  }
  
  // å¤„ç†Jobæäº¤äº‹ä»¶
  private def handleJobSubmitted(job: Job): Unit = {
    // 1. åˆ›å»ºResultStage
    val finalStage = createResultStage(job.rdd, job.partitions, job.jobId)
    
    // 2. æäº¤Stage
    submitStage(finalStage)
  }
  
  // é€’å½’æäº¤Stage
  private def submitStage(stage: Stage): Unit = {
    // 1. æ£€æŸ¥çˆ¶Stageæ˜¯å¦å®Œæˆ
    val missing = getMissingParentStages(stage)
    
    if (missing.isEmpty) {
      // 2. æ‰€æœ‰çˆ¶Stageéƒ½å®Œæˆï¼Œæäº¤å½“å‰Stage
      submitMissingTasks(stage)
    } else {
      // 3. å…ˆæäº¤ç¼ºå¤±çš„çˆ¶Stage
      missing.foreach(submitStage)
      waitingStages += stage
    }
  }
}
```

### 2.2 DAGæ„å»ºè¿‡ç¨‹

```mermaid
graph LR
    subgraph "RDDä¾èµ–å…³ç³»"
        RDD1[textFile<br/>RDD] --> RDD2[flatMap<br/>RDD]
        RDD2 --> RDD3[map<br/>RDD] 
        RDD3 --> RDD4[reduceByKey<br/>RDD]
        RDD4 --> RDD5[collect<br/>Action]
    end
    
    subgraph "ä¾èµ–ç±»å‹æ ‡æ³¨"
        RDD1 -.->|çª„ä¾èµ–| RDD2
        RDD2 -.->|çª„ä¾èµ–| RDD3
        RDD3 -.->|å®½ä¾èµ–<br/>Shuffle| RDD4
        RDD4 -.->|çª„ä¾èµ–| RDD5
    end
```

**DAGæ„å»ºç®—æ³•ï¼š**
```scala
def buildDAG(finalRDD: RDD[_]): DirectedAcyclicGraph = {
  val visited = mutable.Set[RDD[_]]()
  val nodes = mutable.ArrayBuffer[RDD[_]]()
  val edges = mutable.ArrayBuffer[Dependency[_]]()
  
  def traverse(rdd: RDD[_]): Unit = {
    if (!visited.contains(rdd)) {
      visited.add(rdd)
      nodes += rdd
      
      // éå†æ‰€æœ‰ä¾èµ–
      for (dependency <- rdd.dependencies) {
        edges += dependency
        traverse(dependency.rdd) // é€’å½’éå†çˆ¶RDD
      }
    }
  }
  
  traverse(finalRDD)
  new DirectedAcyclicGraph(nodes.toSeq, edges.toSeq)
}
```

---

## 3. Stageåˆ’åˆ†æœºåˆ¶

### 3.1 Stageåˆ’åˆ†åŸç†

**æ ¸å¿ƒè§„åˆ™ï¼šé‡åˆ°å®½ä¾èµ–å°±åˆ‡åˆ†Stage**

```mermaid
graph TB
    subgraph "Stage 0 (ShuffleMapStage)"
        RDD1[textFile] --> RDD2[flatMap]
        RDD2 --> RDD3[map]
        RDD3 --> Shuffle1[Shuffle Write<br/>è¾“å‡ºåˆ°ç£ç›˜]
    end
    
    subgraph "Stage 1 (ResultStage)"
        Shuffle2[Shuffle Read<br/>ä»ç£ç›˜è¯»å–] --> RDD4[reduceByKey]
        RDD4 --> Action[collect]
    end
    
    Shuffle1 -.->|ç½‘ç»œä¼ è¾“| Shuffle2
    
    style Shuffle1 fill:#ffcdd2
    style Shuffle2 fill:#ffcdd2
```

### 3.2 Stageç±»å‹è¯¦è§£

#### ShuffleMapStage
```scala
class ShuffleMapStage(
    id: Int,
    rdd: RDD[_],
    numTasks: Int,
    parents: List[Stage],
    firstJobId: Int,
    shuffleDep: ShuffleDependency[_, _, _]
) extends Stage(id, rdd, numTasks, parents, firstJobId) {
  
  // æ¯ä¸ªåˆ†åŒºçš„è¾“å‡ºä½ç½®
  val outputLocs = Array.fill[Option[MapStatus]](numPartitions)(None)
  
  // æ˜¯å¦æ‰€æœ‰åˆ†åŒºéƒ½å·²å®Œæˆ
  def isAvailable: Boolean = outputLocs.forall(_.isDefined)
  
  // è·å–è¾“å‡ºä½ç½®
  def outputLocInMapOutputTrackerFormat(): Array[MapStatus] = {
    outputLocs.map(_.orNull)
  }
}
```

#### ResultStage  
```scala
class ResultStage(
    id: Int,
    rdd: RDD[_], 
    func: (TaskContext, Iterator[_]) => _,
    partitions: Array[Int],
    parents: List[Stage],
    firstJobId: Int
) extends Stage(id, rdd, partitions.length, parents, firstJobId) {
  
  // éœ€è¦è®¡ç®—çš„åˆ†åŒº
  val partitionsToCompute: Array[Int] = partitions
  
  // ç»“æœå¤„ç†å‡½æ•°
  val resultHandler: (Int, _) => Unit = _
}
```

### 3.3 Stageåˆ’åˆ†ç®—æ³•å®ç°

```scala
private def getOrCreateParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = {
  getShuffleDependencies(rdd).map { shuffleDep =>
    getOrCreateShuffleMapStage(shuffleDep, firstJobId)
  }.toList
}

private def getShuffleDependencies(rdd: RDD[_]): HashSet[ShuffleDependency[_, _, _]] = {
  val parents = new HashSet[ShuffleDependency[_, _, _]]
  val visited = new HashSet[RDD[_]]
  val waitingForVisit = new ArrayStack[RDD[_]]
  
  waitingForVisit.push(rdd)
  
  while (waitingForVisit.nonEmpty) {
    val toVisit = waitingForVisit.pop()
    if (!visited(toVisit)) {
      visited += toVisit
      
      toVisit.dependencies.foreach {
        case shuffleDep: ShuffleDependency[_, _, _] =>
          parents += shuffleDep  // æ‰¾åˆ°å®½ä¾èµ–ï¼Œæ·»åŠ åˆ°çˆ¶Stage
        case dependency =>
          waitingForVisit.push(dependency.rdd)  // çª„ä¾èµ–ï¼Œç»§ç»­éå†
      }
    }
  }
  
  parents
}
```

---

## 4. Taskè°ƒåº¦æ‰§è¡Œ

### 4.1 Taskç±»å‹ä¸ç”Ÿæˆ

#### Taskå±‚æ¬¡ç»“æ„
```scala
abstract class Task[T](
    val stageId: Int,
    val stageAttemptId: Int, 
    val partitionId: Int
) extends Serializable {
  
  // Taskæ‰§è¡Œçš„æ ¸å¿ƒæ–¹æ³•
  def runTask(context: TaskContext): T
  
  // è·å–Taskçš„æœ€ä½³æ‰§è¡Œä½ç½®
  def preferredLocations: Seq[TaskLocation] = Nil
}

// ShuffleMapTaskï¼šè¾“å‡ºæ•°æ®åˆ°Shuffleç³»ç»Ÿ
class ShuffleMapTask(
    stageId: Int,
    stageAttemptId: Int,
    taskBinary: Broadcast[Array[Byte]],
    partition: Partition,
    locs: Seq[TaskLocation]
) extends Task[MapStatus](stageId, stageAttemptId, partition.index) {
  
  override def runTask(context: TaskContext): MapStatus = {
    val rdd = SparkEnv.get.closureSerializer.newInstance().deserialize[RDD[_]](...)
    val writer = SparkEnv.get.shuffleManager.getWriter[Any, Any](...)
    
    // è®¡ç®—å¹¶å†™å…¥Shuffleæ•°æ®
    writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ <: Product2[Any, Any]]])
    writer.stop(success = true).get
  }
}

// ResultTaskï¼šè®¡ç®—æœ€ç»ˆç»“æœ
class ResultTask[T, U](
    stageId: Int,
    stageAttemptId: Int,
    taskBinary: Broadcast[Array[Byte]],
    partition: Partition,
    locs: Seq[TaskLocation],
    func: (TaskContext, Iterator[T]) => U
) extends Task[U](stageId, stageAttemptId, partition.index) {
  
  override def runTask(context: TaskContext): U = {
    val rdd = SparkEnv.get.closureSerializer.newInstance().deserialize[RDD[T]](...)
    func(context, rdd.iterator(partition, context))
  }
}
```

### 4.2 TaskSchedulerå·¥ä½œæœºåˆ¶

```scala
private[spark] class TaskSchedulerImpl(
    val sc: SparkContext,
    val maxTaskFailures: Int
) extends TaskScheduler {
  
  // æäº¤TaskSet
  override def submitTasks(taskSet: TaskSet): Unit = {
    val tasks = taskSet.tasks
    val manager = createTaskSetManager(taskSet, maxTaskFailures)
    
    // æ·»åŠ åˆ°è°ƒåº¦æ± 
    schedulableBuilder.addTaskSetManager(manager, manager.properties)
    
    // è§¦å‘èµ„æºåˆ†é…
    backend.reviveOffers()
  }
  
  // èµ„æºåˆ†é…ç®—æ³•
  def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = {
    // 1. éšæœºæ‰“ä¹±offersï¼Œé¿å…æ€»æ˜¯åˆ†é…ç»™åŒä¸€ä¸ªexecutor
    val shuffledOffers = Random.shuffle(offers)
    
    // 2. ä¸ºæ¯ä¸ªofferåˆ†é…ä»»åŠ¡
    val tasks = shuffledOffers.map(offer => {
      val execId = offer.executorId
      val host = offer.host
      val availableCores = offer.cores
      
      // 3. æ ¹æ®æ•°æ®æœ¬åœ°æ€§åˆ†é…ä»»åŠ¡
      val localityLevels = Array(
        TaskLocality.PROCESS_LOCAL,  // è¿›ç¨‹æœ¬åœ°
        TaskLocality.NODE_LOCAL,     // èŠ‚ç‚¹æœ¬åœ°  
        TaskLocality.RACK_LOCAL,     // æœºæ¶æœ¬åœ°
        TaskLocality.ANY             // ä»»æ„ä½ç½®
      )
      
      val tasksForExecutor = ArrayBuffer[TaskDescription]()
      var availableCpuCores = availableCores
      
      for (locality <- localityLevels if availableCpuCores > 0) {
        val tasks = resourceOfferSingleTaskSet(
          taskSetManager, execId, host, availableCpuCores, locality
        )
        tasksForExecutor ++= tasks
        availableCpuCores -= tasks.size
      }
      
      tasksForExecutor
    })
    
    tasks
  }
}
```

### 4.3 æ•°æ®æœ¬åœ°æ€§è°ƒåº¦

**æ•°æ®æœ¬åœ°æ€§çº§åˆ«ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰ï¼š**

```scala
object TaskLocality extends Enumeration {
  val PROCESS_LOCAL = Value  // æ•°æ®åœ¨åŒä¸€JVMè¿›ç¨‹ä¸­
  val NODE_LOCAL = Value     // æ•°æ®åœ¨åŒä¸€èŠ‚ç‚¹ä¸Š
  val RACK_LOCAL = Value     // æ•°æ®åœ¨åŒä¸€æœºæ¶ä¸Š  
  val ANY = Value           // æ•°æ®åœ¨ä»»æ„ä½ç½®
}
```

**æœ¬åœ°æ€§è°ƒåº¦å®ç°ï¼š**
```scala
private def getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality = {
  // æ ¹æ®ç­‰å¾…æ—¶é—´å†³å®šæœ¬åœ°æ€§çº§åˆ«
  def getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality = {
    while (curTime - lastLaunchTime >= localityWaits(currentLocalityIndex)) {
      currentLocalityIndex += 1
      if (currentLocalityIndex >= localityWaits.length) {
        return TaskLocality.ANY
      }
    }
    myLocalityLevels(currentLocalityIndex)
  }
}

// æœ¬åœ°æ€§ç­‰å¾…æ—¶é—´é…ç½®
val localityWaits = Array(
  spark.locality.wait.process,  // è¿›ç¨‹æœ¬åœ°æ€§ç­‰å¾…æ—¶é—´
  spark.locality.wait.node,     // èŠ‚ç‚¹æœ¬åœ°æ€§ç­‰å¾…æ—¶é—´  
  spark.locality.wait.rack      // æœºæ¶æœ¬åœ°æ€§ç­‰å¾…æ—¶é—´
)
```

---

## 5. è°ƒåº¦å™¨ç»„ä»¶æ¶æ„

### 5.1 è°ƒåº¦å™¨ç»„ä»¶å…³ç³»

```mermaid
graph TB
    subgraph "Sparkè°ƒåº¦æ¶æ„"
        App[Application] --> Job[Job<br/>ç”±Actionè§¦å‘]
        Job --> Stage[Stage<br/>ç”±å®½ä¾èµ–åˆ’åˆ†]
        Stage --> TaskSet[TaskSet<br/>ä¸€ç»„Task]
        TaskSet --> Task[Task<br/>æœ€å°æ‰§è¡Œå•å…ƒ]
        
        subgraph "è°ƒåº¦ç»„ä»¶"
            DS[DAGScheduler<br/>Stageçº§è°ƒåº¦] 
            TS[TaskScheduler<br/>Taskçº§è°ƒåº¦]
            TSM[TaskSetManager<br/>TaskSetç®¡ç†]
            Pool[Pool<br/>è°ƒåº¦æ± ]
        end
        
        DS --> TS
        TS --> TSM
        TSM --> Pool
        
        subgraph "æ‰§è¡Œåç«¯"
            SB[SchedulerBackend<br/>èµ„æºç®¡ç†]
            Exec[Executor<br/>ä»»åŠ¡æ‰§è¡Œ]
        end
        
        Pool --> SB
        SB --> Exec
    end
    
    style DS fill:#e3f2fd
    style TS fill:#e8f5e8  
    style TSM fill:#fff3e0
    style Pool fill:#f3e5f5
    style SB fill:#ffebee
    style Exec fill:#e0f2f1
```

### 5.2 äº‹ä»¶é©±åŠ¨è°ƒåº¦æœºåˆ¶

```scala
// Sparké‡‡ç”¨äº‹ä»¶é©±åŠ¨çš„è°ƒåº¦æœºåˆ¶
private[scheduler] class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)
  extends EventLoop[DAGSchedulerEvent]("dag-scheduler-event-loop") {
  
  override def onReceive(event: DAGSchedulerEvent): Unit = {
    val timerContext = timer.time()
    try {
      doOnReceive(event)
    } finally {
      timerContext.stop()
    }
  }
  
  private def doOnReceive(event: DAGSchedulerEvent): Unit = event match {
    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)
      
    case StageCancelled(stageId, reason) =>
      dagScheduler.handleStageCancellation(stageId, reason)
      
    case JobCancelled(jobId, reason) =>
      dagScheduler.handleJobCancellation(jobId, reason)
      
    case TaskSetFailed(taskSet, reason, exception) =>
      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)
      
    case CompletionEvent(task, reason, result, accumUpdates, metricPeaks, info) =>
      dagScheduler.handleTaskCompletion(task, reason, result, accumUpdates, metricPeaks, info)
  }
}
```

### 5.3 è°ƒåº¦æ± ä¸å…¬å¹³è°ƒåº¦

```scala
private[spark] class Pool(
    val poolName: String,
    val schedulingMode: SchedulingMode,
    initMinShare: Int,
    initWeight: Int
) extends Schedulable {
  
  val schedulableQueue = new ConcurrentLinkedQueue[Schedulable]
  
  // æŒ‰è°ƒåº¦æ¨¡å¼æ’åº
  override def getSortedTaskSetQueue: ArrayBuffer[TaskSetManager] = {
    val sortedTaskSetQueue = new mutable.ArrayBuffer[TaskSetManager]
    
    schedulingMode match {
      case SchedulingMode.FAIR =>
        // å…¬å¹³è°ƒåº¦ï¼šæŒ‰æƒé‡å’Œè¿è¡Œæ—¶é—´æ’åº
        val comparator = new FairSchedulingAlgorithm()
        sortedTaskSetQueue ++= schedulableQueue.asScala.toSeq.sortWith(comparator.comparator)
        
      case SchedulingMode.FIFO =>
        // FIFOè°ƒåº¦ï¼šæŒ‰æäº¤é¡ºåºæ’åº
        val comparator = new FIFOSchedulingAlgorithm()  
        sortedTaskSetQueue ++= schedulableQueue.asScala.toSeq.sortWith(comparator.comparator)
    }
    
    sortedTaskSetQueue
  }
}
```

---

## 6. å®ä¾‹åˆ†æ

### 6.1 WordCountç¨‹åºæ‰§è¡Œæµç¨‹åˆ†æ

```scala
// ç»å…¸WordCountç¨‹åº
val textFile = sc.textFile("hdfs://input/data.txt")      // RDD_1
val words = textFile.flatMap(line => line.split(" "))    // RDD_2  
val pairs = words.map(word => (word, 1))                 // RDD_3
val wordCounts = pairs.reduceByKey(_ + _)                 // RDD_4
val result = wordCounts.collect()                         // Actionè§¦å‘
```

**æ‰§è¡Œæµç¨‹åˆ†è§£ï¼š**

1. **DAGæ„å»º**
```mermaid
graph LR
    RDD1[textFile<br/>HadoopRDD] --> RDD2[flatMap<br/>FlatMappedRDD]
    RDD2 --> RDD3[map<br/>MappedRDD]
    RDD3 --> RDD4[reduceByKey<br/>ShuffledRDD]
    RDD4 --> Action[collect<br/>Action]
    
    RDD1 -.->|çª„ä¾èµ–| RDD2
    RDD2 -.->|çª„ä¾èµ–| RDD3  
    RDD3 -.->|å®½ä¾èµ–<br/>Shuffle| RDD4
    RDD4 -.->|çª„ä¾èµ–| Action
```

2. **Stageåˆ’åˆ†**
```
Stage 0 (ShuffleMapStage):
- Task 0: textFile.partition(0) -> flatMap -> map -> shuffle write
- Task 1: textFile.partition(1) -> flatMap -> map -> shuffle write
- Task 2: textFile.partition(2) -> flatMap -> map -> shuffle write

Stage 1 (ResultStage):  
- Task 0: shuffle read -> reduceByKey.partition(0) -> collect
- Task 1: shuffle read -> reduceByKey.partition(1) -> collect
```

3. **Taskæ‰§è¡Œæ—¶é—´çº¿**
```mermaid
gantt
    title WordCountæ‰§è¡Œæ—¶é—´çº¿
    dateFormat X
    axisFormat %s
    
    section Stage 0
    Task 0    :done, t0, 0, 3s
    Task 1    :done, t1, 0, 4s  
    Task 2    :done, t2, 0, 3s
    
    section Stage 1
    Task 0    :done, t3, after t1, 2s
    Task 1    :done, t4, after t1, 2s
```

### 6.2 é¡¹ç›®ä¸­çš„å¤æ‚å·¥ä½œæµæ‰§è¡Œ

åŸºäºé¡¹ç›®ä»£ç åˆ†æä¸€ä¸ªå…¸å‹çš„æ•°æ®å¤„ç†å·¥ä½œæµï¼š

```java
// é¡¹ç›®ä¸­çš„å·¥ä½œæµæ‰§è¡Œé€»è¾‘
public class ComplexDataProcessingWorkflow {
    
    public void executeETLPipeline() {
        // 1. æ•°æ®æŠ½å– (Extract)
        SparkJarConfig extractConfig = SparkJarConfig.builder()
            .sparkVersion("3.2.0")
            .mainClass("com.shopee.di.etl.ExtractTask")  
            .sparkProperties("--conf spark.sql.adaptive.enabled=true")
            .build();
            
        // 2. æ•°æ®è½¬æ¢ (Transform)  
        PythonSparkConfig transformConfig = PythonSparkConfig.builder()
            .sparkVersion("3.2.0")
            .language("python")
            .mainResource(ResourceConfig.builder()
                .resourcePath("hdfs://scripts/transform.py")
                .build())
            .build();
            
        // 3. æ•°æ®åŠ è½½ (Load)
        SparkSQLConfig loadConfig = SparkSQLConfig.builder()
            .sparkVersion("3.2.0") 
            .sparkProperties("--conf spark.sql.warehouse.dir=hdfs://warehouse/")
            .build();
    }
}
```

**å·¥ä½œæµDAGç¤ºä¾‹ï¼š**
```mermaid
graph TB
    subgraph "ETLå·¥ä½œæµDAG"
        Extract[æ•°æ®æŠ½å–<br/>SparkJarä»»åŠ¡] --> Transform[æ•°æ®è½¬æ¢<br/>PySparkä»»åŠ¡]
        Transform --> Validate[æ•°æ®æ ¡éªŒ<br/>SparkSQLä»»åŠ¡]
        Validate --> Load[æ•°æ®åŠ è½½<br/>SparkSQLä»»åŠ¡]
        Load --> Report[ç”ŸæˆæŠ¥å‘Š<br/>Emailä»»åŠ¡]
        
        subgraph "å¹¶è¡Œåˆ†æ”¯"
            Transform --> Audit[å®¡è®¡æ—¥å¿—<br/>SparkJarä»»åŠ¡]
            Transform --> Metrics[æŒ‡æ ‡è®¡ç®—<br/>SparkSQLä»»åŠ¡]
        end
        
        Audit --> Archive[æ•°æ®å½’æ¡£<br/>Shellä»»åŠ¡]
        Metrics --> Dashboard[æ›´æ–°ä»ªè¡¨æ¿<br/>HTTPä»»åŠ¡]
    end
```

### 6.3 æ€§èƒ½ä¼˜åŒ–å®ä¾‹

```scala
// åŸºäºé¡¹ç›®ç»éªŒçš„Sparkæ€§èƒ½ä¼˜åŒ–é…ç½®
val optimizedSparkConf = new SparkConf()
  .setAppName("Optimized Data Processing")
  
  // èµ„æºé…ç½®ä¼˜åŒ–
  .set("spark.executor.instances", "20")
  .set("spark.executor.cores", "4") 
  .set("spark.executor.memory", "8g")
  .set("spark.executor.memoryFraction", "0.8")
  
  // Shuffleä¼˜åŒ–
  .set("spark.sql.adaptive.enabled", "true")
  .set("spark.sql.adaptive.coalescePartitions.enabled", "true")
  .set("spark.sql.adaptive.skewJoin.enabled", "true")
  
  // åºåˆ—åŒ–ä¼˜åŒ–
  .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
  .set("spark.kryoserializer.buffer.max", "64m")
  
  // å†…å­˜ç®¡ç†ä¼˜åŒ–
  .set("spark.memory.useLegacyMode", "false")
  .set("spark.memory.storageFraction", "0.5")
```

---

## æ€»ç»“

æœ¬ç« æ·±å…¥åˆ†æäº†Sparkæ‰§è¡Œå¼•æ“çš„æ ¸å¿ƒæœºåˆ¶ï¼š

1. **DAGè°ƒåº¦å™¨**ï¼šè´Ÿè´£ä½œä¸šçš„æ•´ä½“è°ƒåº¦ï¼Œå°†RDDä¾èµ–å…³ç³»è½¬æ¢ä¸ºæ‰§è¡Œè®¡åˆ’
2. **Stageåˆ’åˆ†**ï¼šåŸºäºå®½ä¾èµ–åˆ‡åˆ†ï¼Œä¼˜åŒ–æ•°æ®ä¼ è¾“å’Œå¹¶è¡Œåº¦
3. **Taskè°ƒåº¦**ï¼šå®ç°æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–ï¼Œæé«˜æ‰§è¡Œæ•ˆç‡
4. **äº‹ä»¶é©±åŠ¨**ï¼šå¼‚æ­¥äº‹ä»¶å¤„ç†ï¼Œä¿è¯ç³»ç»Ÿå“åº”æ€§
5. **å®é™…åº”ç”¨**ï¼šç»“åˆé¡¹ç›®ä»£ç ç†è§£å¤æ‚å·¥ä½œæµçš„æ‰§è¡Œæœºåˆ¶

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ Spark SQLä¸Catalystä¼˜åŒ–å™¨çš„æ·±åº¦åŸç†ã€‚

---

## æ¨èé˜…è¯»
- [Sparkå®˜æ–¹æ–‡æ¡£ - Job Scheduling](https://spark.apache.org/docs/latest/job-scheduling.html)
- ã€ŠSparkå†…æ ¸è®¾è®¡çš„è‰ºæœ¯ã€‹ç¬¬6-8ç« 
- [Sparkæºç åˆ†æ - DAGScheduler](https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala)
```

```markdown
# Spark SQLä¸Catalystä¼˜åŒ–å™¨æ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [Spark SQLæ¶æ„æ¦‚è¿°](#1-spark-sqlæ¶æ„æ¦‚è¿°)
2. [Catalystä¼˜åŒ–å™¨åŸç†](#2-catalystä¼˜åŒ–å™¨åŸç†)
3. [Tungstenæ‰§è¡Œå¼•æ“](#3-tungstenæ‰§è¡Œå¼•æ“)
4. [SQLæ‰§è¡Œæµç¨‹è¯¦è§£](#4-sqlæ‰§è¡Œæµç¨‹è¯¦è§£)
5. [ä¼˜åŒ–è§„åˆ™ä¸å®è·µ](#5-ä¼˜åŒ–è§„åˆ™ä¸å®è·µ)
6. [ä»£ç ç”ŸæˆæŠ€æœ¯](#6-ä»£ç ç”ŸæˆæŠ€æœ¯)

---

## 1. Spark SQLæ¶æ„æ¦‚è¿°

### 1.1 Spark SQLæ•´ä½“æ¶æ„

```mermaid
graph TB
    subgraph "Spark SQLå®Œæ•´æ¶æ„"
        subgraph "å‰ç«¯æ¥å£"
            SQL[SQLæŸ¥è¯¢] --> Parser[SQLè§£æå™¨]
            DataFrame[DataFrame API] --> Analyzer[åˆ†æå™¨]  
            Dataset[Dataset API] --> Analyzer
        end
        
        subgraph "Catalystä¼˜åŒ–å™¨"
            Parser --> LogicalPlan[é€»è¾‘è®¡åˆ’<br/>LogicalPlan]
            Analyzer --> LogicalPlan
            LogicalPlan --> Optimizer[é€»è¾‘ä¼˜åŒ–å™¨<br/>RuleBasedOptimizer]
            Optimizer --> OptimizedPlan[ä¼˜åŒ–çš„é€»è¾‘è®¡åˆ’]
            OptimizedPlan --> Planner[ç‰©ç†è®¡åˆ’å™¨<br/>SparkPlanner]
            Planner --> PhysicalPlan[ç‰©ç†è®¡åˆ’<br/>SparkPlan]
        end
        
        subgraph "Tungstenæ‰§è¡Œå¼•æ“"
            PhysicalPlan --> CodeGen[ä»£ç ç”Ÿæˆ<br/>WholeStageCodegen]
            CodeGen --> Execution[ä»»åŠ¡æ‰§è¡Œ<br/>RDD Operations]
        end
        
        subgraph "å­˜å‚¨ç³»ç»Ÿ"
            Execution --> DataSource[æ•°æ®æº<br/>Parquet/JSON/Hiveç­‰]
        end
    end
    
    style Parser fill:#e3f2fd
    style Analyzer fill:#e8f5e8
    style Optimizer fill:#fff3e0  
    style Planner fill:#f3e5f5
    style CodeGen fill:#ffebee
    style DataSource fill:#e0f2f1
```

### 1.2 æ ¸å¿ƒç»„ä»¶åŠŸèƒ½

#### 1. **SQLè§£æå™¨ï¼ˆParserï¼‰**
```scala
// SQLè§£æå™¨å°†SQLå­—ç¬¦ä¸²è½¬æ¢ä¸ºAST
class SparkSqlParser extends AbstractSqlParser {
  
  def parsePlan(sqlText: String): LogicalPlan = {
    parse(sqlText) { parser =>
      astBuilder.visitSingleStatement(parser.singleStatement()) match {
        case plan: LogicalPlan => plan
        case _ => throw new ParseException("Expected a LogicalPlan")
      }
    }
  }
}

// ç¤ºä¾‹ï¼šSQLè§£æè¿‡ç¨‹
val sql = "SELECT name, age FROM users WHERE age > 18"
val parsedPlan = sparkSqlParser.parsePlan(sql)
// ç”Ÿæˆçš„ASTç»“æ„ï¼š
// Project([name, age], Filter(age > 18, UnresolvedRelation(users)))
```

#### 2. **åˆ†æå™¨ï¼ˆAnalyzerï¼‰**
```scala
// åˆ†æå™¨è§£æè¡¨åã€åˆ—åã€å‡½æ•°ç­‰å¼•ç”¨
class Analyzer(catalog: SessionCatalog) extends RuleExecutor[LogicalPlan] {
  
  lazy val batches: Seq[Batch] = Seq(
    Batch("Resolution", Once,
      ResolveRelations ::       // è§£æè¡¨å¼•ç”¨
      ResolveReferences ::      // è§£æåˆ—å¼•ç”¨  
      ResolveDeserializer ::    // è§£æååºåˆ—åŒ–å™¨
      ResolveFunctions ::       // è§£æå‡½æ•°è°ƒç”¨
      Nil: _*),
    
    Batch("Post-Hoc Resolution", Once, 
      PreprocessTableInsertion ::
      ResolveUpCast ::
      Nil: _*)
  )
}

// åˆ†æè¿‡ç¨‹ç¤ºä¾‹
// è¾“å…¥ï¼šProject([name, age], Filter(age > 18, UnresolvedRelation(users)))
// è¾“å‡ºï¼šProject([users.name, users.age], Filter(users.age > 18, ResolvedRelation(usersTable)))
```

### 1.3 SQLåˆ°RDDçš„è½¬æ¢è¿‡ç¨‹

```scala
// å®Œæ•´çš„SQLæ‰§è¡Œæµç¨‹
class SparkSession {
  
  def sql(sqlText: String): DataFrame = {
    // 1. è§£æSQL
    val logicalPlan = sparkSqlParser.parsePlan(sqlText)
    
    // 2. åˆ†æå’Œä¼˜åŒ–
    val analyzedPlan = analyzer.executeAndCheck(logicalPlan)  
    val optimizedPlan = optimizer.executeAndCheck(analyzedPlan)
    
    // 3. ç”Ÿæˆç‰©ç†è®¡åˆ’
    val physicalPlans = planner.plan(optimizedPlan)
    val sparkPlan = planner.pruneFilterProject(physicalPlans.head)
    
    // 4. æ‰§è¡Œ
    val rdd = sparkPlan.execute()
    
    // 5. å°è£…ä¸ºDataFrame
    Dataset.ofRows(this, optimizedPlan)
  }
}
```

---

## 2. Catalystä¼˜åŒ–å™¨åŸç†

### 2.1 åŸºäºè§„åˆ™çš„ä¼˜åŒ–æ¡†æ¶

**Catalysté‡‡ç”¨åŸºäºè§„åˆ™çš„ä¼˜åŒ–ï¼ˆRule-Based Optimizationï¼‰**

```scala
// Catalystä¼˜åŒ–å™¨çš„æ ¸å¿ƒæŠ½è±¡
abstract class Rule[TreeType <: TreeNode[_]] extends Logging {
  
  // è§„åˆ™åç§°
  val ruleName: String
  
  // åº”ç”¨è§„åˆ™çš„æ ¸å¿ƒæ–¹æ³•
  def apply(plan: TreeType): TreeType
  
  // æ¡ä»¶ï¼šä½•æ—¶åº”ç”¨æ­¤è§„åˆ™
  def condition(plan: TreeType): Boolean = true
}

// è§„åˆ™æ‰§è¡Œå™¨
abstract class RuleExecutor[TreeType <: TreeNode[_]] {
  
  // å®šä¹‰ä¼˜åŒ–æ‰¹æ¬¡
  protected def batches: Seq[Batch]
  
  // æ‰§è¡Œä¼˜åŒ–
  def execute(plan: TreeType): TreeType = {
    var curPlan = plan
    
    batches.foreach { batch =>
      val batchStartPlan = curPlan
      var iteration = 0
      var lastPlan = curPlan
      
      // è¿­ä»£åº”ç”¨è§„åˆ™ç›´åˆ°è®¡åˆ’ç¨³å®š
      do {
        curPlan = batch.rules.foldLeft(curPlan) {
          case (currentPlan, rule) => rule(currentPlan)
        }
        iteration += 1
      } while (iteration < batch.strategy.maxIterations && !curPlan.fastEquals(lastPlan))
    }
    
    curPlan
  }
}
```

### 2.2 å¸¸ç”¨ä¼˜åŒ–è§„åˆ™

#### 1. **å¸¸é‡æŠ˜å ï¼ˆConstant Foldingï¼‰**
```scala
object ConstantFolding extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan.transformAllExpressions {
    
    // ç®—æœ¯è¡¨è¾¾å¼å¸¸é‡æŠ˜å 
    case Add(Literal(a, dt), Literal(b, _)) if dt.isInstanceOf[NumericType] =>
      Literal(a.asInstanceOf[Number].doubleValue() + b.asInstanceOf[Number].doubleValue())
      
    // å­—ç¬¦ä¸²è¿æ¥å¸¸é‡æŠ˜å   
    case Concat(Literal(a: UTF8String, _) :: Literal(b: UTF8String, _) :: Nil) =>
      Literal(a.concat(b))
      
    // å¸ƒå°”è¡¨è¾¾å¼å¸¸é‡æŠ˜å 
    case And(Literal(true, BooleanType), right) => right
    case And(Literal(false, BooleanType), _) => Literal(false)
  }
}

// ä¼˜åŒ–ç¤ºä¾‹
// åŸå§‹ï¼šSELECT name FROM users WHERE 1 + 2 > age  
// ä¼˜åŒ–ï¼šSELECT name FROM users WHERE 3 > age
```

#### 2. **è°“è¯ä¸‹æ¨ï¼ˆPredicate Pushdownï¼‰**
```scala
object PushDownPredicate extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    
    // å°†Filterä¸‹æ¨åˆ°Joinä¸‹æ–¹
    case Filter(condition, Join(left, right, joinType, joinCondition)) =>
      val (leftConditions, rightConditions, commonConditions) = 
        splitConjunctivePredicates(condition)
      
      val newLeft = leftConditions.foldLeft(left)((l, c) => Filter(c, l))
      val newRight = rightConditions.foldLeft(right)((r, c) => Filter(c, r))
      
      val newJoin = Join(newLeft, newRight, joinType, joinCondition)
      commonConditions.foldLeft[LogicalPlan](newJoin)((j, c) => Filter(c, j))
      
    // å°†Filterä¸‹æ¨åˆ°Projectä¸‹æ–¹  
    case Filter(condition, Project(projectList, child)) =>
      val aliases = getAliasMap(projectList)
      val substitutedCondition = condition.transform {
        case a: Attribute => aliases.getOrElse(a, a)
      }
      Project(projectList, Filter(substitutedCondition, child))
  }
}

// ä¼˜åŒ–ç¤ºä¾‹
// åŸå§‹ï¼šSELECT u.name FROM users u JOIN orders o ON u.id = o.user_id WHERE u.age > 18
// ä¼˜åŒ–ï¼šSELECT u.name FROM (SELECT * FROM users WHERE age > 18) u JOIN orders o ON u.id = o.user_id
```

#### 3. **åˆ—è£å‰ªï¼ˆColumn Pruningï¼‰**
```scala
object ColumnPruning extends Rule[LogicalPlan] {
  def apply(plan: LogicalPlan): LogicalPlan = plan transform {
    
    // Projectåˆ—è£å‰ª
    case Project(projectList, child) if projectList != child.output =>
      val requiredAttributes = projectList.flatMap(_.references).distinct
      Project(projectList, prunedChild(child, requiredAttributes))
      
    // Joinåˆ—è£å‰ª
    case Join(left, right, joinType, condition) =>
      val requiredAttributes = condition.map(_.references).getOrElse(Set.empty) ++
                              plan.references
      val prunedLeft = prunedChild(left, requiredAttributes)  
      val prunedRight = prunedChild(right, requiredAttributes)
      Join(prunedLeft, prunedRight, joinType, condition)
  }
}

// ä¼˜åŒ–ç¤ºä¾‹  
// åŸå§‹ï¼šSELECT name FROM (SELECT name, age, city FROM users)
// ä¼˜åŒ–ï¼šSELECT name FROM (SELECT name FROM users)
```

### 2.3 ä¼˜åŒ–å™¨æ‰§è¡Œæ‰¹æ¬¡

```scala
abstract class SparkOptimizer extends Optimizer {
  
  def batches: Seq[Batch] = {
    Batch("Finish Analysis", Once,
      EliminateSubqueryAliases) ::
      
    Batch("Union", Once,  
      CombineUnions) ::
      
    Batch("Pullup Correlated Predicates", Once,
      PullupCorrelatedPredicates) ::
      
    Batch("Subquery", Once,
      OptimizeSubqueries) ::
      
    Batch("Replace Operators", fixedPoint,
      ReplaceIntersectWithSemiJoin,
      ReplaceExceptWithFilter,
      ReplaceExceptWithAntiJoin,
      ReplaceDistinctWithAggregate) ::
      
    Batch("Aggregate", fixedPoint,
      RemoveLiteralFromGroupExpressions,
      RemoveRepetitionFromGroupExpressions) ::
      
    Batch("Operator Optimizations", fixedPoint, Seq(
      // Operator push down
      PushProjectionThroughUnion,
      PushPredicateThroughJoin,
      PushDownPredicate,
      LimitPushDown,
      ColumnPruning,
      InferFiltersFromConstraints,
      
      // Operator combine  
      CollapseRepartition,
      CollapseProject,
      CollapseWindow,
      CombineFilters,
      CombineLimits,
      CombineUnions,
      
      // Constant folding and strength reduction
      TransposeWindow,
      NullPropagation,
      ConstantFolding,
      ReorderAssociativeOperator,
      LikeSimplification,
      BooleanSimplification,
      SimplifyConditionals,
      RemoveDispensableExpressions,
      SimplifyBinaryComparison,
      PruneFilters,
      EliminateSorts,
      SimplifyCasts,
      SimplifyCaseConversionExpressions,
      RewriteCorrelatedScalarSubquery,
      EliminateMapObjects,
      EliminateUnions,
      SimplifyExtractValueOps,
      CombineConcats) ++
      extendedOperatorOptimizationRules: _*)
  }
}
```

---

## 3. Tungstenæ‰§è¡Œå¼•æ“

### 3.1 Tungstenæ ¸å¿ƒç‰¹æ€§

**Tungstenæ˜¯Spark 2.0å¼•å…¥çš„æ–°ä¸€ä»£æ‰§è¡Œå¼•æ“ï¼Œä¸»è¦ç‰¹æ€§ï¼š**

1. **å †å¤–å†…å­˜ç®¡ç†**ï¼šå‡å°‘GCå‹åŠ›
2. **ç¼“å­˜å‹å¥½çš„æ•°æ®ç»“æ„**ï¼šæé«˜CPUç¼“å­˜å‘½ä¸­ç‡  
3. **å…¨é˜¶æ®µä»£ç ç”Ÿæˆ**ï¼šæ¶ˆé™¤è™šå‡½æ•°è°ƒç”¨å¼€é”€
4. **å‘é‡åŒ–å¤„ç†**ï¼šåˆ©ç”¨ç°ä»£CPUçš„SIMDæŒ‡ä»¤

```mermaid
graph TB
    subgraph "Tungstenæ‰§è¡Œå¼•æ“æ¶æ„"
        subgraph "å†…å­˜ç®¡ç†"
            OnHeap[å †å†…å†…å­˜<br/>Java Objects]
            OffHeap[å †å¤–å†…å­˜<br/>Unsafe Operations]
            Cache[ç¼“å­˜ä¼˜åŒ–<br/>Cache-friendly Layout]
        end
        
        subgraph "ä»£ç ç”Ÿæˆ"
            WholeStage[å…¨é˜¶æ®µä»£ç ç”Ÿæˆ<br/>WholeStageCodegen]
            Vectorization[å‘é‡åŒ–<br/>Columnar Processing] 
            SIMD[SIMDä¼˜åŒ–<br/>CPUæŒ‡ä»¤çº§å¹¶è¡Œ]
        end
        
        subgraph "æ•°æ®æ ¼å¼"
            UnsafeRow[UnsafeRow<br/>äºŒè¿›åˆ¶æ ¼å¼]
            ColumnBatch[ColumnarBatch<br/>åˆ—å¼å­˜å‚¨]
            Parquet[Parquet<br/>åˆ—å¼æ–‡ä»¶æ ¼å¼]
        end
        
        OnHeap --> OffHeap
        OffHeap --> Cache
        WholeStage --> Vectorization
        Vectorization --> SIMD
        UnsafeRow --> ColumnBatch  
        ColumnBatch --> Parquet
    end
    
    style OnHeap fill:#ffcdd2
    style OffHeap fill:#c8e6c9
    style WholeStage fill:#fff3e0
    style UnsafeRow fill:#e1f5fe
```

### 3.2 å †å¤–å†…å­˜ç®¡ç†

```scala
// UnsafeRowï¼šTungstençš„æ ¸å¿ƒæ•°æ®ç»“æ„  
class UnsafeRow extends InternalRow {
  
  private var baseObject: Object = _
  private var baseOffset: Long = _
  private var sizeInBytes: Int = _
  
  // é«˜æ•ˆçš„å­—æ®µè®¿é—®
  def getInt(ordinal: Int): Int = {
    assertIndexIsValid(ordinal)
    Platform.getInt(baseObject, getFieldOffset(ordinal))
  }
  
  def setInt(ordinal: Int, value: Int): Unit = {
    assertIndexIsValid(ordinal)
    Platform.putInt(baseObject, getFieldOffset(ordinal), value)
  }
  
  // å­—æ®µåç§»è®¡ç®—
  private def getFieldOffset(ordinal: Int): Long = {
    baseOffset + bitSetWidthInBytes + ordinal * 8L
  }
}

// å†…å­˜åˆ†é…å™¨
class TaskMemoryManager(
    memoryManager: MemoryManager,
    taskAttemptId: Long
) {
  
  // ç”³è¯·å †å¤–å†…å­˜é¡µ
  def allocatePage(size: Long): MemoryBlock = {
    if (size > MAXIMUM_PAGE_SIZE_BYTES) {
      throw new IllegalArgumentException(s"Cannot allocate page of size $size")
    }
    
    val page = memoryManager.tungstenMemoryAllocator.allocate(size)
    page
  }
  
  // é‡Šæ”¾å†…å­˜é¡µ
  def freePage(page: MemoryBlock): Unit = {
    memoryManager.tungstenMemoryAllocator.free(page)
  }
}
```

### 3.3 å…¨é˜¶æ®µä»£ç ç”Ÿæˆ

**ä»£ç ç”Ÿæˆçš„æ ¸å¿ƒæ€æƒ³ï¼šå°†æ•´ä¸ªStageçš„è®¡ç®—é€»è¾‘ç¼–è¯‘æˆä¸€ä¸ªJavaæ–¹æ³•**

```scala
// WholeStageCodegenå®ç°
case class WholeStageCodegenExec(child: SparkPlan) extends SparkPlan {
  
  override def doExecute(): RDD[InternalRow] = {
    val (ctx, cleanedSource) = doCodeGen()
    
    // ç¼–è¯‘ç”Ÿæˆçš„ä»£ç 
    val compiledClass = CodeGenerator.compile(cleanedSource)
    val generatedIterator = compiledClass.generate(ctx.references.toArray)
    
    child.execute().mapPartitionsWithIndex { (index, iter) =>
      generatedIterator.asInstanceOf[BufferedRowIterator].init(index, Array(iter))
      new Iterator[InternalRow] {
        def hasNext: Boolean = generatedIterator.hasNext
        def next(): InternalRow = generatedIterator.next()
      }
    }
  }
  
  def doCodeGen(): (CodegenContext, String) = {
    val ctx = new CodegenContext
    val code = child.produce(ctx, this)
    
    val source = s"""
      public SpecificIterator generate(Object[] references) {
        return new SpecificIterator(references);
      }
      
      final class SpecificIterator extends BufferedRowIterator {
        
        private Object[] references;
        private Iterator<InternalRow> inputs[];
        
        public SpecificIterator(Object[] references) {
          this.references = references;
        }
        
        public void init(int index, Iterator<InternalRow>[] inputs) {
          this.inputs = inputs;  
        }
        
        protected void processNext() throws java.io.IOException {
          $code
        }
      }
    """
    
    (ctx, source)
  }
}
```

**ç”Ÿæˆçš„ä»£ç ç¤ºä¾‹ï¼š**
```java
// åŸå§‹SQLï¼šSELECT a + b FROM table WHERE c > 10
// ç”Ÿæˆçš„ä»£ç ï¼š
protected void processNext() throws java.io.IOException {
  while (inputs[0].hasNext()) {
    InternalRow row = (InternalRow) inputs[0].next();
    
    // WHERE c > 10
    int c = row.getInt(2);
    if (c > 10) {
      
      // SELECT a + b  
      int a = row.getInt(0);
      int b = row.getInt(1);
      int result = a + b;
      
      // è¾“å‡ºç»“æœ
      UnsafeRow unsafeRow = new UnsafeRow(1);
      unsafeRow.setInt(0, result);
      append(unsafeRow);
    }
  }
}
```

### 3.4 å‘é‡åŒ–æ‰§è¡Œ

```scala
// å‘é‡åŒ–æ‰§è¡Œæ”¯æŒ
trait ColumnarBatchScan extends LeafExecNode {
  
  // æ”¯æŒå‘é‡åŒ–è¯»å–
  override def supportsBatch: Boolean = true
  
  // æ‰¹é‡è¯»å–æ•°æ®
  override def doExecuteColumnar(): RDD[ColumnarBatch] = {
    // ä»æ•°æ®æºè¯»å–åˆ—å¼æ•°æ®
    inputRDD.mapPartitions { iter =>
      new Iterator[ColumnarBatch] {
        def hasNext: Boolean = iter.hasNext
        def next(): ColumnarBatch = {
          // è¯»å–ä¸€æ‰¹æ•°æ®ï¼ˆé»˜è®¤4096è¡Œï¼‰
          val batch = new ColumnarBatch(schema.fields.map(createColumnVector))
          var rowCount = 0
          
          while (iter.hasNext && rowCount < 4096) {
            val row = iter.next()
            // å°†è¡Œæ•°æ®æ·»åŠ åˆ°åˆ—å‘é‡ä¸­
            addRowToBatch(batch, row, rowCount)
            rowCount += 1
          }
          
          batch.setNumRows(rowCount)
          batch
        }
      }
    }
  }
}

// å‘é‡åŒ–ç®—å­ç¤ºä¾‹
case class ColumnarProjectExec(
    projectList: Seq[NamedExpression], 
    child: SparkPlan
) extends SparkPlan {
  
  override def doExecuteColumnar(): RDD[ColumnarBatch] = {
    child.executeColumnar().mapPartitions { batches =>
      batches.map { batch =>
        // å¯¹æ¯ä¸ªæ‰¹æ¬¡è¿›è¡ŒæŠ•å½±è®¡ç®—
        val projectedColumns = projectList.map { expr =>
          evaluateExpression(expr, batch)
        }
        new ColumnarBatch(projectedColumns.toArray, batch.numRows())
      }
    }
  }
}
```

---

## 4. SQLæ‰§è¡Œæµç¨‹è¯¦è§£

### 4.1 å®Œæ•´çš„SQLæ‰§è¡Œç®¡é“

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant Session as SparkSession
    participant Parser as SQLè§£æå™¨
    participant Analyzer as åˆ†æå™¨
    participant Optimizer as ä¼˜åŒ–å™¨
    participant Planner as ç‰©ç†è®¡åˆ’å™¨
    participant Codegen as ä»£ç ç”Ÿæˆå™¨
    participant Execution as æ‰§è¡Œå¼•æ“
    
    User->>Session: spark.sql("SELECT ...")
    Session->>Parser: è§£æSQLå­—ç¬¦ä¸²
    Parser->>Session: LogicalPlan (AST)
    
    Session->>Analyzer: åˆ†æé€»è¾‘è®¡åˆ’
    Analyzer->>Session: AnalyzedLogicalPlan
    
    Session->>Optimizer: ä¼˜åŒ–é€»è¾‘è®¡åˆ’
    Optimizer->>Session: OptimizedLogicalPlan
    
    Session->>Planner: ç”Ÿæˆç‰©ç†è®¡åˆ’
    Planner->>Session: SparkPlan
    
    Session->>Codegen: ä»£ç ç”Ÿæˆä¼˜åŒ–
    Codegen->>Session: GeneratedCode
    
    Session->>Execution: æ‰§è¡Œç‰©ç†è®¡åˆ’
    Execution->>Session: RDD[InternalRow]
    
    Session->>User: DataFrameç»“æœ
```

### 4.2 é€»è¾‘è®¡åˆ’æ ‘ç»“æ„

```scala
// å…¸å‹SQLçš„é€»è¾‘è®¡åˆ’æ ‘
val sql = """
  SELECT u.name, COUNT(*) as order_count
  FROM users u
  JOIN orders o ON u.id = o.user_id  
  WHERE u.age > 18
  GROUP BY u.name
  ORDER BY order_count DESC
  LIMIT 10
"""

// ç”Ÿæˆçš„é€»è¾‘è®¡åˆ’æ ‘ï¼š
/*
Limit(10)
 +- Sort(order_count DESC)
    +- Aggregate(u.name, COUNT(*) as order_count)
       +- Join(u.id = o.user_id)
          :- Filter(age > 18)
          :  +- SubqueryAlias(u)
          :     +- UnresolvedRelation(users)
          +- SubqueryAlias(o)
             +- UnresolvedRelation(orders)
*/
```

### 4.3 ç‰©ç†è®¡åˆ’é€‰æ‹©

```scala
// ç‰©ç†è®¡åˆ’å™¨é€‰æ‹©æœ€ä¼˜æ‰§è¡Œç­–ç•¥
object SparkStrategies extends QueryPlanner[SparkPlan] {
  
  def strategies: Seq[Strategy] = Seq(
    FileSourceStrategy,       // æ–‡ä»¶æ•°æ®æºç­–ç•¥
    DataSourceStrategy,       // é€šç”¨æ•°æ®æºç­–ç•¥  
    SpecialLimits,           // Limitä¼˜åŒ–ç­–ç•¥
    Aggregation,             // èšåˆç­–ç•¥
    Window,                  // çª—å£å‡½æ•°ç­–ç•¥
    JoinSelection,           // Joinç­–ç•¥é€‰æ‹©
    InMemoryScans,           // å†…å­˜ç¼“å­˜æ‰«æ
    BasicOperators           // åŸºç¡€ç®—å­ç­–ç•¥
  )
}

// Joinç­–ç•¥é€‰æ‹©ç¤ºä¾‹
object JoinSelection extends Strategy {
  def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
    
    case Join(left, right, joinType, condition) =>
      
      // 1. å¹¿æ’­Hash Join (å°è¡¨)
      if (canBroadcast(right)) {
        Seq(BroadcastHashJoinExec(left, right, joinType, condition, BuildRight))
      }
      
      // 2. Sort Merge Join (å¤§è¡¨)  
      else if (canSortMergeJoin(joinType)) {
        Seq(SortMergeJoinExec(left, right, joinType, condition))
      }
      
      // 3. Hash Join (é»˜è®¤)
      else {
        Seq(ShuffledHashJoinExec(left, right, joinType, condition, BuildRight))
      }
  }
}
```

---

## 5. ä¼˜åŒ–è§„åˆ™ä¸å®è·µ

### 5.1 Joinä¼˜åŒ–ç­–ç•¥

#### 1. **å¹¿æ’­Joinä¼˜åŒ–**
```scala
// è‡ªåŠ¨å¹¿æ’­å°è¡¨
spark.conf.set("spark.sql.adaptive.enabled", true)
spark.conf.set("spark.sql.adaptive.join.enabled", true)  
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "10MB")

// æ‰‹åŠ¨å¹¿æ’­æç¤º
import org.apache.spark.sql.functions.broadcast

val result = users.join(
  broadcast(small_lookup_table),  // å¼ºåˆ¶å¹¿æ’­
  users("category_id") === small_lookup_table("id")
)
```

#### 2. **Sort Merge Joinä¼˜åŒ–**  
```scala
// å¯¹å¤§è¡¨Joinè¿›è¡Œåˆ†åŒºä¼˜åŒ–
val users = spark.read.parquet("users.parquet")
  .repartition($"user_id")  // æŒ‰Joiné”®é‡åˆ†åŒº

val orders = spark.read.parquet("orders.parquet")
  .repartition($"user_id")  // æŒ‰Joiné”®é‡åˆ†åŒº
  
val result = users.join(orders, "user_id")  // é¿å…Shuffle
```

### 5.2 èšåˆä¼˜åŒ–ç­–ç•¥

#### 1. **é¢„èšåˆä¼˜åŒ–**
```scala
// Catalystè‡ªåŠ¨åº”ç”¨é¢„èšåˆ
val sql = """
  SELECT category, COUNT(*) 
  FROM products 
  GROUP BY category
"""

// ä¼˜åŒ–åçš„æ‰§è¡Œè®¡åˆ’ï¼š
/*
HashAggregateExec(keys=[category], functions=[count(1)])
+- Exchange hashpartitioning(category, 200)
   +- HashAggregateExec(keys=[category], functions=[partial_count(1)])  -- é¢„èšåˆ
      +- FileScan parquet [category]
*/
```

#### 2. **åˆ†ç»„é›†ä¼˜åŒ–**
```scala
// GROUPING SETSä¼˜åŒ–
val sql = """
  SELECT category, brand, COUNT(*)
  FROM products
  GROUP BY GROUPING SETS (
    (category, brand),
    (category), 
    ()
  )
"""

// Catalystè‡ªåŠ¨ä¼˜åŒ–ä¸ºï¼š
// ExpandExec + HashAggregateExec  é¿å…å¤šæ¬¡æ‰«ææ•°æ®
```

### 5.3 å­æŸ¥è¯¢ä¼˜åŒ–

#### 1. **å­æŸ¥è¯¢é‡å†™**
```scala
// ç›¸å…³å­æŸ¥è¯¢ä¼˜åŒ–
val sql = """
  SELECT u.name 
  FROM users u
  WHERE EXISTS (
    SELECT 1 FROM orders o 
    WHERE o.user_id = u.id AND o.amount > 100
  )
"""

// Catalysté‡å†™ä¸ºSemi Joinï¼š
val optimized = users
  .join(orders.filter($"amount" > 100), Seq("user_id"), "leftsemi")
  .select($"name")
```

#### 2. **æ ‡é‡å­æŸ¥è¯¢ä¼˜åŒ–**
```scala
// æ ‡é‡å­æŸ¥è¯¢æå‡
val sql = """
  SELECT name, age,
    (SELECT AVG(amount) FROM orders WHERE user_id = users.id) as avg_order
  FROM users  
"""

// ä¼˜åŒ–ä¸ºLeft Outer Joiné¿å…N+1æŸ¥è¯¢
```

---

## 6. ä»£ç ç”ŸæˆæŠ€æœ¯

### 6.1 è¡¨è¾¾å¼ä»£ç ç”Ÿæˆ

```scala
// è¡¨è¾¾å¼ä»£ç ç”Ÿæˆç¤ºä¾‹
abstract class Expression extends TreeNode[Expression] {
  
  // ç”Ÿæˆè¡¨è¾¾å¼è®¡ç®—ä»£ç 
  def genCode(ctx: CodegenContext): ExprCode = {
    val isNull = ctx.freshName("isNull")
    val value = ctx.freshName("value")
    
    val code = doGenCode(ctx, ExprCode(isNull, value, ""))
    code
  }
  
  // å…·ä½“è¡¨è¾¾å¼å®ç°ä»£ç ç”Ÿæˆ
  protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode
}

// Addè¡¨è¾¾å¼çš„ä»£ç ç”Ÿæˆ
case class Add(left: Expression, right: Expression) extends Expression {
  
  protected def doGenCode(ctx: CodegenContext, ev: ExprCode): ExprCode = {
    val leftGen = left.genCode(ctx)
    val rightGen = right.genCode(ctx)
    
    val code = s"""
      ${leftGen.code}
      ${rightGen.code}
      
      boolean ${ev.isNull} = ${leftGen.isNull} || ${rightGen.isNull};
      ${javaType} ${ev.value} = ${defaultValue};
      
      if (!${ev.isNull}) {
        ${ev.value} = ${leftGen.value} + ${rightGen.value};
      }
    """
    
    ev.copy(code = code)
  }
}
```

### 6.2 ç®—å­ä»£ç ç”Ÿæˆ

```scala
// Filterç®—å­ä»£ç ç”Ÿæˆ
case class FilterExec(condition: Expression, child: SparkPlan) extends SparkPlan {
  
  override def doConsume(ctx: CodegenContext, input: Seq[ExprCode], row: ExprCode): String = {
    val conditionGen = condition.genCode(ctx)
    
    s"""
      ${conditionGen.code}
      if (!${conditionGen.isNull} && ${conditionGen.value}) {
        ${parent.doConsume(ctx, input, row)}
      }
    """
  }
}

// Projectç®—å­ä»£ç ç”Ÿæˆ  
case class ProjectExec(projectList: Seq[NamedExpression], child: SparkPlan) extends SparkPlan {
  
  override def doConsume(ctx: CodegenContext, input: Seq[ExprCode], row: ExprCode): String = {
    val exprs = projectList.map(_.genCode(ctx))
    val exprCodes = exprs.map(_.code).mkString("\n")
    val exprValues = exprs.map(_.value)
    
    s"""
      $exprCodes
      ${parent.doConsume(ctx, exprs, row)}
    """
  }
}
```

### 6.3 ä»£ç ç”Ÿæˆä¼˜åŒ–æŠ€æœ¯

#### 1. **åˆ†æ”¯æ¶ˆé™¤**
```java
// ä¼˜åŒ–å‰ï¼šåŒ…å«åˆ†æ”¯åˆ¤æ–­
if (!input_isNull_0) {
  if (input_value_0 > 18) {
    // å¤„ç†é€»è¾‘
  }
}

// ä¼˜åŒ–åï¼šæ¶ˆé™¤åˆ†æ”¯  
boolean isValid = !input_isNull_0 && (input_value_0 > 18);
if (isValid) {
  // å¤„ç†é€»è¾‘
}
```

#### 2. **å¾ªç¯å±•å¼€**
```java
// ä¼˜åŒ–å‰ï¼šå¾ªç¯å¤„ç†
for (int i = 0; i < batch.numRows(); i++) {
  process(batch.getRow(i));
}

// ä¼˜åŒ–åï¼šå¾ªç¯å±•å¼€
int i = 0;
for (; i < (batch.numRows() & ~3); i += 4) {
  process(batch.getRow(i));
  process(batch.getRow(i + 1));
  process(batch.getRow(i + 2));
  process(batch.getRow(i + 3));
}
for (; i < batch.numRows(); i++) {
  process(batch.getRow(i));
}
```

---

## æ€»ç»“

æœ¬ç« æ·±å…¥å­¦ä¹ äº†Spark SQLä¸Catalystä¼˜åŒ–å™¨çš„æ ¸å¿ƒåŸç†ï¼š

1. **SQLæ¶æ„**ï¼šä»è§£æã€åˆ†æåˆ°ä¼˜åŒ–ã€æ‰§è¡Œçš„å®Œæ•´ç®¡é“
2. **Catalystä¼˜åŒ–å™¨**ï¼šåŸºäºè§„åˆ™çš„ä¼˜åŒ–æ¡†æ¶ï¼ŒåŒ…å«ä¸Šç™¾ç§ä¼˜åŒ–è§„åˆ™  
3. **Tungstenå¼•æ“**ï¼šå †å¤–å†…å­˜ã€ä»£ç ç”Ÿæˆã€å‘é‡åŒ–ç­‰ç°ä»£åŒ–æŠ€æœ¯
4. **æ‰§è¡Œæµç¨‹**ï¼šSQLåˆ°RDDçš„å®Œæ•´è½¬æ¢è¿‡ç¨‹
5. **ä¼˜åŒ–å®è·µ**ï¼šJoinã€èšåˆã€å­æŸ¥è¯¢ç­‰å¸¸è§ä¼˜åŒ–åœºæ™¯
6. **ä»£ç ç”Ÿæˆ**ï¼šè¡¨è¾¾å¼å’Œç®—å­çš„Javaä»£ç ç”ŸæˆæŠ€æœ¯

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ Sparkçš„å†…å­˜ç®¡ç†ä¸Shuffleæœºåˆ¶ã€‚

---

## æ¨èé˜…è¯»
- [Sparkå®˜æ–¹æ–‡æ¡£ - SQL Performance Tuning](https://spark.apache.org/docs/latest/sql-performance-tuning.html)
- [Deep Dive into Spark SQL's Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)
- ã€ŠSpark SQLå†…æ ¸å‰–æã€‹å…¨ä¹¦
```

```markdown
# Sparkå†…å­˜ç®¡ç†ä¸Shuffleæœºåˆ¶æ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [Sparkå†…å­˜ç®¡ç†æ¶æ„](#1-sparkå†…å­˜ç®¡ç†æ¶æ„)
2. [ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶](#2-ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶)
3. [Shuffleæœºåˆ¶è¯¦è§£](#3-shuffleæœºåˆ¶è¯¦è§£)
4. [å­˜å‚¨ç³»ç»ŸBlockManager](#4-å­˜å‚¨ç³»ç»Ÿblockmanager)
5. [å†…å­˜è°ƒä¼˜ç­–ç•¥](#5-å†…å­˜è°ƒä¼˜ç­–ç•¥)
6. [Shuffleä¼˜åŒ–å®è·µ](#6-shuffleä¼˜åŒ–å®è·µ)

---

## 1. Sparkå†…å­˜ç®¡ç†æ¶æ„

### 1.1 Sparkå†…å­˜æ¨¡å‹æ¦‚è§ˆ

```mermaid
graph TB
    subgraph "Sparkå†…å­˜æ¶æ„"
        subgraph "å †å†…å†…å­˜ (On-Heap)"
            Reserved[é¢„ç•™å†…å­˜<br/>300MB<br/>ç³»ç»Ÿä¿ç•™]
            UserMem[ç”¨æˆ·å†…å­˜<br/>User Memory<br/>ç”¨æˆ·ä»£ç ä½¿ç”¨]
            SparkMem[Sparkå†…å­˜<br/>Spark Memory<br/>60%å¯é…ç½®]
            
            subgraph "Sparkå†…å­˜ç»†åˆ†"
                Storage[å­˜å‚¨å†…å­˜<br/>Storage Memory<br/>ç¼“å­˜RDD/DataFrame]
                Execution[æ‰§è¡Œå†…å­˜<br/>Execution Memory<br/>Shuffle/Join/èšåˆ]
            end
        end
        
        subgraph "å †å¤–å†…å­˜ (Off-Heap)"
            OffHeapStorage[å †å¤–å­˜å‚¨å†…å­˜<br/>Off-Heap Storage]
            OffHeapExecution[å †å¤–æ‰§è¡Œå†…å­˜<br/>Off-Heap Execution]
            DirectMem[ç›´æ¥å†…å­˜<br/>DirectByteBuffer]
        end
        
        SparkMem --> Storage
        SparkMem --> Execution
        Storage -.->|åŠ¨æ€å€Ÿç”¨| Execution
        Execution -.->|åŠ¨æ€å€Ÿç”¨| Storage
    end
    
    style Reserved fill:#ffcdd2
    style UserMem fill:#f8bbd9  
    style Storage fill:#c8e6c9
    style Execution fill:#bbdefb
    style OffHeapStorage fill:#dcedc8
    style OffHeapExecution fill:#e1f5fe
```

### 1.2 å†…å­˜åŒºåŸŸåˆ’åˆ†è¯¦è§£

```scala
// Sparkå†…å­˜ç®¡ç†å™¨çš„æ ¸å¿ƒå®ç°
abstract class MemoryManager(
    conf: SparkConf,
    numCores: Int,
    onHeapStorageMemory: Long,
    onHeapExecutionMemory: Long
) {
  
  // 1. æœ€å¤§å †å†…å†…å­˜ = æœ€å¤§å †å¤§å° - é¢„ç•™å†…å­˜
  protected val maxHeapMemory = {
    val systemMemory = conf.getLong("spark.testing.memory", Runtime.getRuntime.maxMemory)
    val reservedMemory = conf.getLong("spark.testing.reservedMemory", 
                                      math.min(systemMemory * 0.3, 300 * 1024 * 1024))
    systemMemory - reservedMemory
  }
  
  // 2. Sparkå†…å­˜ = æœ€å¤§å †å†…å†…å­˜ * spark.memory.fraction(é»˜è®¤0.6)
  protected val maxOnHeapStorageMemory = onHeapStorageMemory
  protected val maxOnHeapExecutionMemory = onHeapExecutionMemory
  
  // 3. å †å¤–å†…å­˜é…ç½®
  protected val maxOffHeapMemory = conf.getSizeAsBytes("spark.memory.offHeap.size", 0)
  protected val offHeapStorageMemory = 
    (maxOffHeapMemory * conf.getDouble("spark.memory.storageFraction", 0.5)).toLong
  protected val offHeapExecutionMemory = maxOffHeapMemory - offHeapStorageMemory
}
```

### 1.3 å†…å­˜è®¡ç®—å…¬å¼

```scala
// å†…å­˜åˆ†é…è®¡ç®—å…¬å¼
object SparkMemoryCalculator {
  
  def calculateMemory(heapSize: Long, sparkConf: SparkConf): MemoryInfo = {
    
    // 1. é¢„ç•™å†…å­˜ï¼šmax(heapSize * 0.3, 300MB)
    val reservedMemory = math.min(heapSize * 0.3, 300 * 1024 * 1024).toLong
    
    // 2. å¯ç”¨å†…å­˜ = å †å¤§å° - é¢„ç•™å†…å­˜  
    val usableMemory = heapSize - reservedMemory
    
    // 3. Sparkå†…å­˜ = å¯ç”¨å†…å­˜ * spark.memory.fraction (é»˜è®¤0.6)
    val memoryFraction = sparkConf.getDouble("spark.memory.fraction", 0.6)
    val sparkMemory = (usableMemory * memoryFraction).toLong
    
    // 4. ç”¨æˆ·å†…å­˜ = å¯ç”¨å†…å­˜ - Sparkå†…å­˜
    val userMemory = usableMemory - sparkMemory
    
    // 5. å­˜å‚¨å†…å­˜ = Sparkå†…å­˜ * spark.memory.storageFraction (é»˜è®¤0.5)
    val storageFraction = sparkConf.getDouble("spark.memory.storageFraction", 0.5)
    val storageMemory = (sparkMemory * storageFraction).toLong
    
    // 6. æ‰§è¡Œå†…å­˜ = Sparkå†…å­˜ - å­˜å‚¨å†…å­˜
    val executionMemory = sparkMemory - storageMemory
    
    MemoryInfo(
      heapSize = heapSize,
      reservedMemory = reservedMemory,
      userMemory = userMemory,
      storageMemory = storageMemory, 
      executionMemory = executionMemory
    )
  }
}

// å®é™…è®¡ç®—ç¤ºä¾‹ï¼ˆ4GBå †å†…å­˜ï¼‰
val memoryInfo = SparkMemoryCalculator.calculateMemory(4096L * 1024 * 1024, sparkConf)
/*
ç»“æœï¼š
- æ€»å †å†…å­˜ï¼š4096 MB
- é¢„ç•™å†…å­˜ï¼š300 MB  
- ç”¨æˆ·å†…å­˜ï¼š1518 MB (37%)
- Sparkå†…å­˜ï¼š2278 MB (60% * 3796)
  - å­˜å‚¨å†…å­˜ï¼š1139 MB (50% * 2278)  
  - æ‰§è¡Œå†…å­˜ï¼š1139 MB (50% * 2278)
*/
```

---

## 2. ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶

### 2.1 ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨ï¼ˆUnifiedMemoryManagerï¼‰

**Spark 1.6å¼•å…¥ç»Ÿä¸€å†…å­˜ç®¡ç†ï¼Œè§£å†³äº†é™æ€å†…å­˜ç®¡ç†çš„é—®é¢˜**

```scala
private[spark] class UnifiedMemoryManager(
    conf: SparkConf,
    val maxHeapMemory: Long,
    onHeapStorageRegionSize: Long,
    numCores: Int
) extends MemoryManager(
  conf,
  numCores,  
  onHeapStorageRegionSize,
  maxHeapMemory - onHeapStorageRegionSize
) {
  
  // å­˜å‚¨å†…å­˜æ± 
  private val onHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.ON_HEAP)
  private val offHeapStorageMemoryPool = new StorageMemoryPool(this, MemoryMode.OFF_HEAP)
  
  // æ‰§è¡Œå†…å­˜æ±   
  private val onHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.ON_HEAP)
  private val offHeapExecutionMemoryPool = new ExecutionMemoryPool(this, MemoryMode.OFF_HEAP)
  
  override def acquireStorageMemory(
      blockId: BlockId,
      numBytes: Long,
      memoryMode: MemoryMode): Boolean = synchronized {
    
    val (executionPool, storagePool, maxMemory) = memoryMode match {
      case MemoryMode.ON_HEAP => 
        (onHeapExecutionMemoryPool, onHeapStorageMemoryPool, maxOnHeapStorageMemory)
      case MemoryMode.OFF_HEAP =>
        (offHeapExecutionMemoryPool, offHeapStorageMemoryPool, maxOffHeapStorageMemory)
    }
    
    // 1. å°è¯•åœ¨å­˜å‚¨å†…å­˜æ± ä¸­åˆ†é…
    if (storagePool.memoryFree >= numBytes) {
      storagePool.acquireMemory(blockId, numBytes)
      return true
    }
    
    // 2. å°è¯•ä»æ‰§è¡Œå†…å­˜æ± å€Ÿç”¨
    val memoryBorrowedFromExecution = math.max(0, 
      math.min(executionPool.memoryFree, numBytes - storagePool.memoryFree))
    
    if (memoryBorrowedFromExecution > 0) {
      executionPool.decrementPoolSize(memoryBorrowedFromExecution)
      storagePool.incrementPoolSize(memoryBorrowedFromExecution)
    }
    
    storagePool.acquireMemory(blockId, numBytes)
  }
}
```

### 2.2 åŠ¨æ€å†…å­˜å…±äº«æœºåˆ¶

```mermaid
graph TB
    subgraph "åŠ¨æ€å†…å­˜å…±äº«ç¤ºä¾‹"
        subgraph "åˆå§‹çŠ¶æ€"
            S1[å­˜å‚¨å†…å­˜<br/>1GB<br/>å¯ç”¨ï¼š1GB]
            E1[æ‰§è¡Œå†…å­˜<br/>1GB<br/>å¯ç”¨ï¼š1GB]
        end
        
        subgraph "æ‰§è¡Œå†…å­˜ä¸è¶³æ—¶"
            S2[å­˜å‚¨å†…å­˜<br/>0.5GB<br/>è¢«å€Ÿç”¨ï¼š0.5GB]
            E2[æ‰§è¡Œå†…å­˜<br/>1.5GB<br/>è·å¾—ï¼š0.5GB]
        end
        
        subgraph "å­˜å‚¨å†…å­˜ä¸è¶³æ—¶"
            S3[å­˜å‚¨å†…å­˜<br/>1.3GB<br/>è·å¾—ï¼š0.3GB]
            E3[æ‰§è¡Œå†…å­˜<br/>0.7GB<br/>è¢«å€Ÿç”¨ï¼š0.3GB]
        end
    end
    
    S1 --> S2
    E1 --> E2
    S2 --> S3  
    E2 --> E3
    
    style S1 fill:#c8e6c9
    style E1 fill:#bbdefb
    style S2 fill:#ffccbc
    style E2 fill:#c5e1a5
    style S3 fill:#a5d6a7
    style E3 fill:#ffcdd2
```

### 2.3 å†…å­˜é©±é€æœºåˆ¶

```scala
// å­˜å‚¨å†…å­˜æ± çš„é©±é€ç­–ç•¥
private[memory] class StorageMemoryPool(
    lock: Object,
    memoryMode: MemoryMode
) extends MemoryPool(lock) {
  
  // LRUé©±é€ç­–ç•¥
  private val memoryStore = new MemoryStore(conf, blockInfoManager, 
                                           serializerManager, memoryManager, this)
  
  def evictBlocksToFreeSpace(
      blockId: Option[BlockId],
      space: Long,
      memoryMode: MemoryMode): Long = {
    
    // 1. é€‰æ‹©è¦é©±é€çš„Blockï¼ˆLRUç­–ç•¥ï¼‰
    val blocksToEvict = new mutable.ArrayBuffer[BlockId]
    var freedMemory = 0L
    
    val iterator = memoryStore.entries.entrySet().iterator()
    while (iterator.hasNext && freedMemory < space) {
      val entry = iterator.next()
      val blockInfo = entry.getValue
      
      // è·³è¿‡æ­£åœ¨ä½¿ç”¨çš„Block
      if (blockInfo.readerCount == 0 && blockInfo.writerTask == TaskContext.get()) {
        blocksToEvict += entry.getKey
        freedMemory += blockInfo.size
      }
    }
    
    // 2. æ‰§è¡Œé©±é€
    blocksToEvict.foreach { blockId =>
      val removedBlock = memoryStore.remove(blockId)
      removedBlock.foreach { block =>
        // å†™å…¥ç£ç›˜æˆ–ä¸¢å¼ƒ
        spillToDisk(block)
      }
    }
    
    freedMemory
  }
}
```

---

## 3. Shuffleæœºåˆ¶è¯¦è§£

### 3.1 Shuffleæ¦‚è¿°ä¸åˆ†ç±»

**Shuffleæ˜¯Sparkä¸­æœ€å¤æ‚å’Œæœ€é‡è¦çš„æœºåˆ¶ä¹‹ä¸€ï¼Œè´Ÿè´£æ•°æ®é‡åˆ†å¸ƒ**

```mermaid
graph TB
    subgraph "Shuffleè¿‡ç¨‹æ¦‚è§ˆ"
        subgraph "Mapé˜¶æ®µ"
            Task1[Task 1<br/>å¤„ç†åˆ†åŒº1æ•°æ®]
            Task2[Task 2<br/>å¤„ç†åˆ†åŒº2æ•°æ®]
            Task3[Task 3<br/>å¤„ç†åˆ†åŒº3æ•°æ®]
        end
        
        subgraph "Shuffle Write"
            Writer1[Shuffle Writer<br/>å†™å…¥æœ¬åœ°ç£ç›˜]
            Writer2[Shuffle Writer<br/>å†™å…¥æœ¬åœ°ç£ç›˜]
            Writer3[Shuffle Writer<br/>å†™å…¥æœ¬åœ°ç£ç›˜]
        end
        
        subgraph "Shuffle Read"
            Reader1[Shuffle Reader<br/>ç½‘ç»œè¯»å–æ•°æ®]
            Reader2[Shuffle Reader<br/>ç½‘ç»œè¯»å–æ•°æ®]
        end
        
        subgraph "Reduceé˜¶æ®µ"
            Reduce1[Task 1<br/>å¤„ç†é‡åˆ†å¸ƒæ•°æ®]
            Reduce2[Task 2<br/>å¤„ç†é‡åˆ†å¸ƒæ•°æ®]
        end
        
        Task1 --> Writer1
        Task2 --> Writer2  
        Task3 --> Writer3
        
        Writer1 -.->|ç½‘ç»œä¼ è¾“| Reader1
        Writer1 -.->|ç½‘ç»œä¼ è¾“| Reader2
        Writer2 -.->|ç½‘ç»œä¼ è¾“| Reader1
        Writer2 -.->|ç½‘ç»œä¼ è¾“| Reader2
        Writer3 -.->|ç½‘ç»œä¼ è¾“| Reader1
        Writer3 -.->|ç½‘ç»œä¼ è¾“| Reader2
        
        Reader1 --> Reduce1
        Reader2 --> Reduce2
    end
    
    style Writer1 fill:#ffcdd2
    style Writer2 fill:#ffcdd2  
    style Writer3 fill:#ffcdd2
    style Reader1 fill:#c8e6c9
    style Reader2 fill:#c8e6c9
```

### 3.2 Shuffle Manageræ¼”è¿›

```scala
// Shuffle Managerçš„æ¼”è¿›å†å²
trait ShuffleManager {
  
  def registerShuffle[K, V, C](
      shuffleId: Int,
      numMaps: Int,
      dependency: ShuffleDependency[K, V, C]): ShuffleHandle
      
  def getWriter[K, V](
      handle: ShuffleHandle,
      mapId: Int,
      context: TaskContext): ShuffleWriter[K, V]
      
  def getReader[K, C](
      handle: ShuffleHandle,
      startPartition: Int,
      endPartition: Int,
      context: TaskContext): ShuffleReader[K, C]
}

// 1. Hash-based Shuffle (Spark 0.8-1.1, å·²åºŸå¼ƒ)
class HashShuffleManager extends ShuffleManager {
  // é—®é¢˜ï¼šäº§ç”Ÿå¤§é‡å°æ–‡ä»¶ M*Rä¸ªæ–‡ä»¶
  // Mä¸ªMapä»»åŠ¡ Ã— Rä¸ªReduceä»»åŠ¡ = M*Rä¸ªè¾“å‡ºæ–‡ä»¶
}

// 2. Sort-based Shuffle (Spark 1.2+, é»˜è®¤)  
class SortShuffleManager extends ShuffleManager {
  // æ”¹è¿›ï¼šæ¯ä¸ªMapä»»åŠ¡åªäº§ç”Ÿ1ä¸ªæ•°æ®æ–‡ä»¶å’Œ1ä¸ªç´¢å¼•æ–‡ä»¶
  // æ€»æ–‡ä»¶æ•°ï¼š2*Mä¸ªæ–‡ä»¶
  
  override def getWriter[K, V](
      handle: ShuffleHandle,
      mapId: Int,
      context: TaskContext): ShuffleWriter[K, V] = {
    
    handle match {
      case unsafeShuffleHandle: SerializedShuffleHandle[K @unchecked, V @unchecked] =>
        // Unsafe Shuffle Writerï¼šæ€§èƒ½æœ€ä¼˜
        new UnsafeShuffleWriter(...)
        
      case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =>
        // Bypass Shuffle Writerï¼šå°‘é‡åˆ†åŒºæ—¶ä½¿ç”¨
        new BypassMergeSortShuffleWriter(...)
        
      case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =>
        // Sort Shuffle Writerï¼šé€šç”¨æƒ…å†µ
        new SortShuffleWriter(...)
    }
  }
}
```

### 3.3 SortShuffleWriterè¯¦è§£

```scala
private[spark] class SortShuffleWriter[K, V, C](
    shuffleBlockResolver: IndexShuffleBlockResolver,
    handle: BaseShuffleHandle[K, V, C],
    mapId: Int,
    context: TaskContext
) extends ShuffleWriter[K, V] with Logging {
  
  private val dep = handle.dependency
  private val blockManager = SparkEnv.get.blockManager
  private val sorter: ExternalSorter[K, V, _] = new ExternalSorter[K, V, C](
    context, dep.aggregator, Some(dep.partitioner), dep.keyOrdering, dep.serializer)
  
  override def write(records: Iterator[Product2[K, V]]): Unit = {
    // 1. æ’å…¥æ•°æ®åˆ°å¤–éƒ¨æ’åºå™¨
    sorter.insertAll(records)
    
    // 2. è·å–è¾“å‡ºæ–‡ä»¶
    val output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)
    val tmp = Utils.tempFileWith(output)
    
    try {
      val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)
      
      // 3. å†™å…¥æ’åºåçš„æ•°æ®å¹¶ç”Ÿæˆåˆ†åŒºç´¢å¼•
      val partitionLengths = sorter.writePartitionedFile(blockId, tmp)
      
      // 4. å†™å…¥ç´¢å¼•æ–‡ä»¶
      shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)
      
      // 5. æ›´æ–°Shuffleå†™å…¥åº¦é‡
      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)
      
    } catch {
      case e: Exception =>
        if (tmp.exists()) tmp.delete()
        throw e
    }
  }
}
```

### 3.4 å¤–éƒ¨æ’åºå™¨ï¼ˆExternalSorterï¼‰

```scala
private[spark] class ExternalSorter[K, V, C](
    context: TaskContext,
    aggregator: Option[Aggregator[K, V, C]] = None,
    partitioner: Option[Partitioner] = None,  
    ordering: Option[Ordering[K]] = None,
    serializer: Serializer = SparkEnv.get.serializer
) extends Spillable[WritablePartitionedPairCollection[K, C]](context.taskMemoryManager())
  with Logging {
  
  // å†…å­˜ä¸­çš„æ•°æ®ç»“æ„
  private var map = new PartitionedAppendOnlyMap[K, C]
  private var buffer = new PartitionedPairBuffer[K, C]
  
  // æº¢å†™æ–‡ä»¶åˆ—è¡¨
  private val spills = new ArrayBuffer[SpilledFile]
  
  def insertAll(records: Iterator[Product2[K, V]]): Unit = {
    val shouldCombine = aggregator.isDefined
    
    if (shouldCombine) {
      // éœ€è¦é¢„èšåˆï¼šä½¿ç”¨AppendOnlyMap
      val mergeValue = aggregator.get.mergeValue
      val createCombiner = aggregator.get.createCombiner
      
      while (records.hasNext) {
        val kv = records.next()
        map.changeValue(kv._1, createCombiner(kv._2), mergeValue)
        maybeSpillCollection(usingMap = true)  // æ£€æŸ¥æ˜¯å¦éœ€è¦æº¢å†™
      }
    } else {
      // ä¸éœ€è¦é¢„èšåˆï¼šä½¿ç”¨PairBuffer  
      while (records.hasNext) {
        val kv = records.next()
        buffer.insert(kv._1, kv._2.asInstanceOf[C])
        maybeSpillCollection(usingMap = false)  // æ£€æŸ¥æ˜¯å¦éœ€è¦æº¢å†™
      }
    }
  }
  
  // å†…å­˜ä¸è¶³æ—¶æº¢å†™åˆ°ç£ç›˜
  protected[this] override def spill(collection: WritablePartitionedPairCollection[K, C]): Unit = {
    val inMemoryIterator = collection.destructiveSortedWritablePartitionedIterator(comparator)
    val spillFile = spillMemoryIteratorToDisk(inMemoryIterator)
    spills += spillFile
  }
}
```

### 3.5 UnsafeShuffleWriterä¼˜åŒ–

**UnsafeShuffleWriteræ˜¯æ€§èƒ½æœ€ä¼˜çš„Shuffle Writerï¼Œä½¿ç”¨å †å¤–å†…å­˜å’Œåºåˆ—åŒ–æ’åº**

```scala
public final class UnsafeShuffleWriter<K, V> extends ShuffleWriter<K, V> {
  
  private final int numPartitions;
  private final TaskMemoryManager memoryManager;
  private final SerializerInstance serializer;
  private final ClassTag<V> valueClassTag;
  
  // æ ¸å¿ƒæ•°æ®ç»“æ„ï¼šShuffleInMemorySorter
  private ShuffleInMemorySorter sorter;
  private MemoryBlock allocatedPages[];
  
  @Override
  public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {
    
    // 1. åˆå§‹åŒ–å†…å­˜æ’åºå™¨
    if (sorter == null) {
      sorter = new ShuffleInMemorySorter(
        memoryManager, (int) Math.min(PackedRecordPointer.MAXIMUM_PARTITION_ID + 1, numPartitions));
    }
    
    // 2. åºåˆ—åŒ–å¹¶æ’å…¥è®°å½•
    final SerializationStream ser = serializer.serializeStream(writeBuffer);
    
    while (records.hasNext()) {
      final Product2<K, V> record = records.next();
      final K key = record._1();
      final V value = record._2();
      
      // è®¡ç®—åˆ†åŒºID
      final int partitionId = partitioner.getPartition(key);
      
      // åºåˆ—åŒ–é”®å€¼å¯¹
      ser.writeKey(key, valueClassTag);
      ser.writeValue(value, valueClassTag);
      ser.flush();
      
      // æ’å…¥åˆ°å†…å­˜æ’åºå™¨
      final int recordLength = writeBuffer.size();
      sorter.insertRecord(writeBuffer.getBuf(), writeBuffer.size(), partitionId);
      
      // æ£€æŸ¥æ˜¯å¦éœ€è¦æº¢å†™
      if (!sorter.hasSpaceForAnotherRecord()) {
        spill();
      }
    }
    
    // 3. æœ€ç»ˆæ’åºå’Œå†™å…¥
    closeAndWriteOutput();
  }
}
```

---

## 4. å­˜å‚¨ç³»ç»ŸBlockManager

### 4.1 BlockManageræ¶æ„

```mermaid
graph TB
    subgraph "BlockManagerå­˜å‚¨æ¶æ„"
        subgraph "å­˜å‚¨çº§åˆ«"
            Memory[å†…å­˜å­˜å‚¨<br/>MemoryStore]
            Disk[ç£ç›˜å­˜å‚¨<br/>DiskStore] 
            Remote[è¿œç¨‹å­˜å‚¨<br/>RemoteBlockStore]
        end
        
        subgraph "ç®¡ç†ç»„ä»¶"
            BM[BlockManager<br/>ç»Ÿä¸€ç®¡ç†æ¥å£]
            BMM[BlockManagerMaster<br/>å…ƒæ•°æ®ç®¡ç†]
            BIM[BlockInfoManager<br/>Blockä¿¡æ¯ç®¡ç†]
            MT[MemoryTracker<br/>å†…å­˜è·Ÿè¸ª]
        end
        
        subgraph "ç½‘ç»œç»„ä»¶"
            BS[BlockTransferService<br/>æ•°æ®ä¼ è¾“æœåŠ¡]
            SS[ShuffleService<br/>ShuffleæœåŠ¡]
        end
        
        BM --> Memory
        BM --> Disk
        BM --> Remote
        BM --> BIM
        BM --> BMM
        Memory --> MT
        BM --> BS
        BS --> SS
    end
    
    style Memory fill:#c8e6c9
    style Disk fill:#ffcc80
    style Remote fill:#b39ddb
    style BM fill:#e3f2fd
    style BMM fill:#f8bbd9
```

### 4.2 å­˜å‚¨çº§åˆ«è¯¦è§£

```scala
// Sparkæä¾›çš„å­˜å‚¨çº§åˆ«
object StorageLevel {
  
  val NONE = new StorageLevel(false, false, false, false)
  
  // ä»…å†…å­˜
  val MEMORY_ONLY = new StorageLevel(true, false, false, false)
  val MEMORY_ONLY_2 = new StorageLevel(true, false, false, false, 2)  // 2å‰¯æœ¬
  
  // å†…å­˜+ç£ç›˜  
  val MEMORY_AND_DISK = new StorageLevel(true, true, false, false)
  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, false, 2)
  
  // åºåˆ—åŒ–å­˜å‚¨
  val MEMORY_ONLY_SER = new StorageLevel(true, false, false, true)
  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, true)
  
  // å †å¤–å†…å­˜
  val OFF_HEAP = new StorageLevel(true, true, true, true)
}

class StorageLevel private(
    private var _useDisk: Boolean,      // æ˜¯å¦ä½¿ç”¨ç£ç›˜
    private var _useMemory: Boolean,    // æ˜¯å¦ä½¿ç”¨å†…å­˜  
    private var _useOffHeap: Boolean,   // æ˜¯å¦ä½¿ç”¨å †å¤–å†…å­˜
    private var _deserialized: Boolean, // æ˜¯å¦ååºåˆ—åŒ–å­˜å‚¨
    private var _replication: Int = 1   // å‰¯æœ¬æ•°é‡
) extends Externalizable {
  
  // å­˜å‚¨çº§åˆ«é€‰æ‹©ç®—æ³•
  def isValid: Boolean = {
    (_useMemory || _useDisk) && (_replication > 0)
  }
}
```

### 4.3 MemoryStoreå®ç°

```scala
private[spark] class MemoryStore(
    conf: SparkConf,
    blockInfoManager: BlockInfoManager,
    serializerManager: SerializerManager,
    memoryManager: MemoryManager,
    blockEvictionHandler: BlockEvictionHandler
) extends BlockStore(conf) with Logging {
  
  // å†…å­˜ä¸­çš„Blockå­˜å‚¨
  private val entries = new LinkedHashMap[BlockId, MemoryEntry[_]](32, 0.75f, true)
  
  def putBytes[T: ClassTag](
      blockId: BlockId,
      size: Long, 
      memoryMode: MemoryMode,
      _bytes: () => ChunkedByteBuffer): Boolean = {
    
    // 1. ç”³è¯·å†…å­˜ç©ºé—´
    val putSucceeded = memoryManager.acquireStorageMemory(blockId, size, memoryMode)
    
    if (putSucceeded) {
      // 2. åºåˆ—åŒ–æ•°æ®
      val bytes = _bytes()
      
      // 3. å­˜å‚¨åˆ°å†…å­˜
      val entry = new SerializedMemoryEntry[T](bytes, memoryMode, classTag[T])
      entries.synchronized {
        entries.put(blockId, entry)
      }
      true
    } else {
      false
    }
  }
  
  def putIteratorAsValues[T](
      blockId: BlockId,
      values: Iterator[T],
      classTag: ClassTag[T]): Either[PartiallyUnrolledIterator[T], Long] = {
    
    var elementsUnrolled = 0
    val vector = new SizeTrackingVector[T]()(classTag)
    var keepUnrolling = true
    val initialMemoryThreshold = unrollMemoryThreshold
    var memoryThreshold = initialMemoryThreshold
    
    // é€æ­¥å±•å¼€Iteratorå¹¶ç›‘æ§å†…å­˜ä½¿ç”¨
    while (values.hasNext && keepUnrolling) {
      vector += values.next()
      elementsUnrolled += 1
      
      if (elementsUnrolled % memoryCheckPeriod == 0) {
        val currentSize = vector.estimateSize()
        if (currentSize >= memoryThreshold) {
          // è¯·æ±‚æ›´å¤šå†…å­˜
          val amountToRequest = (currentSize * memoryGrowthFactor - memoryThreshold).toLong
          keepUnrolling = memoryManager.acquireStorageMemory(blockId, amountToRequest, MemoryMode.ON_HEAP)
          
          if (keepUnrolling) {
            memoryThreshold += amountToRequest
          }
        }
      }
    }
    
    if (keepUnrolling) {
      // å®Œå…¨å±•å¼€æˆåŠŸ
      Right(vector.estimateSize())
    } else {
      // å†…å­˜ä¸è¶³ï¼Œè¿”å›éƒ¨åˆ†å±•å¼€çš„è¿­ä»£å™¨
      Left(new PartiallyUnrolledIterator(this, MemoryMode.ON_HEAP, unrolledMemory, 
                                        unrolled.iterator, values))
    }
  }
}
```

---

## 5. å†…å­˜è°ƒä¼˜ç­–ç•¥

### 5.1 å†…å­˜é…ç½®ä¼˜åŒ–

```scala
// å†…å­˜ç›¸å…³çš„å…³é”®é…ç½®å‚æ•°
val optimizedSparkConf = new SparkConf()
  
  // 1. åŸºç¡€å†…å­˜é…ç½®
  .set("spark.executor.memory", "8g")                    // æ‰§è¡Œå™¨å †å†…å­˜
  .set("spark.executor.cores", "4")                      // CPUæ ¸æ•°
  .set("spark.executor.instances", "10")                 // æ‰§è¡Œå™¨å®ä¾‹æ•°
  
  // 2. å†…å­˜åˆ†å¸ƒé…ç½®  
  .set("spark.memory.fraction", "0.8")                   // Sparkå†…å­˜å æ¯”ï¼ˆé»˜è®¤0.6ï¼‰
  .set("spark.memory.storageFraction", "0.3")            // å­˜å‚¨å†…å­˜å æ¯”ï¼ˆé»˜è®¤0.5ï¼‰
  
  // 3. å †å¤–å†…å­˜é…ç½®
  .set("spark.memory.offHeap.enabled", "true")           // å¯ç”¨å †å¤–å†…å­˜
  .set("spark.memory.offHeap.size", "2g")                // å †å¤–å†…å­˜å¤§å°
  
  // 4. GCä¼˜åŒ–é…ç½®
  .set("spark.executor.extraJavaOptions", 
       "-XX:+UseG1GC " +                                 // ä½¿ç”¨G1GC
       "-XX:+UnlockExperimentalVMOptions " +
       "-XX:+UseZGC " +                                  // æˆ–ä½¿ç”¨ZGCï¼ˆJDK11+ï¼‰
       "-XX:MaxGCPauseMillis=200 " +                     // æœ€å¤§GCæš‚åœæ—¶é—´
       "-XX:G1HeapRegionSize=16m")                       // G1å †åŒºåŸŸå¤§å°
  
  // 5. åºåˆ—åŒ–ä¼˜åŒ–
  .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
  .set("spark.kryoserializer.buffer.max", "64m")
```

### 5.2 ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

```scala
// æ™ºèƒ½ç¼“å­˜ç­–ç•¥é€‰æ‹©
object CacheStrategyOptimizer {
  
  def chooseBestStorageLevel(
      dataSize: Long, 
      accessPattern: AccessPattern,
      memoryAvailable: Long,
      cpuIntensive: Boolean
  ): StorageLevel = {
    
    (dataSize, accessPattern, memoryAvailable, cpuIntensive) match {
      
      // å°æ•°æ®é›†ï¼Œé¢‘ç¹è®¿é—® -> å†…å­˜å­˜å‚¨  
      case (size, AccessPattern.FREQUENT, mem, _) if size < mem * 0.3 =>
        StorageLevel.MEMORY_ONLY
      
      // å¤§æ•°æ®é›†ï¼Œå¶å°”è®¿é—® -> åºåˆ—åŒ–å†…å­˜å­˜å‚¨
      case (size, AccessPattern.OCCASIONAL, mem, false) if size < mem * 0.6 =>
        StorageLevel.MEMORY_ONLY_SER
        
      // éå¸¸å¤§çš„æ•°æ®é›† -> å†…å­˜+ç£ç›˜å­˜å‚¨
      case (size, _, mem, _) if size > mem =>
        StorageLevel.MEMORY_AND_DISK_SER
        
      // CPUå¯†é›†å‹ä»»åŠ¡ -> é¿å…åºåˆ—åŒ–å¼€é”€
      case (_, _, _, true) =>
        StorageLevel.MEMORY_AND_DISK
        
      // é»˜è®¤ç­–ç•¥
      case _ =>
        StorageLevel.MEMORY_AND_DISK
    }
  }
  
  // ç¼“å­˜ä½¿ç”¨ç¤ºä¾‹
  def optimizedCaching(df: DataFrame): DataFrame = {
    val estimatedSize = df.queryExecution.logical.stats.sizeInBytes.longValue()
    val storageLevel = chooseBestStorageLevel(estimatedSize, 
                                            AccessPattern.FREQUENT, 
                                            getAvailableMemory(), 
                                            false)
    
    df.persist(storageLevel)
  }
}
```

### 5.3 å†…å­˜æ³„æ¼æ£€æµ‹ä¸é¢„é˜²

```scala
// å†…å­˜æ³„æ¼æ£€æµ‹å™¨
class MemoryLeakDetector(sparkContext: SparkContext) extends Logging {
  
  private val memoryUsageHistory = mutable.ArrayBuffer[MemorySnapshot]()
  
  def checkMemoryLeak(): Unit = {
    val currentSnapshot = captureMemorySnapshot()
    memoryUsageHistory += currentSnapshot
    
    if (memoryUsageHistory.size > 10) {
      // åˆ†æå†…å­˜ä½¿ç”¨è¶‹åŠ¿
      val trend = analyzeMemoryTrend(memoryUsageHistory.takeRight(10))
      
      if (trend.isLeaking) {
        logWarning(s"Potential memory leak detected: ${trend.description}")
        
        // å»ºè®®çš„è§£å†³æ–¹æ¡ˆ
        suggest(trend.recommendedActions)
      }
      
      // ä¿æŒå†å²è®°å½•åœ¨åˆç†èŒƒå›´å†…
      if (memoryUsageHistory.size > 100) {
        memoryUsageHistory.remove(0, 50)
      }
    }
  }
  
  private def captureMemorySnapshot(): MemorySnapshot = {
    val runtime = Runtime.getRuntime
    val statusStore = sparkContext.statusStore
    
    MemorySnapshot(
      timestamp = System.currentTimeMillis(),
      heapUsed = runtime.totalMemory() - runtime.freeMemory(),
      heapMax = runtime.maxMemory(),
      executorMemoryUsed = statusStore.executorInfos.map(_.memoryUsed).sum,
      storageMemoryUsed = statusStore.rddInfos.map(_.memoryUsed).sum,
      activeTasks = statusStore.activeStages.map(_.numActiveTasks).sum
    )
  }
}

// å†…å­˜æ³„æ¼é¢„é˜²æœ€ä½³å®è·µ
object MemoryLeakPrevention {
  
  // 1. åŠæ—¶é‡Šæ”¾å¤§å¯¹è±¡çš„å¼•ç”¨
  def processLargeDataset(data: Dataset[_]): Unit = {
    val processedData = data.map(processRecord)
    processedData.write.mode("overwrite").parquet("output/path")
    
    // æ˜¾å¼è§£é™¤æŒä¹…åŒ–
    data.unpersist(blocking = true)
    processedData.unpersist(blocking = true)
  }
  
  // 2. ä½¿ç”¨checkpointæˆªæ–­è¡€ç¼˜å…³ç³»
  def preventLongLineage(rdd: RDD[_]): RDD[_] = {
    if (rdd.dependencies.size > 10) {  // è¡€ç¼˜è¿‡é•¿
      rdd.checkpoint()  // è®¾ç½®æ£€æŸ¥ç‚¹
      rdd
    } else {
      rdd
    }
  }
  
  // 3. åˆç†ä½¿ç”¨å¹¿æ’­å˜é‡
  def broadcastLargeVariable[T](sc: SparkContext, data: T): Broadcast[T] = {
    val broadcast = sc.broadcast(data)
    
    // åœ¨ä½œä¸šç»“æŸåæ¸…ç†å¹¿æ’­å˜é‡
    sc.addSparkListener(new SparkListener {
      override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {
        broadcast.destroy(blocking = false)
      }
    })
    
    broadcast
  }
}
```

---

## 6. Shuffleä¼˜åŒ–å®è·µ

### 6.1 Shuffleå‚æ•°è°ƒä¼˜

```scala
// Shuffleæ€§èƒ½ä¼˜åŒ–é…ç½®
val shuffleOptimizedConf = new SparkConf()
  
  // 1. Shuffleå¹¶è¡Œåº¦é…ç½®
  .set("spark.sql.shuffle.partitions", "800")           // é»˜è®¤200ï¼Œå¯æ ¹æ®æ•°æ®é‡è°ƒæ•´
  .set("spark.default.parallelism", "400")              // RDDé»˜è®¤å¹¶è¡Œåº¦
  
  // 2. Shuffleå†…å­˜é…ç½®  
  .set("spark.shuffle.memoryFraction", "0.6")           // Shuffleå†…å­˜å æ¯”
  .set("spark.shuffle.spill.compress", "true")          // å‹ç¼©æº¢å†™æ–‡ä»¶
  .set("spark.shuffle.compress", "true")                // å‹ç¼©Shuffleè¾“å‡º
  
  // 3. Shuffle I/Oä¼˜åŒ–
  .set("spark.shuffle.file.buffer", "64k")              // æ–‡ä»¶ç¼“å†²åŒºå¤§å°
  .set("spark.reducer.maxSizeInFlight", "96m")          // Shuffleè¯»å–ç¼“å†²åŒº
  .set("spark.reducer.maxReqsInFlight", "Int.MaxValue") // æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
  
  // 4. Shuffleæ’åºé…ç½®
  .set("spark.shuffle.sort.bypassMergeThreshold", "400") // ç»•è¿‡æ’åºé˜ˆå€¼
  .set("spark.shuffle.spill.numElementsForceSpillThreshold", "1000000") // å¼ºåˆ¶æº¢å†™é˜ˆå€¼
  
  // 5. ç½‘ç»œé…ç½®ä¼˜åŒ–
  .set("spark.network.timeout", "800s")                 // ç½‘ç»œè¶…æ—¶
  .set("spark.shuffle.io.retryWait", "60s")             // é‡è¯•ç­‰å¾…æ—¶é—´
  .set("spark.shuffle.io.maxRetries", "5")              // æœ€å¤§é‡è¯•æ¬¡æ•°
```

### 6.2 æ•°æ®å€¾æ–œä¼˜åŒ–ç­–ç•¥

```scala
// æ•°æ®å€¾æ–œæ£€æµ‹ä¸ä¼˜åŒ–
object DataSkewOptimizer {
  
  // 1. æ£€æµ‹æ•°æ®å€¾æ–œ
  def detectDataSkew(df: DataFrame, groupByCol: String): SkewReport = {
    val partitionSizes = df.mapPartitions { iter =>
      val counts = mutable.Map[String, Long]()
      iter.foreach { row =>
        val key = row.getAs[String](groupByCol)
        counts(key) = counts.getOrElse(key, 0L) + 1
      }
      counts.iterator
    }.reduceByKey(_ + _)
    
    val stats = partitionSizes.mapValues(identity).collect().toMap
    val totalCount = stats.values.sum
    val avgCount = totalCount / stats.size
    val skewedKeys = stats.filter(_._2 > avgCount * 10).keys.toSeq  // 10å€äºå¹³å‡å€¼è®¤ä¸ºå€¾æ–œ
    
    SkewReport(
      totalKeys = stats.size,
      totalRecords = totalCount,
      skewedKeys = skewedKeys,
      skewRatio = skewedKeys.size.toDouble / stats.size
    )
  }
  
  // 2. å€¾æ–œKeyåŠ ç›å¤„ç†
  def saltSkewedJoin(
      leftDF: DataFrame, 
      rightDF: DataFrame, 
      joinKey: String,
      skewedKeys: Seq[String],
      saltFactor: Int = 100
  ): DataFrame = {
    
    import org.apache.spark.sql.functions._
    
    // ä¸ºå€¾æ–œçš„keyæ·»åŠ éšæœºç›å€¼
    val saltedLeft = leftDF.withColumn("salted_key", 
      when(col(joinKey).isin(skewedKeys: _*), 
           concat(col(joinKey), lit("_"), (rand() * saltFactor).cast("int")))
      .otherwise(col(joinKey))
    )
    
    // å³è¡¨å¯¹å€¾æ–œkeyè¿›è¡Œæ‰©å±•
    val skewedRightDF = rightDF
      .filter(col(joinKey).isin(skewedKeys: _*))
      .withColumn("salt_id", explode(array((0 until saltFactor).map(lit): _*)))
      .withColumn("salted_key", concat(col(joinKey), lit("_"), col("salt_id")))
      .drop("salt_id")
    
    val normalRightDF = rightDF
      .filter(!col(joinKey).isin(skewedKeys: _*))
      .withColumn("salted_key", col(joinKey))
    
    val expandedRightDF = skewedRightDF.union(normalRightDF)
    
    // æ‰§è¡ŒJoin
    saltedLeft.join(expandedRightDF, "salted_key")
      .drop("salted_key")
  }
  
  // 3. åŒé˜¶æ®µèšåˆä¼˜åŒ–
  def twoPhaseAggregation(df: DataFrame, groupCols: Seq[String], aggCol: String): DataFrame = {
    import org.apache.spark.sql.functions._
    
    // ç¬¬ä¸€é˜¶æ®µï¼šå±€éƒ¨èšåˆï¼ˆåŠ ç›ï¼‰
    val saltedDF = df.withColumn("salt", (rand() * 100).cast("int"))
    
    val localAgg = saltedDF
      .groupBy(groupCols :+ "salt": _*)
      .agg(sum(aggCol).as("local_sum"))
    
    // ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€èšåˆï¼ˆå»ç›ï¼‰  
    val globalAgg = localAgg
      .drop("salt")
      .groupBy(groupCols: _*)
      .agg(sum("local_sum").as(s"sum_$aggCol"))
    
    globalAgg
  }
}
```

### 6.3 è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œï¼ˆAQEï¼‰ä¼˜åŒ–

```scala
// Spark 3.0+ è‡ªé€‚åº”æŸ¥è¯¢æ‰§è¡Œä¼˜åŒ–
val aqeOptimizedConf = new SparkConf()
  
  // 1. å¯ç”¨AQE
  .set("spark.sql.adaptive.enabled", "true")
  
  // 2. è‡ªåŠ¨åˆå¹¶å°åˆ†åŒº
  .set("spark.sql.adaptive.coalescePartitions.enabled", "true")
  .set("spark.sql.adaptive.coalescePartitions.minPartitionNum", "1")
  .set("spark.sql.adaptive.coalescePartitions.initialPartitionNum", "800")
  .set("spark.sql.adaptive.advisoryPartitionSizeInBytes", "256MB")
  
  // 3. è‡ªåŠ¨å€¾æ–œJoinä¼˜åŒ–
  .set("spark.sql.adaptive.skewJoin.enabled", "true")
  .set("spark.sql.adaptive.skewJoin.skewedPartitionFactor", "5")
  .set("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "256MB")
  
  // 4. åŠ¨æ€Joinç­–ç•¥é€‰æ‹©
  .set("spark.sql.adaptive.localShuffleReader.enabled", "true")
  
// AQEä½¿ç”¨ç¤ºä¾‹
class AdaptiveQueryExecutionExample(spark: SparkSession) {
  
  def optimizedEtlPipeline(): Unit = {
    // AQEä¼šè‡ªåŠ¨ä¼˜åŒ–ä»¥ä¸‹æŸ¥è¯¢
    val result = spark.sql("""
      SELECT u.user_id, u.name, COUNT(*) as order_count, SUM(o.amount) as total_amount
      FROM users u
      JOIN orders o ON u.user_id = o.user_id
      WHERE u.registration_date >= '2023-01-01'
      GROUP BY u.user_id, u.name
      HAVING COUNT(*) > 5
      ORDER BY total_amount DESC
      LIMIT 1000
    """)
    
    result.write
      .mode("overwrite")
      .option("compression", "snappy")
      .parquet("output/user_order_summary")
  }
}
```

---

## æ€»ç»“

æœ¬ç« æ·±å…¥å­¦ä¹ äº†Sparkå†…å­˜ç®¡ç†ä¸Shuffleæœºåˆ¶çš„æ ¸å¿ƒåŸç†ï¼š

1. **å†…å­˜ç®¡ç†**ï¼šç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶ã€åŠ¨æ€å†…å­˜å…±äº«ã€å †å¤–å†…å­˜ä¼˜åŒ–
2. **Shuffleæœºåˆ¶**ï¼šä»HashShuffleåˆ°SortShuffleçš„æ¼”è¿›ã€UnsafeShuffleWriterä¼˜åŒ–
3. **å­˜å‚¨ç³»ç»Ÿ**ï¼šBlockManageræ¶æ„ã€å­˜å‚¨çº§åˆ«é€‰æ‹©ã€ç¼“å­˜ç­–ç•¥
4. **å†…å­˜è°ƒä¼˜**ï¼šå†…å­˜é…ç½®ä¼˜åŒ–ã€æ³„æ¼æ£€æµ‹ä¸é¢„é˜²ã€GCè°ƒä¼˜ç­–ç•¥
5. **Shuffleä¼˜åŒ–**ï¼šå‚æ•°è°ƒä¼˜ã€æ•°æ®å€¾æ–œå¤„ç†ã€AQEè‡ªé€‚åº”ä¼˜åŒ–
6. **æœ€ä½³å®è·µ**ï¼šåŸºäºå®é™…é¡¹ç›®ç»éªŒçš„ä¼˜åŒ–å»ºè®®å’Œä»£ç ç¤ºä¾‹

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ Spark Streamingä¸æµå¤„ç†æŠ€æœ¯ã€‚

---

## æ¨èé˜…è¯»
- [Sparkå®˜æ–¹æ–‡æ¡£ - Memory Management](https://spark.apache.org/docs/latest/tuning.html#memory-management)
- [æ·±åº¦è§£æSpark Shuffle](https://databricks.com/blog/2014/05/07/optimizing-spark-shuffle-for-large-scale-workloads.html)  
- ã€ŠSparkæ€§èƒ½ä¼˜åŒ–æŒ‡å—ã€‹ç¬¬4-6ç« 
```

```markdown
# Spark Streamingä¸æµå¤„ç†æ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [æµå¤„ç†æ¦‚è¿°ä¸æ¶æ„](#1-æµå¤„ç†æ¦‚è¿°ä¸æ¶æ„)
2. [DStream APIè¯¦è§£](#2-dstream-apiè¯¦è§£)
3. [Structured StreamingåŸç†](#3-structured-streamingåŸç†)
4. [æµæ‰¹ä¸€ä½“åŒ–æ¶æ„](#4-æµæ‰¹ä¸€ä½“åŒ–æ¶æ„)
5. [å®¹é”™ä¸çŠ¶æ€ç®¡ç†](#5-å®¹é”™ä¸çŠ¶æ€ç®¡ç†)
6. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#6-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)

---

## 1. æµå¤„ç†æ¦‚è¿°ä¸æ¶æ„

### 1.1 æµå¤„ç†åŸºæœ¬æ¦‚å¿µ

**æµå¤„ç† vs æ‰¹å¤„ç†çš„æœ¬è´¨åŒºåˆ«**

```mermaid
graph TB
    subgraph "æ‰¹å¤„ç†æ¨¡å¼"
        BatchData[å†å²æ•°æ®é›†<br/>æœ‰ç•Œæ•°æ®] --> BatchProcess[æ‰¹å¤„ç†å¼•æ“<br/>é«˜ååé‡]
        BatchProcess --> BatchResult[æ‰¹é‡ç»“æœ<br/>å»¶è¿Ÿè¾ƒé«˜]
    end
    
    subgraph "æµå¤„ç†æ¨¡å¼"  
        StreamData[å®æ—¶æ•°æ®æµ<br/>æ— ç•Œæ•°æ®] --> StreamProcess[æµå¤„ç†å¼•æ“<br/>ä½å»¶è¿Ÿ]
        StreamProcess --> StreamResult[å¢é‡ç»“æœ<br/>è¿‘å®æ—¶]
    end
    
    subgraph "æµæ‰¹ä¸€ä½“"
        UnifiedData[ç»Ÿä¸€æ•°æ®æŠ½è±¡] --> UnifiedEngine[ç»Ÿä¸€å¤„ç†å¼•æ“]
        UnifiedEngine --> UnifiedResult[ä¸€è‡´æ€§ç»“æœ]
    end
    
    style BatchData fill:#ffcdd2
    style StreamData fill:#c8e6c9
    style UnifiedData fill:#e1f5fe
    style BatchProcess fill:#fff3e0
    style StreamProcess fill:#f3e5f5
    style UnifiedEngine fill:#e8f5e8
```

### 1.2 Sparkæµå¤„ç†æ¶æ„æ¼”è¿›

```scala
// Sparkæµå¤„ç†æŠ€æœ¯æ ˆæ¼”è¿›
object SparkStreamingEvolution {
  
  // 1. Spark Streaming (DStream) - Spark 0.7+
  class DStreamExample {
    val streamingContext = new StreamingContext(sparkConf, Seconds(10))
    
    val lines = streamingContext.socketTextStream("localhost", 9999)
    val words = lines.flatMap(_.split(" "))
    val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
    
    // å¾®æ‰¹å¤„ç†æ¨¡å¼ï¼šæ¯10ç§’å¤„ç†ä¸€æ‰¹æ•°æ®
    streamingContext.start()
    streamingContext.awaitTermination()
  }
  
  // 2. Structured Streaming - Spark 2.0+  
  class StructuredStreamingExample {
    val spark = SparkSession.builder().getOrCreate()
    
    val lines = spark.readStream
      .format("socket")
      .option("host", "localhost") 
      .option("port", 9999)
      .load()
    
    val words = lines.as[String].flatMap(_.split(" "))
    val wordCounts = words.groupBy("value").count()
    
    // è¿ç»­å¤„ç†æ¨¡å¼ï¼šçœŸæ­£çš„æµå¤„ç†
    val query = wordCounts.writeStream
      .outputMode("complete")
      .format("console")
      .trigger(Trigger.ProcessingTime("10 seconds"))
      .start()
      
    query.awaitTermination()
  }
}
```

### 1.3 æµå¤„ç†å…³é”®æ¦‚å¿µ

#### 1. **äº‹ä»¶æ—¶é—´ vs å¤„ç†æ—¶é—´**
```scala
// äº‹ä»¶æ—¶é—´ï¼šæ•°æ®å®é™…äº§ç”Ÿçš„æ—¶é—´
case class Event(
  eventTime: Timestamp,    // äº‹ä»¶å‘ç”Ÿæ—¶é—´
  processTime: Timestamp,  // å¤„ç†æ—¶é—´  
  data: String
)

// å¤„ç†æ—¶é—´ï¼šç³»ç»Ÿå¤„ç†æ•°æ®çš„æ—¶é—´
val eventTimeQuery = spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "localhost:9092")
  .load()
  .select(
    from_json($"value", eventSchema).as("event"),
    $"timestamp".as("processTime")  // Kafkaæ¶ˆæ¯æ—¶é—´æˆ³
  )
  .select("event.*", "processTime")
  
// åŸºäºäº‹ä»¶æ—¶é—´çš„çª—å£èšåˆ
val windowedCounts = eventTimeQuery
  .withWatermark("eventTime", "10 minutes")  // è®¾ç½®æ°´ä½çº¿
  .groupBy(
    window($"eventTime", "5 minutes", "1 minute"),  // 5åˆ†é’Ÿçª—å£ï¼Œ1åˆ†é’Ÿæ»‘åŠ¨
    $"category"
  )
  .count()
```

#### 2. **æ°´ä½çº¿ï¼ˆWatermarkï¼‰æœºåˆ¶**
```scala
// æ°´ä½çº¿ç”¨äºå¤„ç†ä¹±åºæ•°æ®å’Œå»¶è¿Ÿæ•°æ®
class WatermarkExample {
  
  def createWatermarkedStream(): Dataset[Row] = {
    val events = spark.readStream
      .format("kafka")
      .load()
      .select(from_json($"value", eventSchema).as("event"))
      .select("event.*")
    
    // è®¾ç½®æ°´ä½çº¿ï¼šå®¹å¿10åˆ†é’Ÿçš„å»¶è¿Ÿ
    events.withWatermark("eventTime", "10 minutes")
  }
  
  // æ°´ä½çº¿å·¥ä½œåŸç†
  def watermarkLogic(): Unit = {
    /*
    å‡è®¾å½“å‰å¤„ç†æ—¶é—´ä¸º 12:15:00
    è®¾ç½®æ°´ä½çº¿ä¸º 10åˆ†é’Ÿå»¶è¿Ÿå®¹å¿
    
    æ°´ä½çº¿ = max(äº‹ä»¶æ—¶é—´) - å»¶è¿Ÿå®¹å¿åº¦
    
    å¦‚æœæœ€æ–°äº‹ä»¶æ—¶é—´æ˜¯ 12:10:00ï¼Œåˆ™ï¼š
    æ°´ä½çº¿ = 12:10:00 - 10åˆ†é’Ÿ = 12:00:00
    
    æ‰€æœ‰äº‹ä»¶æ—¶é—´ < 12:00:00 çš„å»¶è¿Ÿæ•°æ®å°†è¢«ä¸¢å¼ƒ
    */
  }
}
```

---

## 2. DStream APIè¯¦è§£

### 2.1 DStreamæ ¸å¿ƒæ¦‚å¿µ

**DStream (Discretized Stream) å°†è¿ç»­çš„æ•°æ®æµç¦»æ•£åŒ–ä¸ºä¸€ç³»åˆ—RDD**

```scala
// DStreamçš„å†…éƒ¨å®ç°åŸç†
abstract class DStream[T: ClassTag](@transient private var _ssc: StreamingContext) 
  extends Serializable with Logging {
  
  // DStreamçš„ä¾èµ–å…³ç³»
  def dependencies: List[DStream[_]]
  
  // ç”ŸæˆæŒ‡å®šæ—¶é—´çš„RDD
  def compute(time: Time): Option[RDD[T]]
  
  // æ»‘åŠ¨é—´éš”  
  def slideDuration: Duration = _ssc.graph.batchDuration
  
  // æ£€æŸ¥ç‚¹é—´éš”
  private var _checkpointDuration: Duration = null
}

// DStreamå·¥ä½œæµç¨‹ç¤ºä¾‹
class DStreamWorkflow {
  
  val streamingContext = new StreamingContext(sparkConf, Seconds(5))
  
  // 1. è¾“å…¥DStreamï¼šä»æ•°æ®æºæ¥æ”¶æ•°æ®
  val kafkaStream = KafkaUtils.createDirectStream[String, String](
    streamingContext,
    PreferConsistent,
    Subscribe[String, String](topics, kafkaParams)
  )
  
  // 2. è½¬æ¢DStreamï¼šåº”ç”¨å„ç§è½¬æ¢æ“ä½œ
  val processedStream = kafkaStream
    .map(record => record.value())           // æå–æ¶ˆæ¯å†…å®¹
    .filter(_.nonEmpty)                      // è¿‡æ»¤ç©ºæ¶ˆæ¯
    .flatMap(_.split("\\s+"))               // åˆ†è¯
    .map(word => (word, 1))                 // è½¬æ¢ä¸ºé”®å€¼å¯¹
    .reduceByKeyAndWindow(_ + _, Seconds(30), Seconds(10))  // çª—å£èšåˆ
  
  // 3. è¾“å‡ºDStreamï¼šå°†ç»“æœè¾“å‡ºåˆ°å¤–éƒ¨ç³»ç»Ÿ
  processedStream.foreachRDD { rdd =>
    if (!rdd.isEmpty()) {
      rdd.foreachPartition { partition =>
        // å†™å…¥æ•°æ®åº“æˆ–å…¶ä»–å­˜å‚¨ç³»ç»Ÿ
        writeToDatabase(partition)
      }
    }
  }
}
```

### 2.2 DStreamè½¬æ¢æ“ä½œ

```scala
// DStreamè½¬æ¢æ“ä½œè¯¦è§£
class DStreamTransformations {
  
  val ssc = new StreamingContext(sparkConf, Seconds(10))
  val inputStream = ssc.socketTextStream("localhost", 9999)
  
  // 1. åŸºç¡€è½¬æ¢
  def basicTransformations(): Unit = {
    val mappedStream = inputStream.map(_.toUpperCase)
    val filteredStream = inputStream.filter(_.contains("ERROR"))
    val flatMappedStream = inputStream.flatMap(_.split(" "))
  }
  
  // 2. çª—å£æ“ä½œ
  def windowOperations(): Unit = {
    // çª—å£é•¿åº¦30ç§’ï¼Œæ»‘åŠ¨é—´éš”10ç§’
    val windowedStream = inputStream.window(Seconds(30), Seconds(10))
    
    // çª—å£å†…çš„å•è¯è®¡æ•°
    val wordCounts = inputStream
      .flatMap(_.split(" "))
      .map(word => (word, 1))
      .reduceByKeyAndWindow(
        (a: Int, b: Int) => a + b,        // èšåˆå‡½æ•°
        (a: Int, b: Int) => a - b,        // åå‘èšåˆå‡½æ•°ï¼ˆä¼˜åŒ–ï¼‰
        Seconds(30),                       // çª—å£é•¿åº¦
        Seconds(10)                        // æ»‘åŠ¨é—´éš”
      )
  }
  
  // 3. çŠ¶æ€è½¬æ¢  
  def statefulTransformations(): Unit = {
    val stateStream = inputStream
      .flatMap(_.split(" "))
      .map(word => (word, 1))
      .updateStateByKey[Int] { (values: Seq[Int], state: Option[Int]) =>
        val currentCount = values.sum
        val previousCount = state.getOrElse(0)
        Some(currentCount + previousCount)  // ç´¯è®¡è®¡æ•°
      }
  }
  
  // 4. è¿æ¥æ“ä½œ
  def joinOperations(): Unit = {
    val stream1 = ssc.socketTextStream("localhost", 9999)
      .map(line => (line.split(",")(0), line.split(",")(1)))
      
    val stream2 = ssc.socketTextStream("localhost", 8888)  
      .map(line => (line.split(",")(0), line.split(",")(1)))
    
    // æµä¸æµçš„è¿æ¥ï¼ˆåœ¨åŒä¸€æ‰¹æ¬¡å†…ï¼‰
    val joinedStream = stream1.join(stream2)
    
    // æµä¸é™æ€æ•°æ®é›†çš„è¿æ¥
    val staticRDD = ssc.sparkContext.parallelize(List(("key1", "value1")))
    val transformedStream = stream1.transform { rdd =>
      rdd.join(staticRDD)
    }
  }
}
```

### 2.3 è¾“å‡ºæ“ä½œä¸å®¹é”™

```scala
// DStreamè¾“å‡ºæ“ä½œ
class DStreamOutputOperations {
  
  val ssc = new StreamingContext(sparkConf, Seconds(10))
  val processedStream: DStream[(String, Int)] = createProcessedStream()
  
  // 1. æ ‡å‡†è¾“å‡ºæ“ä½œ
  def standardOutputs(): Unit = {
    processedStream.print()                    // æ‰“å°åˆ°æ§åˆ¶å°
    processedStream.saveAsTextFiles("hdfs://output/")  // ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
    processedStream.saveAsObjectFiles("hdfs://objects/") // ä¿å­˜ä¸ºå¯¹è±¡æ–‡ä»¶
  }
  
  // 2. è‡ªå®šä¹‰è¾“å‡ºæ“ä½œ
  def customOutputs(): Unit = {
    processedStream.foreachRDD { rdd =>
      // è¿æ¥æ± ç®¡ç†
      val connection = ConnectionPool.getConnection()
      
      rdd.foreachPartition { partitionOfRecords =>
        partitionOfRecords.foreach { record =>
          // å†™å…¥æ•°æ®åº“
          connection.send(record)
        }
      }
      
      ConnectionPool.returnConnection(connection)
    }
  }
  
  // 3. å®¹é”™è¾“å‡ºæ“ä½œ
  def faultTolerantOutput(): Unit = {
    processedStream.foreachRDD { (rdd, time) =>
      // å¹‚ç­‰æ€§è¾“å‡ºï¼šç¡®ä¿é‡å¤æ‰§è¡Œä¸ä¼šäº§ç”Ÿå‰¯ä½œç”¨
      val outputId = s"output_${time.milliseconds}"
      
      if (!outputAlreadyExists(outputId)) {
        rdd.coalesce(1).saveAsTextFile(s"hdfs://output/$outputId")
        markOutputAsCompleted(outputId)
      }
    }
  }
  
  // 4. æ£€æŸ¥ç‚¹é…ç½®
  def checkpointConfiguration(): Unit = {
    ssc.checkpoint("hdfs://checkpoints/streaming/")
    
    // è®¾ç½®æ£€æŸ¥ç‚¹é—´éš”ï¼ˆå»ºè®®ä¸ºæ‰¹æ¬¡é—´éš”çš„5-10å€ï¼‰
    processedStream.checkpoint(Minutes(1))
  }
}
```

---

## 3. Structured StreamingåŸç†

### 3.1 Structured Streamingæ¶æ„

**Structured Streamingå°†æµå¤„ç†è§†ä¸ºæ— ç•Œè¡¨ä¸Šçš„å¢é‡æŸ¥è¯¢**

```mermaid
graph TB
    subgraph "Structured Streamingæ¶æ„"
        subgraph "æ•°æ®æºå±‚"
            Kafka[Kafka<br/>æ¶ˆæ¯é˜Ÿåˆ—]
            FileSource[æ–‡ä»¶æº<br/>ç›®å½•ç›‘å¬]
            SocketSource[Socketæº<br/>TCPè¿æ¥]
            RateSource[Rateæº<br/>æµ‹è¯•æ•°æ®]
        end
        
        subgraph "æ ¸å¿ƒå¼•æ“å±‚"
            StreamReader[StreamReader<br/>æ•°æ®è¯»å–]
            CatalystOptimizer[Catalystä¼˜åŒ–å™¨<br/>æŸ¥è¯¢ä¼˜åŒ–]
            IncrementalExecution[å¢é‡æ‰§è¡Œå¼•æ“<br/>MicroBatchExecution]
            StateStore[çŠ¶æ€å­˜å‚¨<br/>åˆ†å¸ƒå¼çŠ¶æ€ç®¡ç†]
        end
        
        subgraph "è¾“å‡ºå±‚"
            ConsoleSink[Console<br/>æ§åˆ¶å°è¾“å‡º]
            FileSink[File<br/>æ–‡ä»¶è¾“å‡º]
            KafkaSink[Kafka<br/>æ¶ˆæ¯è¾“å‡º]
            MemorySink[Memory<br/>å†…å­˜è¾“å‡º]
            ForeachSink[Foreach<br/>è‡ªå®šä¹‰è¾“å‡º]
        end
        
        Kafka --> StreamReader
        FileSource --> StreamReader
        SocketSource --> StreamReader
        RateSource --> StreamReader
        
        StreamReader --> CatalystOptimizer
        CatalystOptimizer --> IncrementalExecution
        IncrementalExecution --> StateStore
        
        IncrementalExecution --> ConsoleSink
        IncrementalExecution --> FileSink
        IncrementalExecution --> KafkaSink
        IncrementalExecution --> MemorySink
        IncrementalExecution --> ForeachSink
    end
    
    style StreamReader fill:#e3f2fd
    style CatalystOptimizer fill:#e8f5e8
    style IncrementalExecution fill:#fff3e0
    style StateStore fill:#f3e5f5
```

### 3.2 å¢é‡æŸ¥è¯¢æ¨¡å‹

```scala
// Structured Streamingçš„æ ¸å¿ƒæ¦‚å¿µï¼šæ— ç•Œè¡¨
class StructuredStreamingConcepts {
  
  val spark = SparkSession.builder().getOrCreate()
  import spark.implicits._
  
  // 1. è¾“å…¥è¡¨ï¼šæ— ç•Œè¡¨æŠ½è±¡
  val inputTable = spark.readStream
    .format("kafka")
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "events")
    .load()
    .select(
      from_json($"value", eventSchema).as("event"),
      $"timestamp"
    )
    .select("event.*", "timestamp")
  
  /*
  æ— ç•Œè¡¨æ¦‚å¿µï¼š
  +---+----------+-----+
  |id |timestamp |value|
  +---+----------+-----+
  |1  |10:00:01  |100  |  <- Batch 1
  |2  |10:00:02  |200  |
  |3  |10:00:03  |300  |  <- Batch 2  
  |4  |10:00:04  |150  |
  |...           |...  |  <- æŒç»­å¢é•¿
  +---+----------+-----+
  */
  
  // 2. æŸ¥è¯¢é€»è¾‘ï¼šæ ‡å‡†DataFrame/SQLæ“ä½œ
  val processedTable = inputTable
    .withColumn("hour", hour($"timestamp"))
    .groupBy($"hour", window($"timestamp", "1 hour"))
    .agg(
      count("*").as("count"),
      avg("value").as("avg_value"),
      max("value").as("max_value")
    )
  
  // 3. è¾“å‡ºæ¨¡å¼å®šä¹‰
  def outputModes(): Unit = {
    // Complete Modeï¼šè¾“å‡ºå®Œæ•´ç»“æœè¡¨
    processedTable.writeStream
      .outputMode("complete")  // æ¯æ¬¡è¾“å‡ºå®Œæ•´ç»“æœ
      .format("console")
      .start()
    
    // Append Modeï¼šåªè¾“å‡ºæ–°å¢è¡Œ
    inputTable
      .filter($"value" > 100)
      .writeStream
      .outputMode("append")    // åªè¾“å‡ºæ–°å¢çš„è¿‡æ»¤ç»“æœ
      .format("console") 
      .start()
    
    // Update Modeï¼šè¾“å‡ºæ›´æ–°çš„è¡Œ
    processedTable.writeStream
      .outputMode("update")    // åªè¾“å‡ºæ›´æ–°çš„èšåˆç»“æœ
      .format("console")
      .start()
  }
}
```

### 3.3 è§¦å‘å™¨ä¸æ‰§è¡Œæ¨¡å¼

```scala
// è§¦å‘å™¨ç±»å‹ä¸é…ç½®
class TriggerConfiguration {
  
  val spark = SparkSession.builder().getOrCreate()
  val query = createStreamingQuery()
  
  // 1. å¾®æ‰¹å¤„ç†è§¦å‘å™¨ï¼ˆé»˜è®¤ï¼‰
  def microBatchTrigger(): Unit = {
    query.writeStream
      .trigger(Trigger.ProcessingTime("10 seconds"))  // æ¯10ç§’è§¦å‘ä¸€æ¬¡
      .format("console")
      .start()
  }
  
  // 2. ä¸€æ¬¡æ€§è§¦å‘å™¨  
  def onceTrigger(): Unit = {
    query.writeStream
      .trigger(Trigger.Once())  // å¤„ç†ä¸€æ¬¡ååœæ­¢
      .format("parquet")
      .option("path", "hdfs://output/")
      .start()
  }
  
  // 3. è¿ç»­å¤„ç†è§¦å‘å™¨ï¼ˆå®éªŒæ€§ï¼‰
  def continuousTrigger(): Unit = {
    query.writeStream
      .trigger(Trigger.Continuous("1 second"))  // 1ç§’å»¶è¿Ÿçš„è¿ç»­å¤„ç†
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("topic", "output")
      .start()
  }
  
  // 4. å¯ç”¨å³å¤„ç†è§¦å‘å™¨
  def availableNowTrigger(): Unit = {
    query.writeStream
      .trigger(Trigger.AvailableNow())  // å¤„ç†æ‰€æœ‰å¯ç”¨æ•°æ®ååœæ­¢
      .format("delta")
      .option("path", "s3://bucket/delta-table/")
      .start()
  }
}
```

### 3.4 çŠ¶æ€å­˜å‚¨æœºåˆ¶

```scala
// Structured StreamingçŠ¶æ€ç®¡ç†
class StateManagement {
  
  val spark = SparkSession.builder()
    .config("spark.sql.streaming.stateStore.providerClass", 
            "org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider")
    .config("spark.sql.streaming.checkpointLocation", "hdfs://checkpoints/")
    .getOrCreate()
  
  // 1. èšåˆçŠ¶æ€ï¼šè‡ªåŠ¨ç®¡ç†
  def aggregationState(): Unit = {
    val events = readKafkaStream()
    
    val aggregated = events
      .withWatermark("eventTime", "10 minutes")
      .groupBy(
        window($"eventTime", "5 minutes"),
        $"userId"
      )
      .agg(
        count("*").as("eventCount"),
        sum("amount").as("totalAmount")
      )
    
    // Sparkè‡ªåŠ¨ç®¡ç†èšåˆçŠ¶æ€
    aggregated.writeStream
      .outputMode("update")
      .format("console")
      .start()
  }
  
  // 2. è‡ªå®šä¹‰çŠ¶æ€ï¼šmapGroupsWithState
  def customState(): Unit = {
    case class UserSession(
      userId: String,
      sessionStart: Timestamp,
      eventCount: Int,
      lastActivity: Timestamp
    )
    
    def updateUserSession(
        key: String,
        values: Iterator[Event], 
        state: GroupState[UserSession]
    ): UserSession = {
      
      val events = values.toSeq
      val currentSession = state.getOption.getOrElse(
        UserSession(key, events.head.timestamp, 0, events.head.timestamp)
      )
      
      val updatedSession = currentSession.copy(
        eventCount = currentSession.eventCount + events.size,
        lastActivity = events.map(_.timestamp).max
      )
      
      // ä¼šè¯è¶…æ—¶æ£€æŸ¥ï¼ˆ30åˆ†é’Ÿæ— æ´»åŠ¨ï¼‰
      if (updatedSession.lastActivity.getTime < 
          System.currentTimeMillis() - 30 * 60 * 1000) {
        state.remove()  // åˆ é™¤è¿‡æœŸçŠ¶æ€
        updatedSession.copy(eventCount = -1)  // æ ‡è®°ä¼šè¯ç»“æŸ
      } else {
        state.update(updatedSession)
        state.setTimeoutTimestamp(updatedSession.lastActivity.getTime + 30 * 60 * 1000)
        updatedSession
      }
    }
    
    val events = readKafkaStream()
    val sessionUpdates = events
      .groupByKey(_.userId)
      .mapGroupsWithState(GroupStateTimeout.EventTimeTimeout)(updateUserSession)
    
    sessionUpdates.writeStream
      .outputMode("update")
      .format("console")
      .start()
  }
  
  // 3. å¹³å¦æ˜ å°„çŠ¶æ€ï¼šflatMapGroupsWithState
  def flatMapState(): Unit = {
    def processUserEvents(
        key: String,
        values: Iterator[Event],
        state: GroupState[UserSession]
    ): Iterator[Alert] = {
      
      val events = values.toSeq
      val alerts = ArrayBuffer[Alert]()
      
      // æ£€æµ‹å¼‚å¸¸æ¨¡å¼å¹¶ç”Ÿæˆå‘Šè­¦
      if (events.size > 100) {  // çŸ­æ—¶é—´å†…å¤§é‡äº‹ä»¶
        alerts += Alert(key, "HIGH_FREQUENCY", events.size)
      }
      
      // æ›´æ–°çŠ¶æ€...
      
      alerts.iterator
    }
    
    val events = readKafkaStream()
    val alerts = events
      .groupByKey(_.userId)
      .flatMapGroupsWithState(OutputMode.Append, GroupStateTimeout.NoTimeout)(processUserEvents)
    
    alerts.writeStream
      .format("kafka")
      .option("topic", "alerts")
      .start()
  }
}
```

---

## 4. æµæ‰¹ä¸€ä½“åŒ–æ¶æ„

### 4.1 Lambda vs Kappaæ¶æ„

```mermaid
graph TB
    subgraph "Lambdaæ¶æ„ï¼ˆä¼ ç»Ÿï¼‰"
        subgraph "æ•°æ®æº"
            DS1[æ•°æ®æº]
        end
        
        subgraph "æ‰¹å¤„ç†å±‚"
            BatchLayer[æ‰¹å¤„ç†å±‚<br/>Spark Batch]
            BatchStorage[æ‰¹å­˜å‚¨<br/>HDFS/S3]
            BatchView[æ‰¹è§†å›¾<br/>é¢„è®¡ç®—ç»“æœ]
        end
        
        subgraph "æµå¤„ç†å±‚"  
            StreamLayer[æµå¤„ç†å±‚<br/>Spark Streaming]
            StreamStorage[æµå­˜å‚¨<br/>Redis/HBase]
            StreamView[æµè§†å›¾<br/>å®æ—¶ç»“æœ]
        end
        
        subgraph "æœåŠ¡å±‚"
            ServingLayer[æœåŠ¡å±‚<br/>æŸ¥è¯¢åˆå¹¶]
        end
        
        DS1 --> BatchLayer
        DS1 --> StreamLayer
        BatchLayer --> BatchStorage
        BatchStorage --> BatchView
        StreamLayer --> StreamStorage
        StreamStorage --> StreamView
        BatchView --> ServingLayer
        StreamView --> ServingLayer
    end
    
    subgraph "Kappaæ¶æ„ï¼ˆç°ä»£ï¼‰"
        subgraph "ç»Ÿä¸€æ•°æ®æº"
            DS2[æ•°æ®æº<br/>äº‹ä»¶æµ]
        end
        
        subgraph "ç»Ÿä¸€å¤„ç†å±‚"
            UnifiedProcessing[ç»Ÿä¸€å¤„ç†å¼•æ“<br/>Structured Streaming]
            ReprocessingEngine[é‡å¤„ç†å¼•æ“<br/>å†å²æ•°æ®é‡ç®—]
        end
        
        subgraph "ç»Ÿä¸€å­˜å‚¨"
            UnifiedStorage[ç»Ÿä¸€å­˜å‚¨<br/>Delta Lake/Hudi]
        end
        
        DS2 --> UnifiedProcessing
        DS2 --> ReprocessingEngine  
        UnifiedProcessing --> UnifiedStorage
        ReprocessingEngine --> UnifiedStorage
    end
    
    style BatchLayer fill:#ffcdd2
    style StreamLayer fill:#c8e6c9
    style UnifiedProcessing fill:#e1f5fe
    style UnifiedStorage fill:#fff3e0
```

### 4.2 æµæ‰¹ä¸€ä½“å®ç°

```scala
// æµæ‰¹ä¸€ä½“åŒ–æ•°æ®å¤„ç†ç®¡é“
class StreamBatchUnifiedPipeline {
  
  val spark = SparkSession.builder()
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .getOrCreate()
  
  // 1. ç»Ÿä¸€æ•°æ®å¤„ç†é€»è¾‘
  def unifiedProcessingLogic(df: DataFrame): DataFrame = {
    df.withColumn("processed_time", current_timestamp())
      .withColumn("date_partition", to_date($"event_time"))
      .filter($"event_type".isNotNull)
      .groupBy($"date_partition", $"user_id", $"event_type")
      .agg(
        count("*").as("event_count"),
        sum("value").as("total_value"),
        max("event_time").as("last_event_time")
      )
  }
  
  // 2. æ‰¹å¤„ç†æ¨¡å¼  
  def batchProcessing(): Unit = {
    // è¯»å–å†å²æ•°æ®
    val batchDF = spark.read
      .format("delta")
      .load("s3://datalake/raw_events/")
      .filter($"date_partition" >= "2023-01-01")
    
    // åº”ç”¨ç»Ÿä¸€å¤„ç†é€»è¾‘
    val processedBatch = unifiedProcessingLogic(batchDF)
    
    // å†™å…¥ç»“æœè¡¨
    processedBatch.write
      .format("delta")
      .mode("overwrite")
      .partitionBy("date_partition")
      .save("s3://datalake/processed_events/")
  }
  
  // 3. æµå¤„ç†æ¨¡å¼
  def streamProcessing(): Unit = {
    // è¯»å–å®æ—¶æ•°æ®æµ
    val streamDF = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("subscribe", "raw_events")
      .load()
      .select(from_json($"value", eventSchema).as("event"))
      .select("event.*")
    
    // åº”ç”¨ç›¸åŒçš„å¤„ç†é€»è¾‘
    val processedStream = unifiedProcessingLogic(streamDF)
    
    // å†™å…¥ç›¸åŒçš„ç»“æœè¡¨
    val query = processedStream.writeStream
      .format("delta")
      .outputMode("append")
      .option("checkpointLocation", "s3://checkpoints/processed_events/")
      .trigger(Trigger.ProcessingTime("1 minute"))
      .start("s3://datalake/processed_events/")
    
    query.awaitTermination()
  }
  
  // 4. æ•°æ®è´¨é‡ä¿è¯
  def dataQualityCheck(): Unit = {
    // æµå¤„ç†ç»“æœéªŒè¯
    val streamResults = spark.read
      .format("delta")
      .load("s3://datalake/processed_events/")
      .filter($"date_partition" === current_date())
    
    // æ‰¹å¤„ç†ç»“æœéªŒè¯ï¼ˆé‡ç®—ï¼‰
    val batchResults = batchProcessing() // é‡æ–°è®¡ç®—å½“å¤©æ•°æ®
    
    // ç»“æœå¯¹æ¯”
    val diff = streamResults.except(batchResults)
    if (!diff.isEmpty) {
      // å‘é€å‘Šè­¦å¹¶è§¦å‘æ•°æ®ä¿®å¤
      sendAlert("Stream-Batch inconsistency detected")
      repairData(diff)
    }
  }
}
```

### 4.3 äº‹ä»¶æ—¶é—´å¤„ç†

```scala
// äº‹ä»¶æ—¶é—´å¤„ç†çš„å®Œæ•´ç¤ºä¾‹
class EventTimeProcessing {
  
  val spark = SparkSession.builder().getOrCreate()
  import spark.implicits._
  
  // 1. ä¹±åºæ•°æ®å¤„ç†
  def handleOutOfOrderData(): Unit = {
    val events = spark.readStream
      .format("kafka")
      .load()
      .select(
        from_json($"value", eventSchema).as("event"),
        $"timestamp".as("ingestion_time")
      )
      .select(
        $"event.event_id",
        $"event.event_time",  // äº‹ä»¶å®é™…å‘ç”Ÿæ—¶é—´
        $"event.user_id", 
        $"event.value",
        $"ingestion_time"     // æ•°æ®æ‘„å…¥æ—¶é—´
      )
    
    // è®¾ç½®æ°´ä½çº¿å¤„ç†å»¶è¿Ÿæ•°æ®
    val watermarkedEvents = events
      .withWatermark("event_time", "15 minutes")  // å®¹å¿15åˆ†é’Ÿå»¶è¿Ÿ
    
    val windowedAggregates = watermarkedEvents
      .groupBy(
        window($"event_time", "10 minutes", "5 minutes"),  // 10åˆ†é’Ÿçª—å£ï¼Œ5åˆ†é’Ÿæ»‘åŠ¨
        $"user_id"
      )
      .agg(
        count("*").as("event_count"),
        sum("value").as("total_value"),
        min("event_time").as("window_start"),
        max("event_time").as("window_end")
      )
    
    windowedAggregates.writeStream
      .outputMode("append")  // çª—å£å…³é—­åæ‰è¾“å‡º
      .format("console")
      .option("truncate", false)
      .start()
  }
  
  // 2. å»¶è¿Ÿæ•°æ®ç›‘æ§
  def monitorLateData(): Unit = {
    val events = readEventStream()
    
    // è®¡ç®—æ•°æ®å»¶è¿Ÿ
    val delayedData = events
      .withColumn("processing_delay", 
                  unix_timestamp($"ingestion_time") - unix_timestamp($"event_time"))
      .filter($"processing_delay" > 300)  // å»¶è¿Ÿè¶…è¿‡5åˆ†é’Ÿçš„æ•°æ®
    
    // å»¶è¿Ÿæ•°æ®ç»Ÿè®¡
    val delayStats = delayedData
      .withWatermark("event_time", "1 hour")
      .groupBy(window($"event_time", "1 hour"))
      .agg(
        count("*").as("delayed_count"),
        avg("processing_delay").as("avg_delay"),
        max("processing_delay").as("max_delay")
      )
    
    delayStats.writeStream
      .format("console")
      .outputMode("append")
      .start()
  }
  
  // 3. å¤šæµè¿æ¥å¤„ç†
  def multiStreamJoin(): Unit = {
    val clickStream = readClickStream()
      .withWatermark("click_time", "10 minutes")
    
    val impressionStream = readImpressionStream()
      .withWatermark("impression_time", "5 minutes")
    
    // åŸºäºäº‹ä»¶æ—¶é—´çš„æµè¿æ¥
    val joinedStream = clickStream
      .join(
        impressionStream,
        expr("""
          click_user_id = impression_user_id AND
          click_time >= impression_time AND  
          click_time <= impression_time + interval 30 minutes
        """),
        "inner"
      )
    
    // è¿æ¥ç»“æœå¤„ç†
    val conversionMetrics = joinedStream
      .groupBy(
        window($"click_time", "1 hour"),
        $"campaign_id"
      )
      .agg(
        count("*").as("conversions"),
        countDistinct("click_user_id").as("unique_users")
      )
    
    conversionMetrics.writeStream
      .format("delta")
      .outputMode("append")
      .option("checkpointLocation", "s3://checkpoints/conversions/")
      .start("s3://datalake/conversion_metrics/")
  }
}
```

---

## 5. å®¹é”™ä¸çŠ¶æ€ç®¡ç†

### 5.1 æ£€æŸ¥ç‚¹æœºåˆ¶

```scala
// Spark Streamingæ£€æŸ¥ç‚¹æœºåˆ¶è¯¦è§£
class CheckpointMechanism {
  
  // 1. å…ƒæ•°æ®æ£€æŸ¥ç‚¹
  def metadataCheckpoint(): Unit = {
    val ssc = StreamingContext.getOrCreate(checkpointDir, createStreamingContext _)
    
    def createStreamingContext(): StreamingContext = {
      val ssc = new StreamingContext(sparkConf, Seconds(10))
      
      // è®¾ç½®æ£€æŸ¥ç‚¹ç›®å½•
      ssc.checkpoint("hdfs://checkpoints/streaming/")
      
      val lines = ssc.socketTextStream("localhost", 9999)
      val wordCounts = lines
        .flatMap(_.split(" "))
        .map(word => (word, 1))
        .updateStateByKey[Int] { (values, state) =>
          Some(values.sum + state.getOrElse(0))
        }
      
      // è®¾ç½®RDDæ£€æŸ¥ç‚¹é—´éš”
      wordCounts.checkpoint(Minutes(2))
      
      wordCounts.print()
      ssc
    }
    
    ssc.start()
    ssc.awaitTermination()
  }
  
  // 2. æ•°æ®æ£€æŸ¥ç‚¹ä¼˜åŒ–
  def dataCheckpointOptimization(): Unit = {
    val ssc = new StreamingContext(sparkConf, Seconds(10))
    ssc.checkpoint("hdfs://checkpoints/")
    
    val kafkaStream = createKafkaStream(ssc)
    
    val statefulStream = kafkaStream
      .map(parseMessage)
      .updateStateByKey[StateData] { (values, state) =>
        // å¤æ‚çŠ¶æ€æ›´æ–°é€»è¾‘
        updateComplexState(values, state)
      }
    
    // æ£€æŸ¥ç‚¹é—´éš”å»ºè®®ï¼šbatch intervalçš„5-10å€
    statefulStream.checkpoint(Minutes(1))  // 10s * 6 = 1min
    
    statefulStream.foreachRDD { rdd =>
      // è¾“å‡ºåˆ°å¤–éƒ¨å­˜å‚¨
      rdd.foreachPartition(writeToExternalStore)
    }
  }
}
```

### 5.2 çŠ¶æ€å­˜å‚¨ä¼˜åŒ–

```scala
// Structured StreamingçŠ¶æ€å­˜å‚¨ä¼˜åŒ–
class StateStoreOptimization {
  
  val spark = SparkSession.builder()
    .config("spark.sql.streaming.stateStore.providerClass", 
            "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider")
    .config("spark.sql.streaming.stateStore.rocksdb.compactOnCommit", "true")
    .config("spark.sql.streaming.stateStore.rocksdb.blockSizeKB", "32")
    .getOrCreate()
  
  // 1. çŠ¶æ€TTLç®¡ç†
  def stateTTLManagement(): Unit = {
    case class SessionState(
      userId: String,
      sessionId: String, 
      startTime: Timestamp,
      lastActivity: Timestamp,
      eventCount: Int
    )
    
    def updateSessionWithTTL(
        key: String,
        values: Iterator[Event],
        state: GroupState[SessionState]
    ): SessionState = {
      
      val events = values.toSeq
      val now = Timestamp.valueOf(LocalDateTime.now())
      
      val currentState = state.getOption.getOrElse(
        SessionState(key, UUID.randomUUID().toString, now, now, 0)
      )
      
      val updatedState = currentState.copy(
        lastActivity = now,
        eventCount = currentState.eventCount + events.size
      )
      
      // ä¼šè¯è¶…æ—¶ç®¡ç†ï¼ˆ30åˆ†é’Ÿæ— æ´»åŠ¨ï¼‰
      val timeoutDuration = 30 * 60 * 1000  // 30åˆ†é’Ÿ
      if (now.getTime - updatedState.lastActivity.getTime > timeoutDuration) {
        state.remove()  // æ¸…ç†è¿‡æœŸçŠ¶æ€
        updatedState.copy(eventCount = -1)  // æ ‡è®°ä¼šè¯ç»“æŸ
      } else {
        state.update(updatedState)
        // è®¾ç½®åŸºäºäº‹ä»¶æ—¶é—´çš„è¶…æ—¶
        state.setTimeoutTimestamp(updatedState.lastActivity.getTime + timeoutDuration)
        updatedState
      }
    }
    
    val events = readEventStream()
    val sessionUpdates = events
      .withWatermark("event_time", "10 minutes")
      .groupByKey(_.userId)
      .mapGroupsWithState(GroupStateTimeout.EventTimeTimeout)(updateSessionWithTTL)
    
    sessionUpdates.writeStream
      .outputMode("update")
      .format("console")
      .start()
  }
  
  // 2. çŠ¶æ€åˆ†åŒºç­–ç•¥
  def statePartitioning(): Unit = {
    // è‡ªå®šä¹‰åˆ†åŒºå™¨å‡å°‘çŠ¶æ€çƒ­ç‚¹
    class StatePartitioner(numPartitions: Int) extends Partitioner {
      override def numPartitions: Int = numPartitions
      
      override def getPartition(key: Any): Int = {
        val keyStr = key.toString
        // ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œé¿å…æ•°æ®å€¾æ–œ
        Math.abs(keyStr.hashCode % numPartitions)
      }
    }
    
    val events = readEventStream()
    val partitionedEvents = events
      .repartition(200, $"user_id")  // æ˜¾å¼æ§åˆ¶åˆ†åŒºæ•°
      .groupByKey(_.userId)
      .mapGroupsWithState(GroupStateTimeout.ProcessingTimeTimeout)(updateUserState)
    
    partitionedEvents.writeStream
      .option("checkpointLocation", "hdfs://checkpoints/partitioned/")
      .start()
  }
  
  // 3. çŠ¶æ€å‹ç¼©ä¼˜åŒ–
  def stateCompaction(): Unit = {
    // é…ç½®RocksDBçŠ¶æ€å­˜å‚¨çš„å‹ç¼©å‚æ•°
    val optimizedSpark = spark.newSession()
    optimizedSpark.conf.set("spark.sql.streaming.stateStore.rocksdb.compactOnCommit", "true")
    optimizedSpark.conf.set("spark.sql.streaming.stateStore.rocksdb.compression", "lz4")
    optimizedSpark.conf.set("spark.sql.streaming.stateStore.maintenance.interval", "30s")
    
    // å®šæœŸæ¸…ç†å†å²ç‰ˆæœ¬
    optimizedSpark.conf.set("spark.sql.streaming.minBatchesToRetain", "10")
  }
}
```

### 5.3 æ•…éšœæ¢å¤æœºåˆ¶

```scala
// æµå¤„ç†æ•…éšœæ¢å¤æœºåˆ¶
class FaultRecoveryMechanism {
  
  // 1. è‡ªåŠ¨é‡å¯ç­–ç•¥
  def automaticRestart(): Unit = {
    val spark = SparkSession.builder()
      .config("spark.sql.streaming.stopGracefullyOnShutdown", "true")
      .config("spark.sql.streaming.stopActiveRunOnRestart", "true")
      .getOrCreate()
    
    def createQuery(): StreamingQuery = {
      val events = readEventStream()
      val processed = processEvents(events)
      
      processed.writeStream
        .format("delta")
        .outputMode("append")
        .option("checkpointLocation", "hdfs://checkpoints/auto-restart/")
        .trigger(Trigger.ProcessingTime("10 seconds"))
        .start("hdfs://output/processed/")
    }
    
    var query = createQuery()
    
    // ç›‘æ§æŸ¥è¯¢çŠ¶æ€å¹¶è‡ªåŠ¨é‡å¯
    while (true) {
      try {
        if (query.exception.isDefined) {
          logError(s"Query failed: ${query.exception.get}")
          query.stop()
          Thread.sleep(5000)  // ç­‰å¾…5ç§’
          query = createQuery()  // é‡æ–°åˆ›å»ºæŸ¥è¯¢
          logInfo("Query restarted successfully")
        }
        Thread.sleep(10000)  // æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
      } catch {
        case e: Exception =>
          logError(s"Monitoring error: $e")
          Thread.sleep(30000)  // å¼‚å¸¸æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
      }
    }
  }
  
  // 2. æ•°æ®ä¸€è‡´æ€§ä¿è¯
  def ensureDataConsistency(): Unit = {
    val events = readEventStream()
    
    // ä½¿ç”¨å¹‚ç­‰å†™å…¥ä¿è¯ä¸€è‡´æ€§
    val processed = processEvents(events)
      .withColumn("batch_id", spark_partition_id())
      .withColumn("process_time", current_timestamp())
    
    processed.writeStream
      .foreachBatch { (batchDF: DataFrame, batchId: Long) =>
        // å¹‚ç­‰æ€§æ£€æŸ¥
        val existingBatch = checkBatchExists(batchId)
        if (!existingBatch) {
          // äº‹åŠ¡æ€§å†™å…¥
          batchDF.write
            .format("delta")
            .mode("append")
            .option("txnVersion", batchId.toString)
            .save("hdfs://output/processed/")
          
          // è®°å½•æ‰¹æ¬¡å®ŒæˆçŠ¶æ€
          markBatchCompleted(batchId)
        }
      }
      .option("checkpointLocation", "hdfs://checkpoints/consistency/")
      .start()
  }
  
  // 3. èƒŒå‹æ§åˆ¶æœºåˆ¶
  def backPressureControl(): Unit = {
    val spark = SparkSession.builder()
      .config("spark.streaming.backpressure.enabled", "true")
      .config("spark.streaming.backpressure.initialRate", "1000")  
      .config("spark.streaming.receiver.maxRate", "5000")
      .config("spark.streaming.kafka.maxRatePerPartition", "2000")
      .getOrCreate()
    
    val events = spark.readStream
      .format("kafka")
      .option("maxOffsetsPerTrigger", "100000")  // æ¯æ‰¹æ¬¡æœ€å¤§åç§»é‡
      .load()
    
    val processed = events
      .repartition(200)  // åˆç†è®¾ç½®å¹¶è¡Œåº¦
      .mapPartitions { partition =>
        // åˆ†åŒºçº§å¤„ç†ï¼Œæ§åˆ¶å†…å­˜ä½¿ç”¨
        partition.map(processRecord).filter(_.isDefined).map(_.get)
      }
    
    processed.writeStream
      .trigger(Trigger.ProcessingTime("30 seconds"))  // é€‚å½“çš„æ‰¹æ¬¡é—´éš”
      .format("console")
      .start()
  }
}
```

---

## 6. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 6.1 ååé‡ä¼˜åŒ–

```scala
// æµå¤„ç†ååé‡ä¼˜åŒ–ç­–ç•¥
class ThroughputOptimization {
  
  val spark = SparkSession.builder()
    // åŸºç¡€æ€§èƒ½é…ç½®
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .config("spark.sql.adaptive.enabled", "true")
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
    
    // æµå¤„ç†ç‰¹å®šä¼˜åŒ–
    .config("spark.sql.streaming.metricsEnabled", "true")
    .config("spark.sql.streaming.numRecentProgressUpdates", "100")
    .getOrCreate()
  
  // 1. æ•°æ®æºä¼˜åŒ–
  def optimizeDataSources(): Unit = {
    // Kafkaæ¶ˆè´¹ä¼˜åŒ–
    val kafkaStream = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("subscribe", "high-throughput-topic")
      
      // æ‰¹é‡æ‹‰å–ä¼˜åŒ–
      .option("kafka.fetch.min.bytes", "50000")         // æœ€å°æ‹‰å–å­—èŠ‚æ•°
      .option("kafka.fetch.max.wait.ms", "500")         // æœ€å¤§ç­‰å¾…æ—¶é—´
      .option("kafka.max.partition.fetch.bytes", "10485760")  // 10MBæ¯åˆ†åŒº
      
      // å¹¶å‘ä¼˜åŒ–
      .option("kafka.consumer.session.timeout.ms", "30000")
      .option("kafka.consumer.heartbeat.interval.ms", "3000")
      
      // åç§»é‡ç®¡ç†
      .option("startingOffsets", "latest")
      .option("failOnDataLoss", "false")
      .load()
    
    // åˆ†åŒºç­–ç•¥ä¼˜åŒ–
    val optimizedStream = kafkaStream
      .repartition(400, $"key")  // æ ¹æ®keyé‡åˆ†åŒºï¼Œæé«˜å¹¶è¡Œåº¦
      .coalesce(200)             // è¾“å‡ºæ—¶åˆå¹¶åˆ†åŒºï¼Œå‡å°‘å°æ–‡ä»¶
  }
  
  // 2. å¤„ç†é€»è¾‘ä¼˜åŒ–
  def optimizeProcessingLogic(): Unit = {
    val events = readOptimizedStream()
    
    // ä½¿ç”¨mapPartitionså‡å°‘å‡½æ•°è°ƒç”¨å¼€é”€
    val processed = events.mapPartitions { partition =>
      // æ‰¹é‡åˆå§‹åŒ–èµ„æº
      val connectionPool = initializeConnectionPool()
      val cache = new LRUCache[String, String](1000)
      
      partition.map { event =>
        // åˆ©ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—
        val enrichedData = cache.getOrElseUpdate(event.userId) {
          getUserProfile(connectionPool, event.userId)
        }
        
        processEventWithProfile(event, enrichedData)
      }
    }
    
    // é¢„èšåˆå‡å°‘Shuffle
    val preAggregated = processed
      .groupBy($"category", $"hour")
      .agg(
        count("*").as("count"),
        sum("value").as("sum"),
        max("timestamp").as("max_time")
      )
    
    preAggregated.writeStream
      .format("delta")
      .outputMode("append")
      .trigger(Trigger.ProcessingTime("30 seconds"))  // é€‚å½“çš„æ‰¹æ¬¡å¤§å°
      .start()
  }
  
  // 3. å†…å­˜ä½¿ç”¨ä¼˜åŒ–
  def optimizeMemoryUsage(): Unit = {
    val optimizedSpark = spark.newSession()
    
    // å†…å­˜é…ç½®ä¼˜åŒ–
    optimizedSpark.conf.set("spark.executor.memory", "8g")
    optimizedSpark.conf.set("spark.executor.memoryFraction", "0.8")
    optimizedSpark.conf.set("spark.executor.cores", "4")
    
    // æµå¤„ç†å†…å­˜ä¼˜åŒ–
    optimizedSpark.conf.set("spark.sql.streaming.stateStore.providerClass", 
                           "org.apache.spark.sql.execution.streaming.state.RocksDBStateStoreProvider")
    optimizedSpark.conf.set("spark.sql.streaming.stateStore.rocksdb.writeBufferSizeMB", "64")
    optimizedSpark.conf.set("spark.sql.streaming.stateStore.rocksdb.blockCacheSizeMB", "128")
    
    // ä½¿ç”¨åˆ—å¼ç¼“å­˜
    val events = readEventStream()
    events.cache()  // è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜å­˜å‚¨çº§åˆ«
    
    // åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„ç¼“å­˜
    optimizedSpark.catalog.clearCache()
  }
}
```

### 6.2 å»¶è¿Ÿä¼˜åŒ–

```scala
// æµå¤„ç†å»¶è¿Ÿä¼˜åŒ–ç­–ç•¥  
class LatencyOptimization {
  
  // 1. è¿ç»­å¤„ç†æ¨¡å¼
  def continuousProcessing(): Unit = {
    val spark = SparkSession.builder()
      .config("spark.sql.streaming.continuous.enabled", "true")
      .getOrCreate()
    
    val events = spark.readStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092") 
      .load()
    
    val processed = events
      .select(
        from_json($"value", eventSchema).as("event"),
        $"timestamp"
      )
      .select("event.*", "timestamp")
      .filter($"event.priority" === "HIGH")  // åªå¤„ç†é«˜ä¼˜å…ˆçº§äº‹ä»¶
    
    // è¿ç»­å¤„ç†ï¼šäºšç§’çº§å»¶è¿Ÿ
    processed.writeStream
      .format("kafka")
      .option("kafka.bootstrap.servers", "localhost:9092")
      .option("topic", "processed-events")
      .trigger(Trigger.Continuous("1 second"))  // 1ç§’å»¶è¿Ÿ
      .start()
  }
  
  // 2. æœ¬åœ°çŠ¶æ€ä¼˜åŒ–
  def localStateOptimization(): Unit = {
    // ä½¿ç”¨æœ¬åœ°èšåˆå‡å°‘ç½‘ç»œå¼€é”€
    val events = readEventStream()
    
    val locallyAggregated = events
      .repartition($"partition_key")  // ç¡®ä¿ç›¸åŒkeyåœ¨åŒä¸€åˆ†åŒº
      .mapPartitions { partition =>
        val localState = mutable.Map[String, Long]()
        
        partition.map { event =>
          // æœ¬åœ°çŠ¶æ€æ›´æ–°
          val currentCount = localState.getOrElse(event.key, 0L)
          localState(event.key) = currentCount + 1
          
          (event.key, localState(event.key))
        }
      }
    
    locallyAggregated.writeStream
      .format("console")
      .trigger(Trigger.ProcessingTime("1 second"))  // é«˜é¢‘è¾“å‡º
      .start()
  }
  
  // 3. é¢„çƒ­æœºåˆ¶
  def warmupMechanism(): Unit = {
    // é¢„çƒ­JVMå’Œç¼“å­˜
    def warmupQuery(): Unit = {
      val warmupData = spark.range(10000).toDF("id")
      warmupData.collect()  // è§¦å‘JVMé¢„çƒ­
    }
    
    // é¢„çƒ­è¿æ¥æ± 
    def warmupConnections(): Unit = {
      val connectionPool = createConnectionPool()
      (1 to 10).foreach { _ =>
        val conn = connectionPool.getConnection()
        conn.ping()  // é¢„çƒ­ç½‘ç»œè¿æ¥
        connectionPool.returnConnection(conn)
      }
    }
    
    // åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œé¢„çƒ­
    warmupQuery()
    warmupConnections()
    
    // æ­£å¼å¤„ç†æµç¨‹
    val events = readEventStream()
    events.writeStream
      .format("console") 
      .start()
  }
}
```

### 6.3 ç›‘æ§ä¸è°ƒè¯•

```scala
// æµå¤„ç†ç›‘æ§ä¸è°ƒè¯•
class StreamingMonitoring {
  
  // 1. æ€§èƒ½æŒ‡æ ‡ç›‘æ§
  def performanceMonitoring(): Unit = {
    val query = createStreamingQuery()
    
    // å®šæœŸæ£€æŸ¥æŸ¥è¯¢çŠ¶æ€
    val monitor = new Thread(() => {
      while (query.isActive) {
        val progress = query.lastProgress
        
        if (progress != null) {
          // è¾“å…¥é€Ÿç‡ç›‘æ§
          val inputRowsPerSecond = progress.inputRowsPerSecond
          val processingDuration = progress.durationMs.get("triggerExecution")
          
          // å»¶è¿Ÿç›‘æ§
          val batchId = progress.batchId
          val timestamp = progress.timestamp
          
          // çŠ¶æ€å­˜å‚¨ç›‘æ§
          val stateOperators = progress.stateOperators
          stateOperators.foreach { operator =>
            val memoryUsedBytes = operator.memoryUsedBytes
            val numRowsTotal = operator.numRowsTotal
            
            logInfo(s"State operator memory: ${memoryUsedBytes / 1024 / 1024}MB, rows: $numRowsTotal")
          }
          
          // å‘Šè­¦æ£€æŸ¥
          if (inputRowsPerSecond < expectedThroughput * 0.8) {
            sendAlert(s"Low throughput: $inputRowsPerSecond rows/sec")
          }
          
          if (processingDuration > maxAllowedLatency) {
            sendAlert(s"High latency: ${processingDuration}ms")
          }
        }
        
        Thread.sleep(30000)  // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
      }
    })
    
    monitor.start()
  }
  
  // 2. æ•°æ®è´¨é‡ç›‘æ§
  def dataQualityMonitoring(): Unit = {
    val events = readEventStream()
    
    // æ•°æ®è´¨é‡æŒ‡æ ‡è®¡ç®—
    val qualityMetrics = events
      .withWatermark("event_time", "5 minutes")
      .groupBy(window($"event_time", "1 minute"))
      .agg(
        count("*").as("total_events"),
        count(when($"user_id".isNull, 1)).as("null_user_ids"),
        count(when($"event_type".isNull, 1)).as("null_event_types"),
        count(when($"value" < 0, 1)).as("negative_values"),
        approx_count_distinct("user_id").as("unique_users")
      )
      .withColumn("null_user_id_rate", $"null_user_ids" / $"total_events")
      .withColumn("data_quality_score", 
                  lit(1.0) - ($"null_user_ids" + $"null_event_types" + $"negative_values") / $"total_events")
    
    // è´¨é‡å‘Šè­¦
    qualityMetrics.writeStream
      .foreachBatch { (batchDF: DataFrame, batchId: Long) =>
        batchDF.collect().foreach { row =>
          val qualityScore = row.getAs[Double]("data_quality_score")
          if (qualityScore < 0.95) {  // è´¨é‡åˆ†æ•°ä½äº95%
            sendDataQualityAlert(batchId, qualityScore, row)
          }
        }
      }
      .start()
  }
  
  // 3. è°ƒè¯•å·¥å…·
  def debuggingTools(): Unit = {
    val events = readEventStream()
    
    // 1. é‡‡æ ·è°ƒè¯•
    val sampledEvents = events.sample(0.01)  // 1%é‡‡æ ·
    sampledEvents.writeStream
      .format("console")
      .option("numRows", 50)
      .option("truncate", false)
      .start()
    
    // 2. æ•°æ®è¡€ç¼˜è¿½è¸ª
    val processedEvents = events
      .withColumn("debug_batch_id", spark_partition_id())
      .withColumn("debug_timestamp", current_timestamp())
      .filter($"user_id" === "debug_user_123")  // ç‰¹å®šç”¨æˆ·è°ƒè¯•
    
    processedEvents.writeStream
      .format("json")
      .option("path", "hdfs://debug/traces/")
      .option("checkpointLocation", "hdfs://debug/checkpoints/")
      .start()
    
    // 3. å¼‚å¸¸äº‹ä»¶æ•è·
    val anomalyEvents = events.filter(
      $"value" > 1000000 ||  // å¼‚å¸¸å¤§å€¼
      $"user_id".rlike("^test_") ||  // æµ‹è¯•æ•°æ®
      $"event_time" < date_sub(current_date(), 7)  // è¿‡æœŸæ•°æ®
    )
    
    anomalyEvents.writeStream
      .format("kafka")
      .option("topic", "anomaly-events")
      .start()
  }
}
```

---

## æ€»ç»“

æœ¬ç« æ·±å…¥å­¦ä¹ äº†Spark Streamingä¸æµå¤„ç†çš„æ ¸å¿ƒæŠ€æœ¯ï¼š

1. **æµå¤„ç†åŸºç¡€**ï¼šDStream APIã€äº‹ä»¶æ—¶é—´å¤„ç†ã€æ°´ä½çº¿æœºåˆ¶
2. **Structured Streaming**ï¼šç»Ÿä¸€æ‰¹æµå¤„ç†ã€å¢é‡æŸ¥è¯¢æ¨¡å‹ã€çŠ¶æ€ç®¡ç†
3. **æµæ‰¹ä¸€ä½“åŒ–**ï¼šLambda vs Kappaæ¶æ„ã€ç»Ÿä¸€å¤„ç†é€»è¾‘ã€æ•°æ®ä¸€è‡´æ€§
4. **å®¹é”™æœºåˆ¶**ï¼šæ£€æŸ¥ç‚¹ã€çŠ¶æ€å­˜å‚¨ã€æ•…éšœæ¢å¤ã€èƒŒå‹æ§åˆ¶
5. **æ€§èƒ½ä¼˜åŒ–**ï¼šååé‡ä¼˜åŒ–ã€å»¶è¿Ÿä¼˜åŒ–ã€è¿ç»­å¤„ç†æ¨¡å¼
6. **ç›‘æ§è°ƒè¯•**ï¼šæ€§èƒ½ç›‘æ§ã€æ•°æ®è´¨é‡ç›‘æ§ã€è°ƒè¯•å·¥å…·

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†å­¦ä¹ Sparkæ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µã€‚

---

## æ¨èé˜…è¯»
- [Sparkå®˜æ–¹æ–‡æ¡£ - Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Spark Streaming vs Structured Streaming](https://databricks.com/blog/2018/03/20/low-latency-continuous-processing-mode-in-structured-streaming-in-apache-spark-2-3-0.html)
- ã€ŠSparkæµå¤„ç†å®æˆ˜ã€‹å…¨ä¹¦
```

```markdown
# Sparkæ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µæ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [æ€§èƒ½è°ƒä¼˜æ–¹æ³•è®º](#1-æ€§èƒ½è°ƒä¼˜æ–¹æ³•è®º)
2. [èµ„æºé…ç½®ä¼˜åŒ–](#2-èµ„æºé…ç½®ä¼˜åŒ–)
3. [ä»£ç å±‚é¢ä¼˜åŒ–](#3-ä»£ç å±‚é¢ä¼˜åŒ–)
4. [æ•°æ®å€¾æ–œå¤„ç†](#4-æ•°æ®å€¾æ–œå¤„ç†)
5. [ç¼“å­˜ç­–ç•¥ä¼˜åŒ–](#5-ç¼“å­˜ç­–ç•¥ä¼˜åŒ–)
6. [ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ](#6-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ)

---

## 1. æ€§èƒ½è°ƒä¼˜æ–¹æ³•è®º

### 1.1 æ€§èƒ½åˆ†ææ¡†æ¶

```mermaid
graph TB
    subgraph "Sparkæ€§èƒ½è°ƒä¼˜é‡‘å­—å¡”"
        subgraph "åº”ç”¨å±‚ä¼˜åŒ–"
            Algorithm[ç®—æ³•ä¼˜åŒ–<br/>é€‰æ‹©åˆé€‚çš„ç®—æ³•]
            DataStructure[æ•°æ®ç»“æ„<br/>é€‰æ‹©é«˜æ•ˆçš„æ•°æ®ç»“æ„]
            Logic[ä¸šåŠ¡é€»è¾‘<br/>ä¼˜åŒ–è®¡ç®—é€»è¾‘]
        end
        
        subgraph "Sparkå±‚ä¼˜åŒ–"  
            SQL[SQLä¼˜åŒ–<br/>æŸ¥è¯¢é‡å†™ä¸ä¼˜åŒ–]
            Cache[ç¼“å­˜ç­–ç•¥<br/>åˆç†ä½¿ç”¨ç¼“å­˜]
            Partition[åˆ†åŒºä¼˜åŒ–<br/>æ•°æ®åˆ†åŒºç­–ç•¥]
            Shuffle[Shuffleä¼˜åŒ–<br/>å‡å°‘æ•°æ®ä¼ è¾“]
        end
        
        subgraph "ç³»ç»Ÿå±‚ä¼˜åŒ–"
            Resource[èµ„æºé…ç½®<br/>CPU/å†…å­˜/å­˜å‚¨]
            JVM[JVMè°ƒä¼˜<br/>åƒåœ¾æ”¶é›†å™¨ä¼˜åŒ–]
            Network[ç½‘ç»œä¼˜åŒ–<br/>å¸¦å®½ä¸å»¶è¿Ÿ]
            Storage[å­˜å‚¨ä¼˜åŒ–<br/>ç£ç›˜I/Oä¼˜åŒ–]
        end
        
        Algorithm --> SQL
        DataStructure --> Cache
        Logic --> Partition
        SQL --> Resource
        Cache --> JVM
        Partition --> Network
        Shuffle --> Storage
    end
    
    style Algorithm fill:#e3f2fd
    style SQL fill:#e8f5e8
    style Resource fill:#fff3e0
    style JVM fill:#f3e5f5
```

### 1.2 æ€§èƒ½è¯Šæ–­å·¥å…·é“¾

```scala
// Sparkæ€§èƒ½è¯Šæ–­å·¥å…·é›†åˆ
class PerformanceDiagnostics {
  
  val spark = SparkSession.builder().getOrCreate()
  
  // 1. Spark UIåˆ†æå·¥å…·
  def analyzeSparkUI(): Unit = {
    val sc = spark.sparkContext
    
    // ç›‘æ§ä½œä¸šæ‰§è¡Œæƒ…å†µ
    sc.addSparkListener(new SparkListener {
      override def onJobStart(jobStart: SparkListenerJobStart): Unit = {
        logInfo(s"Job ${jobStart.jobId} started with ${jobStart.stageIds.length} stages")
      }
      
      override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {
        val duration = jobEnd.time - getJobStartTime(jobEnd.jobId)
        logInfo(s"Job ${jobEnd.jobId} completed in ${duration}ms")
        
        // åˆ†æä½œä¸šæ€§èƒ½æŒ‡æ ‡
        if (duration > expectedJobDuration) {
          analyzeSlowJob(jobEnd.jobId)
        }
      }
      
      override def onStageCompleted(stageCompleted: SparkListenerStageCompleted): Unit = {
        val stageInfo = stageCompleted.stageInfo
        val taskMetrics = stageInfo.taskMetrics
        
        // åˆ†æStageæ€§èƒ½
        if (taskMetrics.executorRunTime > expectedStageRunTime) {
          logWarning(s"Slow stage detected: ${stageInfo.stageId}")
          analyzeStageMetrics(stageInfo)
        }
      }
    })
  }
  
  // 2. è‡ªå®šä¹‰æ€§èƒ½åº¦é‡
  def customPerformanceMetrics(): Unit = {
    val performanceAccumulator = spark.sparkContext.longAccumulator("PerformanceCounter")
    
    val rdd = spark.sparkContext.parallelize(1 to 1000000)
    val processedRDD = rdd.map { value =>
      val startTime = System.nanoTime()
      
      // ä¸šåŠ¡å¤„ç†é€»è¾‘
      val result = complexProcessing(value)
      
      val endTime = System.nanoTime()
      val duration = (endTime - startTime) / 1000000  // è½¬æ¢ä¸ºæ¯«ç§’
      
      // è®°å½•å¤„ç†æ—¶é—´
      performanceAccumulator.add(duration)
      
      result
    }
    
    val results = processedRDD.collect()
    val avgProcessingTime = performanceAccumulator.value / results.length
    
    logInfo(s"Average processing time: ${avgProcessingTime}ms per record")
  }
  
  // 3. å†…å­˜ä½¿ç”¨åˆ†æ
  def analyzeMemoryUsage(): Unit = {
    val memoryTracker = new MemoryTracker()
    
    val largeDf = spark.range(10000000).toDF("id")
      .withColumn("data", rand() * 1000)
      .withColumn("category", (rand() * 100).cast("int"))
    
    // ç¼“å­˜å‰çš„å†…å­˜ä½¿ç”¨
    val beforeCache = memoryTracker.getUsedMemory()
    
    largeDf.cache()
    largeDf.count()  // è§¦å‘ç¼“å­˜
    
    // ç¼“å­˜åçš„å†…å­˜ä½¿ç”¨  
    val afterCache = memoryTracker.getUsedMemory()
    val cacheMemoryUsage = afterCache - beforeCache
    
    logInfo(s"Cache memory usage: ${cacheMemoryUsage / 1024 / 1024}MB")
    
    // åˆ†æå­˜å‚¨çº§åˆ«æ•ˆç‡
    analyzeCacheEfficiency(largeDf)
  }
  
  // 4. Shuffleåˆ†æå·¥å…·
  def analyzeShufflePerformance(): DataFrame = {
    val df = createLargeDataFrame()
    
    // è®°å½•Shuffleå‰åçš„æŒ‡æ ‡
    val shuffleMetrics = spark.sparkContext.longAccumulator("ShuffleBytes")
    
    val result = df
      .repartition(200, $"key")  // è§¦å‘Shuffle
      .groupBy("key")
      .agg(
        count("*").as("count"),
        sum("value").as("sum")
      )
    
    // è·å–ShuffleæŒ‡æ ‡
    val lastStageInfo = spark.sparkContext.statusTracker.getExecutorInfos
    val shuffleReadBytes = lastStageInfo.map(_.totalShuffleRead).sum
    val shuffleWriteBytes = lastStageInfo.map(_.totalShuffleWrite).sum
    
    logInfo(s"Shuffle Read: ${shuffleReadBytes / 1024 / 1024}MB")
    logInfo(s"Shuffle Write: ${shuffleWriteBytes / 1024 / 1024}MB")
    
    result
  }
}
```

### 1.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

```scala
// Sparkæ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶
class SparkBenchmark {
  
  val spark = SparkSession.builder().getOrCreate()
  
  // 1. CPUå¯†é›†å‹åŸºå‡†æµ‹è¯•
  def cpuIntensiveBenchmark(): BenchmarkResult = {
    val testData = spark.range(1000000).toDF("number")
    
    val startTime = System.currentTimeMillis()
    
    val result = testData
      .withColumn("sqrt", sqrt($"number"))
      .withColumn("sin", sin($"number"))
      .withColumn("cos", cos($"number"))
      .withColumn("complex", 
                  sqrt($"number") * sin($"number") + cos($"number"))
      .count()
    
    val endTime = System.currentTimeMillis()
    val duration = endTime - startTime
    
    BenchmarkResult("CPU_INTENSIVE", duration, result, getCPUMetrics())
  }
  
  // 2. å†…å­˜å¯†é›†å‹åŸºå‡†æµ‹è¯•
  def memoryIntensiveBenchmark(): BenchmarkResult = {
    val largeData = spark.range(10000000)
      .withColumn("data1", rand())
      .withColumn("data2", rand()) 
      .withColumn("data3", rand())
      .cache()
    
    val startTime = System.currentTimeMillis()
    
    // å¤šæ¬¡è®¿é—®ç¼“å­˜æ•°æ®
    val results = (1 to 5).map { _ =>
      largeData.filter($"data1" > 0.5).count()
    }
    
    val endTime = System.currentTimeMillis()
    val duration = endTime - startTime
    
    BenchmarkResult("MEMORY_INTENSIVE", duration, results.sum, getMemoryMetrics())
  }
  
  // 3. ç½‘ç»œI/OåŸºå‡†æµ‹è¯•
  def networkIOBenchmark(): BenchmarkResult = {
    val df1 = createDataFrame(1000000, "table1")
    val df2 = createDataFrame(1000000, "table2")
    
    val startTime = System.currentTimeMillis()
    
    // è§¦å‘Shuffleæ“ä½œ
    val joinResult = df1.join(df2, "key")
      .groupBy("category")
      .agg(count("*"))
      .collect()
    
    val endTime = System.currentTimeMillis()
    val duration = endTime - startTime
    
    BenchmarkResult("NETWORK_IO", duration, joinResult.length, getNetworkMetrics())
  }
  
  // 4. ç£ç›˜I/OåŸºå‡†æµ‹è¯•
  def diskIOBenchmark(): BenchmarkResult = {
    val testData = createLargeDataFrame()
    val outputPath = "hdfs://benchmark/disk-io-test"
    
    val startTime = System.currentTimeMillis()
    
    // å†™å…¥æµ‹è¯•
    testData.write
      .mode("overwrite")
      .parquet(outputPath)
    
    // è¯»å–æµ‹è¯•
    val readData = spark.read.parquet(outputPath)
    val count = readData.count()
    
    val endTime = System.currentTimeMillis() 
    val duration = endTime - startTime
    
    BenchmarkResult("DISK_IO", duration, count, getDiskMetrics())
  }
  
  // 5. ç»¼åˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š
  def generateBenchmarkReport(): BenchmarkSuite = {
    val cpuResult = cpuIntensiveBenchmark()
    val memoryResult = memoryIntensiveBenchmark() 
    val networkResult = networkIOBenchmark()
    val diskResult = diskIOBenchmark()
    
    val suite = BenchmarkSuite(
      timestamp = System.currentTimeMillis(),
      sparkVersion = spark.version,
      results = Seq(cpuResult, memoryResult, networkResult, diskResult),
      environment = getEnvironmentInfo()
    )
    
    // ç”ŸæˆæŠ¥å‘Š
    generateHTMLReport(suite)
    suite
  }
}
```

---

## 2. èµ„æºé…ç½®ä¼˜åŒ–

### 2.1 Executorèµ„æºé…ç½®

```scala
// Executorèµ„æºé…ç½®æœ€ä½³å®è·µ
class ExecutorResourceOptimization {
  
  // 1. CPUé…ç½®ç­–ç•¥
  def optimizeCPUConfiguration(): SparkConf = {
    val conf = new SparkConf()
    
    // åŸºæœ¬åŸåˆ™ï¼šæ¯ä¸ªexecutoræ ¸æ•°ä¸è¦è¶…è¿‡5ä¸ª
    // åŸå› ï¼šé¿å…HDFSå®¢æˆ·ç«¯å¹¶å‘é—®é¢˜ï¼Œå‡å°‘å†…å­˜ç«äº‰
    
    val totalCores = getClusterTotalCores()  // å‡è®¾é›†ç¾¤æ€»å…±1000æ ¸
    val executorCores = 4  // æ¯ä¸ªexecutor 4æ ¸å¿ƒ
    val executorInstances = totalCores / executorCores / 2  // é¢„ç•™50%èµ„æº
    
    conf.set("spark.executor.cores", executorCores.toString)
        .set("spark.executor.instances", executorInstances.toString)
        .set("spark.default.parallelism", (executorInstances * executorCores * 2).toString)
  }
  
  // 2. å†…å­˜é…ç½®ç­–ç•¥
  def optimizeMemoryConfiguration(): SparkConf = {
    val conf = new SparkConf()
    
    // å†…å­˜é…ç½®å…¬å¼ï¼š
    // executor-memory = (yarn.nodemanager.resource.memory-mb Ã— num-cores-per-executor) / num-executors-per-node - 1GB
    
    val nodeMemory = 64  // GBï¼ŒèŠ‚ç‚¹æ€»å†…å­˜
    val reservedMemory = 8  // GBï¼Œç³»ç»Ÿé¢„ç•™å†…å­˜
    val availableMemory = nodeMemory - reservedMemory  // 56GB
    val executorsPerNode = 2  // æ¯èŠ‚ç‚¹executoræ•°é‡
    val executorMemory = availableMemory / executorsPerNode - 1  // 27GB
    
    conf.set("spark.executor.memory", s"${executorMemory}g")
        .set("spark.executor.memoryFraction", "0.8")  // 80%ç”¨äºè®¡ç®—å’Œå­˜å‚¨
        .set("spark.memory.storageFraction", "0.3")   // 30%ç”¨äºå­˜å‚¨ï¼Œ70%ç”¨äºè®¡ç®—
        
        // å †å¤–å†…å­˜é…ç½®
        .set("spark.memory.offHeap.enabled", "true")
        .set("spark.memory.offHeap.size", "8g")  // å †å¤–å†…å­˜8GB
  }
  
  // 3. åŠ¨æ€èµ„æºåˆ†é…
  def configureDynamicAllocation(): SparkConf = {
    val conf = new SparkConf()
    
    conf.set("spark.dynamicAllocation.enabled", "true")
        .set("spark.dynamicAllocation.minExecutors", "10")       // æœ€å°executoræ•°é‡
        .set("spark.dynamicAllocation.maxExecutors", "200")      // æœ€å¤§executoræ•°é‡  
        .set("spark.dynamicAllocation.initialExecutors", "20")   // åˆå§‹executoræ•°é‡
        .set("spark.dynamicAllocation.executorIdleTimeout", "60s")  // executorç©ºé—²è¶…æ—¶
        .set("spark.dynamicAllocation.cachedExecutorIdleTimeout", "300s")  // ç¼“å­˜æ•°æ®çš„executorè¶…æ—¶
        .set("spark.dynamicAllocation.schedulerBacklogTimeout", "1s")      // è°ƒåº¦å»¶è¿Ÿè¶…æ—¶
        .set("spark.shuffle.service.enabled", "true")  // å¯ç”¨å¤–éƒ¨ShuffleæœåŠ¡
  }
  
  // 4. èµ„æºé…ç½®å†³ç­–æ ‘
  def recommendResourceConfiguration(
      dataSize: Long,
      complexity: ProcessingComplexity,
      cluster: ClusterInfo
  ): ResourceConfiguration = {
    
    val (executorMemory, executorCores, executorInstances) = (dataSize, complexity) match {
      
      // å°æ•°æ®é›†ï¼ˆ<10GBï¼‰ï¼Œç®€å•å¤„ç†
      case (size, ProcessingComplexity.SIMPLE) if size < 10L * 1024 * 1024 * 1024 =>
        ("4g", 2, 10)
      
      // ä¸­ç­‰æ•°æ®é›†ï¼ˆ10GB-100GBï¼‰ï¼Œä¸­ç­‰å¤æ‚åº¦
      case (size, ProcessingComplexity.MODERATE) if size < 100L * 1024 * 1024 * 1024 =>
        ("8g", 4, 20)
      
      // å¤§æ•°æ®é›†ï¼ˆ100GB-1TBï¼‰ï¼Œå¤æ‚å¤„ç†
      case (size, ProcessingComplexity.COMPLEX) if size < 1024L * 1024 * 1024 * 1024 =>
        ("16g", 4, 50)
      
      // è¶…å¤§æ•°æ®é›†ï¼ˆ>1TBï¼‰ï¼Œéå¸¸å¤æ‚
      case (size, ProcessingComplexity.VERY_COMPLEX) =>
        ("32g", 5, 100)
        
      // é»˜è®¤é…ç½®
      case _ =>
        ("8g", 4, 30)
    }
    
    ResourceConfiguration(
      executorMemory = executorMemory,
      executorCores = executorCores,
      executorInstances = executorInstances,
      driverMemory = calculateDriverMemory(executorInstances),
      offHeapMemory = calculateOffHeapMemory(executorMemory)
    )
  }
}
```

### 2.2 Driverèµ„æºé…ç½®

```scala
// Driverèµ„æºé…ç½®ä¼˜åŒ–
class DriverResourceOptimization {
  
  // 1. Driverå†…å­˜é…ç½®
  def configureDriverMemory(): SparkConf = {
    val conf = new SparkConf()
    
    // Driverå†…å­˜éœ€æ±‚åˆ†æï¼š
    // - æ”¶é›†ç»“æœï¼šcollect(), take()æ“ä½œéœ€è¦è¶³å¤Ÿå†…å­˜
    // - å¹¿æ’­å˜é‡ï¼šå¤§çš„å¹¿æ’­å˜é‡éœ€è¦Driverå†…å­˜
    // - ç¨‹åºå…ƒæ•°æ®ï¼šå¤§é‡å°æ–‡ä»¶çš„å…ƒæ•°æ®
    
    val expectedResultSize = calculateExpectedResultSize()  // é¢„æœŸç»“æœå¤§å°
    val broadcastSize = calculateBroadcastSize()           // å¹¿æ’­å˜é‡å¤§å°
    val metadataSize = calculateMetadataSize()             // å…ƒæ•°æ®å¤§å°
    
    val totalDriverMemory = (expectedResultSize + broadcastSize + metadataSize) * 1.5  // 50%ç¼“å†²
    val driverMemoryGB = Math.max(4, Math.min(totalDriverMemory / 1024 / 1024 / 1024, 32))
    
    conf.set("spark.driver.memory", s"${driverMemoryGB}g")
        .set("spark.driver.maxResultSize", "8g")  // é™åˆ¶ç»“æœé›†å¤§å°
  }
  
  // 2. Driver CPUé…ç½®
  def configureDriverCores(): SparkConf = {
    val conf = new SparkConf()
    
    // Driveré€šå¸¸æ˜¯å•çº¿ç¨‹ï¼Œä½†æŸäº›æ“ä½œå¯ä»¥å¹¶è¡Œï¼š
    // - æ–‡ä»¶åˆ—ä¸¾
    // - ä»»åŠ¡è°ƒåº¦
    // - ç»“æœæ”¶é›†
    
    val executorCount = getExecutorCount()
    val driverCores = if (executorCount > 100) 4 else 2  // å¤§é›†ç¾¤ä½¿ç”¨æ›´å¤šæ ¸å¿ƒ
    
    conf.set("spark.driver.cores", driverCores.toString)
  }
  
  // 3. Driverç½‘ç»œé…ç½®
  def configureDriverNetwork(): SparkConf = {
    val conf = new SparkConf()
    
    conf.set("spark.rpc.message.maxSize", "256")          // å¢åŠ RPCæ¶ˆæ¯å¤§å°é™åˆ¶
        .set("spark.network.timeout", "800s")             // ç½‘ç»œè¶…æ—¶é…ç½®
        .set("spark.rpc.askTimeout", "600s")              // RPCè¯·æ±‚è¶…æ—¶
        .set("spark.sql.execution.arrow.maxRecordsPerBatch", "10000")  // Arrowæ‰¹æ¬¡å¤§å°
  }
}
```

### 2.3 å­˜å‚¨ç³»ç»Ÿä¼˜åŒ–

```scala
// å­˜å‚¨ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–
class StorageOptimization {
  
  // 1. HDFSä¼˜åŒ–é…ç½®
  def optimizeHDFS(): SparkConf = {
    val conf = new SparkConf()
    
    conf.set("spark.hadoop.fs.hdfs.impl.disable.cache", "true")  // ç¦ç”¨HDFSç¼“å­˜
        .set("spark.hadoop.fs.file.impl.disable.cache", "true")   // ç¦ç”¨æ–‡ä»¶ç³»ç»Ÿç¼“å­˜
        
        // HDFSå®¢æˆ·ç«¯ä¼˜åŒ–
        .set("spark.hadoop.dfs.client.read.shortcircuit", "true")  // çŸ­è·¯è¯»å–
        .set("spark.hadoop.dfs.domain.socket.path", "/var/lib/hadoop-hdfs/dn_socket")
        .set("spark.hadoop.dfs.client.cache.readahead", "268435456")  // 256MBé¢„è¯»
        
        // å¹¶å‘æ§åˆ¶
        .set("spark.hadoop.fs.hdfs.connection.idle.timeout.ms", "30000")
        .set("spark.hadoop.dfs.client.socket.timeout", "120000")
  }
  
  // 2. S3ä¼˜åŒ–é…ç½®
  def optimizeS3(): SparkConf = {
    val conf = new SparkConf()
    
    conf.set("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
        .set("spark.hadoop.fs.s3a.fast.upload", "true")
        .set("spark.hadoop.fs.s3a.fast.upload.buffer", "bytebuffer")  // ä½¿ç”¨å †å¤–ç¼“å†²
        
        // è¿æ¥æ± ä¼˜åŒ–
        .set("spark.hadoop.fs.s3a.connection.maximum", "200")
        .set("spark.hadoop.fs.s3a.threads.max", "64")
        .set("spark.hadoop.fs.s3a.max.total.tasks", "64")
        
        // å¤šéƒ¨åˆ†ä¸Šä¼ ä¼˜åŒ–
        .set("spark.hadoop.fs.s3a.multipart.size", "134217728")      // 128MBåˆ†å—
        .set("spark.hadoop.fs.s3a.multipart.threshold", "536870912")  // 512MBé˜ˆå€¼
        
        // é‡è¯•æœºåˆ¶
        .set("spark.hadoop.fs.s3a.retry.limit", "10")
        .set("spark.hadoop.fs.s3a.retry.interval", "500ms")
  }
  
  // 3. æœ¬åœ°å­˜å‚¨ä¼˜åŒ–
  def optimizeLocalStorage(): SparkConf = {
    val conf = new SparkConf()
    
    // ä¸´æ—¶ç›®å½•é…ç½®
    val tempDirs = getLocalDisks().map(disk => s"$disk/spark-temp").mkString(",")
    conf.set("spark.local.dir", tempDirs)
    
    // ç£ç›˜I/Oä¼˜åŒ–
    conf.set("spark.shuffle.file.buffer", "64k")           // Shuffleæ–‡ä»¶ç¼“å†²åŒº
        .set("spark.unsafe.sorter.spill.reader.buffer.size", "1m")  // æº¢å†™è¯»å–ç¼“å†²åŒº
        .set("spark.file.transferTo", "true")              // é›¶æ‹·è´ä¼ è¾“
        .set("spark.shuffle.unsafe.file.output.buffer", "64k")      // Unsafe Shuffleç¼“å†²åŒº
  }
  
  // 4. å­˜å‚¨æ ¼å¼ä¼˜åŒ–
  def optimizeStorageFormat(): Unit = {
    val spark = SparkSession.builder().getOrCreate()
    
    // Parquetä¼˜åŒ–é…ç½®
    spark.conf.set("spark.sql.parquet.columnarReaderBatchSize", "10000")
    spark.conf.set("spark.sql.parquet.compression.codec", "snappy")  // å‹ç¼©ç®—æ³•
    spark.conf.set("spark.sql.parquet.filterPushdown", "true")       // è°“è¯ä¸‹æ¨
    spark.conf.set("spark.sql.parquet.mergeSchema", "false")         // ç¦ç”¨Schemaåˆå¹¶
    
    // Delta Lakeä¼˜åŒ–é…ç½®
    spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
    spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
    
    // åˆ—å¼å­˜å‚¨æœ€ä½³å®è·µç¤ºä¾‹
    val df = createSampleDataFrame()
    
    df.write
      .format("delta")
      .mode("overwrite")
      .option("compression", "snappy")
      .option("mergeSchema", "false")
      .partitionBy("date", "region")  // åˆç†çš„åˆ†åŒºç­–ç•¥
      .save("s3://datalake/optimized_table/")
  }
}
```

---

## 3. ä»£ç å±‚é¢ä¼˜åŒ–

### 3.1 SQLæŸ¥è¯¢ä¼˜åŒ–

```scala
// SQLæŸ¥è¯¢ä¼˜åŒ–æœ€ä½³å®è·µ
class SQLQueryOptimization {
  
  val spark = SparkSession.builder().getOrCreate()
  
  // 1. æŸ¥è¯¢é‡å†™ä¼˜åŒ–
  def queryRewriteOptimizations(): Unit = {
    
    // âŒ ä½æ•ˆæŸ¥è¯¢ï¼šå­æŸ¥è¯¢ + å¤šæ¬¡æ‰«æ
    val inefficientQuery = spark.sql("""
      SELECT u.user_id, u.name,
        (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.user_id) as order_count,
        (SELECT SUM(amount) FROM orders o WHERE o.user_id = u.user_id) as total_amount
      FROM users u
      WHERE u.status = 'ACTIVE'
    """)
    
    // âœ… é«˜æ•ˆæŸ¥è¯¢ï¼šJOIN + å•æ¬¡æ‰«æ  
    val efficientQuery = spark.sql("""
      SELECT u.user_id, u.name, 
             COALESCE(o.order_count, 0) as order_count,
             COALESCE(o.total_amount, 0.0) as total_amount
      FROM users u
      LEFT JOIN (
        SELECT user_id, 
               COUNT(*) as order_count,
               SUM(amount) as total_amount
        FROM orders
        GROUP BY user_id
      ) o ON u.user_id = o.user_id
      WHERE u.status = 'ACTIVE'
    """)
  }
  
  // 2. è¿æ¥ä¼˜åŒ–ç­–ç•¥
  def joinOptimizations(): Unit = {
    val users = spark.table("users")
    val orders = spark.table("orders")
    
    // âŒ ä½æ•ˆï¼šå¤§è¡¨ JOIN å¤§è¡¨ï¼Œæ²¡æœ‰ä¼˜åŒ–æç¤º
    val inefficientJoin = users.join(orders, "user_id")
    
    // âœ… é«˜æ•ˆï¼šä½¿ç”¨å¹¿æ’­JOINï¼ˆå°è¡¨ < 10MBï¼‰
    val broadcastJoin = users.join(
      broadcast(orders.filter($"amount" > 1000)),  // è¿‡æ»¤åå¹¿æ’­
      "user_id"
    )
    
    // âœ… é«˜æ•ˆï¼šæ¡¶åŒ–è¡¨JOINï¼ˆé¿å…Shuffleï¼‰
    spark.sql("""
      CREATE TABLE users_bucketed (
        user_id BIGINT, name STRING, status STRING
      ) USING DELTA
      CLUSTERED BY (user_id) INTO 50 BUCKETS
    """)
    
    spark.sql("""
      CREATE TABLE orders_bucketed (
        order_id BIGINT, user_id BIGINT, amount DOUBLE
      ) USING DELTA  
      CLUSTERED BY (user_id) INTO 50 BUCKETS
    """)
    
    // æ¡¶åŒ–è¡¨JOINï¼Œæ— éœ€Shuffle
    val bucketedJoin = spark.sql("""
      SELECT u.name, COUNT(*) as order_count
      FROM users_bucketed u
      JOIN orders_bucketed o ON u.user_id = o.user_id
      GROUP BY u.name
    """)
  }
  
  // 3. èšåˆä¼˜åŒ–ç­–ç•¥
  def aggregationOptimizations(): Unit = {
    
    // âŒ ä½æ•ˆï¼šå¤šä¸ªèšåˆå‡½æ•°ï¼Œå¤šæ¬¡æ‰«æ
    val multipleScans = spark.sql("""
      SELECT 
        (SELECT COUNT(*) FROM orders WHERE status = 'COMPLETED') as completed,
        (SELECT COUNT(*) FROM orders WHERE status = 'PENDING') as pending,
        (SELECT COUNT(*) FROM orders WHERE status = 'CANCELLED') as cancelled
    """)
    
    // âœ… é«˜æ•ˆï¼šå•æ¬¡æ‰«æ + æ¡ä»¶èšåˆ
    val singleScan = spark.sql("""
      SELECT 
        COUNT(CASE WHEN status = 'COMPLETED' THEN 1 END) as completed,
        COUNT(CASE WHEN status = 'PENDING' THEN 1 END) as pending,
        COUNT(CASE WHEN status = 'CANCELLED' THEN 1 END) as cancelled
      FROM orders
    """)
    
    // âœ… é«˜æ•ˆï¼šä½¿ç”¨CUBEå’ŒROLLUP
    val cubeAggregation = spark.sql("""
      SELECT region, product_category, SUM(sales)
      FROM sales_data
      GROUP BY CUBE(region, product_category)
    """)
  }
  
  // 4. çª—å£å‡½æ•°ä¼˜åŒ–
  def windowFunctionOptimizations(): Unit = {
    
    // âŒ ä½æ•ˆï¼šå¤šä¸ªçª—å£å‡½æ•°ï¼Œä¸åŒåˆ†åŒº
    val multipleWindows = spark.sql("""
      SELECT user_id, order_date, amount,
        ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY order_date) as rn1,
        RANK() OVER (PARTITION BY region ORDER BY amount DESC) as rn2,
        SUM(amount) OVER (PARTITION BY product_id ORDER BY order_date) as running_sum
      FROM orders
    """)
    
    // âœ… é«˜æ•ˆï¼šç›¸åŒåˆ†åŒºçš„çª—å£å‡½æ•°åˆå¹¶
    val optimizedWindows = spark.sql("""
      SELECT user_id, order_date, amount,
        ROW_NUMBER() OVER w1 as rn,
        LAG(amount, 1) OVER w1 as prev_amount,
        SUM(amount) OVER w1 as running_sum
      FROM orders
      WINDOW w1 AS (PARTITION BY user_id ORDER BY order_date)
    """)
  }
}
```

### 3.2 DataFrame APIä¼˜åŒ–

```scala
// DataFrame APIä¼˜åŒ–æŠ€å·§
class DataFrameOptimization {
  
  val spark = SparkSession.builder().getOrCreate()
  import spark.implicits._
  
  // 1. åˆ—é€‰æ‹©ä¼˜åŒ–
  def columnSelectionOptimization(): Unit = {
    val df = spark.table("large_table")  // å‡è®¾æœ‰50ä¸ªåˆ—
    
    // âŒ ä½æ•ˆï¼šè¯»å–æ‰€æœ‰åˆ—åå†é€‰æ‹©
    val inefficient = df.filter($"status" === "ACTIVE")
                       .select("user_id", "name", "email")
    
    // âœ… é«˜æ•ˆï¼šæå‰åˆ—è£å‰ª
    val efficient = df.select("user_id", "name", "email", "status")
                     .filter($"status" === "ACTIVE")
                     .drop("status")
  }
  
  // 2. è¿‡æ»¤æ¡ä»¶ä¼˜åŒ–
  def filterOptimization(): Unit = {
    val df = spark.table("events")
    
    // âŒ ä½æ•ˆï¼šå¤æ‚æ¡ä»¶ï¼Œéš¾ä»¥ä¸‹æ¨
    val complexFilter = df.filter(
      $"event_date".isNotNull && 
      year($"event_date") === 2023 && 
      month($"event_date").isin(List(1,2,3): _*) &&
      $"user_id".isNotNull
    )
    
    // âœ… é«˜æ•ˆï¼šç®€å•æ¡ä»¶ï¼Œæ˜“äºä¸‹æ¨
    val simpleFilter = df
      .filter($"event_date" >= "2023-01-01" && $"event_date" < "2023-04-01")
      .filter($"user_id".isNotNull)
  }
  
  // 3. ç±»å‹è½¬æ¢ä¼˜åŒ–
  def typeConversionOptimization(): Unit = {
    val df = spark.read.csv("data.csv")
    
    // âŒ ä½æ•ˆï¼šå¤šæ¬¡ç±»å‹è½¬æ¢
    val inefficient = df
      .withColumn("amount", $"amount".cast("double"))
      .withColumn("user_id", $"user_id".cast("long"))
      .withColumn("timestamp", $"timestamp".cast("timestamp"))
      .withColumn("amount_rounded", round($"amount", 2))
    
    // âœ… é«˜æ•ˆï¼šæ‰¹é‡ç±»å‹è½¬æ¢
    val efficient = df.select(
      $"user_id".cast("long"),
      $"amount".cast("double"),
      $"timestamp".cast("timestamp"),
      round($"amount".cast("double"), 2).as("amount_rounded")
    )
  }
  
  // 4. UDFä¼˜åŒ–
  def udfOptimization(): Unit = {
    
    // âŒ ä½æ•ˆï¼šPython UDFï¼ˆåºåˆ—åŒ–å¼€é”€å¤§ï¼‰
    val pythonUDF = spark.udf.register("python_func", (x: String) => {
      // å¤æ‚å¤„ç†é€»è¾‘
      x.toUpperCase.reverse
    })
    
    // âœ… é«˜æ•ˆï¼šå†…ç½®å‡½æ•°ç»„åˆ
    val builtinFunctions = df.withColumn("result", 
      reverse(upper($"text"))
    )
    
    // âœ… é«˜æ•ˆï¼šScala UDFï¼ˆæ€§èƒ½æ›´å¥½ï¼‰
    val scalaUDF = udf((text: String) => {
      if (text != null) text.toUpperCase.reverse else null
    })
    
    val optimizedDF = df.withColumn("result", scalaUDF($"text"))
  }
  
  // 5. åˆ†ç»„èšåˆä¼˜åŒ–
  def groupByOptimization(): Unit = {
    val df = spark.table("sales")
    
    // âŒ ä½æ•ˆï¼šå¤šæ¬¡groupByæ“ä½œ
    val multiple = df.groupBy("region").count().union(
      df.groupBy("region").agg(sum("amount"))
    ).union(
      df.groupBy("region").agg(avg("amount"))
    )
    
    // âœ… é«˜æ•ˆï¼šå•æ¬¡groupByï¼Œå¤šä¸ªèšåˆ
    val single = df.groupBy("region").agg(
      count("*").as("count"),
      sum("amount").as("total_amount"), 
      avg("amount").as("avg_amount")
    )
    
    // âœ… é«˜æ•ˆï¼šé¢„èšåˆ + æœ€ç»ˆèšåˆ
    val preAggregated = df
      .groupBy("region", "product_category", "date")
      .agg(sum("amount").as("daily_sum"))
      .groupBy("region", "product_category") 
      .agg(sum("daily_sum").as("total_sum"))
  }
}
```

### 3.3 RDDæ“ä½œä¼˜åŒ–

```scala
// RDDæ“ä½œä¼˜åŒ–æŠ€å·§
class RDDOptimization {
  
  val spark = SparkSession.builder().getOrCreate()
  val sc = spark.sparkContext
  
  // 1. è½¬æ¢æ“ä½œä¼˜åŒ–
  def transformationOptimization(): Unit = {
    val rdd = sc.textFile("hdfs://data/large_file.txt")
    
    // âŒ ä½æ•ˆï¼šå¤šæ¬¡mapæ“ä½œ
    val inefficient = rdd
      .map(_.trim)
      .map(_.toLowerCase) 
      .map(_.split(","))
      .map(arr => (arr(0), arr(1)))
    
    // âœ… é«˜æ•ˆï¼šå•æ¬¡mapæ“ä½œ
    val efficient = rdd.map { line =>
      val parts = line.trim.toLowerCase.split(",")
      (parts(0), parts(1))
    }
    
    // âœ… é«˜æ•ˆï¼šmapPartitionsæ‰¹é‡å¤„ç†
    val batchProcessed = rdd.mapPartitions { partition =>
      val processor = createProcessor()  // æ¯åˆ†åŒºåˆå§‹åŒ–ä¸€æ¬¡
      partition.map { line =>
        processor.process(line)  // å¤ç”¨å¤„ç†å™¨
      }
    }
  }
  
  // 2. é”®å€¼å¯¹æ“ä½œä¼˜åŒ–
  def keyValueOptimization(): Unit = {
    val rdd1 = sc.parallelize(List(("a", 1), ("b", 2), ("c", 3)))
    val rdd2 = sc.parallelize(List(("a", 4), ("b", 5), ("d", 6)))
    
    // âŒ ä½æ•ˆï¼šä½¿ç”¨groupByKeyï¼ˆæ•°æ®é‡å¤§æ—¶å†…å­˜é—®é¢˜ï¼‰
    val grouped = rdd1.union(rdd2)
                     .groupByKey()
                     .map { case (k, values) => (k, values.sum) }
    
    // âœ… é«˜æ•ˆï¼šä½¿ç”¨reduceByKeyï¼ˆé¢„èšåˆï¼‰
    val reduced = rdd1.union(rdd2)
                     .reduceByKey(_ + _)
    
    // âœ… é«˜æ•ˆï¼šä½¿ç”¨combineByKeyï¼ˆè‡ªå®šä¹‰èšåˆï¼‰
    val combined = rdd1.combineByKey(
      (v: Int) => v,              // createCombiner
      (acc: Int, v: Int) => acc + v,  // mergeValue
      (acc1: Int, acc2: Int) => acc1 + acc2  // mergeCombiner
    )
  }
  
  // 3. åˆ†åŒºä¼˜åŒ–
  def partitionOptimization(): Unit = {
    val rdd = sc.textFile("hdfs://data/", minPartitions = 200)
    
    // âŒ ä½æ•ˆï¼šåˆ†åŒºæ•°è¿‡å¤šï¼Œä»»åŠ¡è¿‡å°
    val overPartitioned = rdd.repartition(1000)
    
    // âœ… é«˜æ•ˆï¼šåˆç†çš„åˆ†åŒºæ•°ï¼ˆé€šå¸¸æ˜¯coresçš„2-3å€ï¼‰
    val reasonablePartitions = rdd.coalesce(100)
    
    // âœ… é«˜æ•ˆï¼šè‡ªå®šä¹‰åˆ†åŒºå™¨
    class CustomPartitioner(numParts: Int) extends Partitioner {
      override def numPartitions: Int = numParts
      override def getPartition(key: Any): Int = {
        // åŸºäºä¸šåŠ¡é€»è¾‘çš„åˆ†åŒºç­–ç•¥
        key.hashCode() % numPartitions match {
          case p if p < 0 => p + numPartitions
          case p => p
        }
      }
    }
    
    val customPartitioned = rdd.map(line => (extractKey(line), line))
                              .partitionBy(new CustomPartitioner(100))
                              .values
  }
  
  // 4. æŒä¹…åŒ–ä¼˜åŒ–
  def persistenceOptimization(): Unit = {
    val rdd = sc.textFile("hdfs://data/large_dataset.txt")
    
    val processedRDD = rdd
      .map(parseRecord)
      .filter(isValid)
      .map(transform)
    
    // âŒ ä½æ•ˆï¼šé‡å¤è®¡ç®—
    val count1 = processedRDD.count()
    val count2 = processedRDD.filter(_.value > 100).count()
    
    // âœ… é«˜æ•ˆï¼šé€‚å½“æŒä¹…åŒ–
    processedRDD.persist(StorageLevel.MEMORY_AND_DISK_SER)
    val count1_cached = processedRDD.count()
    val count2_cached = processedRDD.filter(_.value > 100).count()
    
    // åŠæ—¶æ¸…ç†ç¼“å­˜
    processedRDD.unpersist(blocking = true)
  }
}
```

---

## 4. æ•°æ®å€¾æ–œå¤„ç†

### 4.1 æ•°æ®å€¾æ–œæ£€æµ‹

```scala
// æ•°æ®å€¾æ–œæ£€æµ‹å·¥å…·
class DataSkewDetector {
  
  val spark = SparkSession.builder().getOrCreate()
  import spark.implicits._
  
  // 1. åˆ†åŒºæ•°æ®åˆ†å¸ƒæ£€æµ‹
  def detectPartitionSkew(df: DataFrame): PartitionSkewReport = {
    val partitionSizes = df.rdd.mapPartitionsWithIndex { (index, partition) =>
      val count = partition.size
      val memoryUsage = estimatePartitionMemory(partition)
      Iterator((index, count, memoryUsage))
    }.collect()
    
    val stats = partitionSizes.map(_._2)  // å„åˆ†åŒºè®°å½•æ•°
    val mean = stats.sum.toDouble / stats.length
    val std = math.sqrt(stats.map(x => math.pow(x - mean, 2)).sum / stats.length)
    val cv = std / mean  // å˜å¼‚ç³»æ•°
    
    val skewedPartitions = partitionSizes.filter(_._2 > mean + 2 * std)
    
    PartitionSkewReport(
      totalPartitions = partitionSizes.length,
      averageSize = mean.toLong,
      standardDeviation = std,
      coefficientOfVariation = cv,
      skewedPartitions = skewedPartitions.toSeq,
      isSkewed = cv > 0.5  // CV > 0.5è®¤ä¸ºå­˜åœ¨å€¾æ–œ
    )
  }
  
  // 2. Keyåˆ†å¸ƒæ£€æµ‹
  def detectKeySkew(df: DataFrame, keyColumn: String): KeySkewReport = {
    val keyDistribution = df
      .groupBy(keyColumn)
      .count()
      .orderBy($"count".desc)
    
    val stats = keyDistribution.select($"count").as[Long].collect()
    val total = stats.sum
    val mean = total.toDouble / stats.length
    
    // è®¡ç®—Top Keysçš„æ•°æ®å æ¯”
    val top10Keys = keyDistribution.limit(10).collect()
    val top10Percentage = top10Keys.map(_.getLong(1)).sum.toDouble / total
    
    // æ£€æµ‹çƒ­ç‚¹Key
    val hotKeys = top10Keys.filter(_.getLong(1) > mean * 10)
    
    KeySkewReport(
      totalKeys = stats.length,
      totalRecords = total,
      averageRecordsPerKey = mean.toLong,
      top10KeysPercentage = top10Percentage,
      hotKeys = hotKeys.map(row => (row.getString(0), row.getLong(1))).toSeq,
      isSkewed = top10Percentage > 0.8 || hotKeys.nonEmpty
    )
  }
  
  // 3. Joinå€¾æ–œæ£€æµ‹
  def detectJoinSkew(left: DataFrame, right: DataFrame, joinKeys: Seq[String]): JoinSkewReport = {
    val leftKeyDist = analyzeKeyDistribution(left, joinKeys)
    val rightKeyDist = analyzeKeyDistribution(right, joinKeys)
    
    // é¢„ä¼°Joinåçš„æ•°æ®åˆ†å¸ƒ
    val joinedKeys = leftKeyDist.intersect(rightKeyDist)
    val potentialHotKeys = joinedKeys.filter { case (key, leftCount, rightCount) =>
      leftCount * rightCount > 1000000  // é¢„ä¼°å•ä¸ªKeyçš„è¾“å‡ºè®°å½•æ•°è¶…è¿‡100ä¸‡
    }
    
    JoinSkewReport(
      leftTableKeys = leftKeyDist.size,
      rightTableKeys = rightKeyDist.size,
      commonKeys = joinedKeys.size,
      potentialHotKeys = potentialHotKeys,
      estimatedSkewImpact = calculateSkewImpact(potentialHotKeys)
    )
  }
}
```

### 4.2 æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆ

```scala
// æ•°æ®å€¾æ–œè§£å†³æ–¹æ¡ˆé›†åˆ
class DataSkewSolutions {
  
  val spark = SparkSession.builder().getOrCreate()
  import spark.implicals._
  
  // 1. ä¸¤é˜¶æ®µèšåˆï¼ˆåŠ ç›èšåˆï¼‰
  def twoPhaseAggregation(
      df: DataFrame, 
      groupCols: Seq[String], 
      aggCol: String
  ): DataFrame = {
    
    // ç¬¬ä¸€é˜¶æ®µï¼šå±€éƒ¨èšåˆï¼ˆåŠ ç›ï¼‰
    val saltFactor = 100  // ç›å€¼èŒƒå›´
    val saltedDF = df.withColumn("salt", 
      concat(col(groupCols.head), lit("_"), (rand() * saltFactor).cast("int"))
    )
    
    val localAgg = saltedDF
      .groupBy("salt" +: groupCols.tail: _*)
      .agg(sum(aggCol).as("local_sum"))
    
    // ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€èšåˆï¼ˆå»ç›ï¼‰
    val globalAgg = localAgg
      .withColumn("original_key", split($"salt", "_").getItem(0))
      .drop("salt")
      .groupBy("original_key" +: groupCols.tail: _*)
      .agg(sum("local_sum").as(s"sum_$aggCol"))
    
    globalAgg
  }
  
  // 2. å€¾æ–œKeyéš”ç¦»å¤„ç†
  def isolateSkewedKeys(
      df: DataFrame,
      joinKey: String,
      skewedKeys: Seq[String],
      threshold: Long = 100000
  ): DataFrame = {
    
    // åˆ†ç¦»å€¾æ–œæ•°æ®å’Œæ­£å¸¸æ•°æ®
    val skewedData = df.filter(col(joinKey).isin(skewedKeys: _*))
    val normalData = df.filter(!col(joinKey).isin(skewedKeys: _*))
    
    // å¯¹å€¾æ–œæ•°æ®è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼ˆé‡åˆ†åŒºåˆ°æ›´å¤šåˆ†åŒºï¼‰
    val processedSkewedData = skewedData
      .repartition(200, col(joinKey))  // å¢åŠ åˆ†åŒºæ•°
      .withColumn("processing_flag", lit("skewed"))
    
    // æ­£å¸¸æ•°æ®å¸¸è§„å¤„ç†
    val processedNormalData = normalData
      .withColumn("processing_flag", lit("normal"))
    
    // åˆå¹¶ç»“æœ
    processedSkewedData.union(processedNormalData)
  }
  
  // 3. å¹¿æ’­Joinä¼˜åŒ–ï¼ˆå°è¡¨æ‰©å±•ï¼‰
  def broadcastJoinWithReplication(
      largeDf: DataFrame,
      smallDf: DataFrame, 
      joinKey: String,
      replicationFactor: Int = 100
  ): DataFrame = {
    
    // æ£€æµ‹å°è¡¨å¤§å°
    val smallTableSize = estimateDataFrameSize(smallDf)
    
    if (smallTableSize < 100 * 1024 * 1024) {  // å°äº100MB
      // ç›´æ¥å¹¿æ’­Join
      largeDf.join(broadcast(smallDf), joinKey)
    } else {
      // å°è¡¨å¤åˆ¶ + å¤§è¡¨åŠ ç›
      val saltedLargeDF = largeDf.withColumn("salt", 
        (rand() * replicationFactor).cast("int")
      ).withColumn("salted_key", 
        concat(col(joinKey), lit("_"), col("salt"))
      )
      
      // å°è¡¨å¤åˆ¶åˆ°å¤šä¸ªç›å€¼
      val replicatedSmallDF = smallDf
        .withColumn("salt", explode(array((0 until replicationFactor).map(lit): _*)))
        .withColumn("salted_key", 
          concat(col(joinKey), lit("_"), col("salt"))
        )
      
      // åŸºäºç›å€¼Keyè¿›è¡ŒJoin
      saltedLargeDF.join(broadcast(replicatedSmallDF), "salted_key")
        .drop("salt", "salted_key")
    }
  }
  
  // 4. éšæœºå‰ç¼€Joinï¼ˆåŒå€¾æ–œè¡¨ï¼‰
  def randomPrefixJoin(
      left: DataFrame,
      right: DataFrame,
      joinKey: String,
      prefixRange: Int = 100
  ): DataFrame = {
    
    // å·¦è¡¨æ·»åŠ éšæœºå‰ç¼€
    val prefixedLeft = left.withColumn("prefix", 
      (rand() * prefixRange).cast("int")
    ).withColumn("prefixed_key", 
      concat(col("prefix"), lit("_"), col(joinKey))
    )
    
    // å³è¡¨æ‰©å±•æ‰€æœ‰å‰ç¼€
    val expandedRight = right
      .withColumn("prefix", explode(array((0 until prefixRange).map(lit): _*)))
      .withColumn("prefixed_key", 
        concat(col("prefix"), lit("_"), col(joinKey))
      )
    
    // åŸºäºå‰ç¼€Keyè¿›è¡ŒJoin
    prefixedLeft.join(expandedRight, "prefixed_key")
      .drop("prefix", "prefixed_key")
  }
  
  // 5. åŠ¨æ€å€¾æ–œæ£€æµ‹ä¸å¤„ç†
  def adaptiveSkewHandling(
      df: DataFrame,
      groupCol: String,
      aggCol: String
  ): DataFrame = {
    
    // è¿è¡Œæ—¶æ£€æµ‹æ•°æ®å€¾æ–œ
    val keyDistribution = df.groupBy(groupCol).count()
    val stats = keyDistribution.agg(
      avg("count").as("avg_count"),
      stddev("count").as("std_count")
    ).collect().head
    
    val avgCount = stats.getDouble(0)
    val stdCount = stats.getDouble(1)
    val threshold = avgCount + 3 * stdCount  // 3ÏƒåŸåˆ™
    
    val skewedKeys = keyDistribution
      .filter($"count" > threshold)
      .select(groupCol)
      .as[String]
      .collect()
    
    if (skewedKeys.nonEmpty) {
      logInfo(s"Detected ${skewedKeys.length} skewed keys, applying two-phase aggregation")
      twoPhaseAggregation(df, Seq(groupCol), aggCol)
    } else {
      logInfo("No skew detected, using normal aggregation")
      df.groupBy(groupCol).agg(sum(aggCol))
    }
  }
}
```

### 4.3 Joinå€¾æ–œä¼˜åŒ–

```scala
// Joinå€¾æ–œä¼˜åŒ–ç­–ç•¥
class JoinSkewOptimization {
  
  val spark = SparkSession.builder().getOrCreate()
  import spark.implicits._
  
  // 1. è‡ªé€‚åº”Joinç­–ç•¥é€‰æ‹©
  def adaptiveJoinStrategy(
      left: DataFrame,
      right: DataFrame, 
      joinKeys: Seq[String]
  ): DataFrame = {
    
    val leftSize = estimateSize(left)
    val rightSize = estimateSize(right) 
    val joinKeySkew = detectJoinSkew(left, right, joinKeys)
    
    (leftSize, rightSize, joinKeySkew.isSkewed) match {
      
      // å°è¡¨å¹¿æ’­Join
      case (_, rightSz, false) if rightSz < 100 * 1024 * 1024 =>
        left.join(broadcast(right), joinKeys)
      
      // å¤§è¡¨ä½†æ•°æ®å‡åŒ€ -> Sort Merge Join
      case (_, _, false) =>
        left.join(right, joinKeys) 
      
      // å­˜åœ¨å€¾æ–œ -> å€¾æ–œå¤„ç†ç­–ç•¥
      case (_, _, true) =>
        skewedJoinOptimization(left, right, joinKeys, joinKeySkew.hotKeys)
    }
  }
  
  // 2. å€¾æ–œJoinå¤„ç†
  def skewedJoinOptimization(
      left: DataFrame,
      right: DataFrame,
      joinKeys: Seq[String], 
      hotKeys: Seq[String]
  ): DataFrame = {
    
    // åˆ†ç¦»çƒ­ç‚¹æ•°æ®å’Œæ­£å¸¸æ•°æ®
    val (leftHot, leftNormal) = partitionByKeys(left, joinKeys.head, hotKeys)
    val (rightHot, rightNormal) = partitionByKeys(right, joinKeys.head, hotKeys)
    
    // æ­£å¸¸æ•°æ®ï¼šå¸¸è§„Join
    val normalJoin = leftNormal.join(rightNormal, joinKeys)
    
    // çƒ­ç‚¹æ•°æ®ï¼šç‰¹æ®Šå¤„ç†
    val hotJoin = if (estimateSize(rightHot) < 50 * 1024 * 1024) {
      // å°çƒ­ç‚¹è¡¨ï¼šå¹¿æ’­Join
      leftHot.join(broadcast(rightHot), joinKeys)
    } else {
      // å¤§çƒ­ç‚¹è¡¨ï¼šç›å€¼Join
      saltedJoin(leftHot, rightHot, joinKeys.head)
    }
    
    // åˆå¹¶ç»“æœ
    normalJoin.union(hotJoin)
  }
  
  // 3. ç›å€¼Joinå®ç°
  def saltedJoin(
      left: DataFrame,
      right: DataFrame,
      joinKey: String,
      saltRange: Int = 100
  ): DataFrame = {
    
    // å·¦è¡¨åŠ éšæœºç›å€¼
    val saltedLeft = left.withColumn("salt", 
      (rand() * saltRange).cast("int")
    ).withColumn("salted_key", 
      concat(col(joinKey), lit("_"), col("salt"))
    )
    
    // å³è¡¨æ‰©å±•æ‰€æœ‰ç›å€¼
    val expandedRight = right
      .withColumn("salt_id", explode(array((0 until saltRange).map(lit): _*)))
      .withColumn("salted_key", 
        concat(col(joinKey), lit("_"), col("salt_id"))
      )
      .drop("salt_id")
    
    // Joinå¹¶æ¸…ç†è¾…åŠ©åˆ—
    saltedLeft.join(expandedRight, "salted_key")
      .drop("salt", "salted_key")
  }
  
  // 4. åˆ†æ¡¶Joinä¼˜åŒ–ï¼ˆé¢„å¤„ç†ç­–ç•¥ï¼‰
  def bucketedJoinOptimization(): Unit = {
    
    // åˆ›å»ºåˆ†æ¡¶è¡¨
    spark.sql("""
      CREATE TABLE users_bucketed (
        user_id BIGINT, name STRING, region STRING
      ) USING DELTA
      CLUSTERED BY (user_id) INTO 200 BUCKETS
    """)
    
    spark.sql("""  
      CREATE TABLE orders_bucketed (
        order_id BIGINT, user_id BIGINT, amount DOUBLE
      ) USING DELTA
      CLUSTERED BY (user_id) INTO 200 BUCKETS
    """)
    
    // å†™å…¥æ•°æ®æ—¶è‡ªåŠ¨åˆ†æ¡¶
    val users = spark.table("users") 
    val orders = spark.table("orders")
    
    users.write
      .format("delta")
      .mode("overwrite") 
      .option("path", "s3://warehouse/users_bucketed")
      .bucketBy(200, "user_id")
      .saveAsTable("users_bucketed")
    
    orders.write
      .format("delta")
      .mode("overwrite")
      .option("path", "s3://warehouse/orders_bucketed") 
      .bucketBy(200, "user_id")
      .saveAsTable("orders_bucketed")
    
    // åˆ†æ¡¶è¡¨Joinï¼ˆæ— Shuffleï¼‰
    val result = spark.sql("""
      SELECT u.name, COUNT(*) as order_count, SUM(o.amount) as total_amount
      FROM users_bucketed u
      JOIN orders_bucketed o ON u.user_id = o.user_id
      GROUP BY u.name
    """)
  }
}
```

---

## 5. ç¼“å­˜ç­–ç•¥ä¼˜åŒ–

### 5.1 ç¼“å­˜ç­–ç•¥é€‰æ‹©

```scala
// ç¼“å­˜ç­–ç•¥ä¼˜åŒ–æŒ‡å—
class CacheStrategyOptimizer {
  
  val spark = SparkSession.builder().getOrCreate()
  
  // 1. æ™ºèƒ½ç¼“å­˜ç­–ç•¥é€‰æ‹©
  def chooseCacheStrategy(
      df: DataFrame,
      accessPattern: AccessPattern,
      dataCharacteristics: DataCharacteristics
  ): StorageLevel = {
    
    val estimatedSize = estimateDataFrameSize(df)
    val availableMemory = getAvailableMemory()
    
    (accessPattern, dataCharacteristics, estimatedSize) match {
      
      // é¢‘ç¹è®¿é—® + å°æ•°æ® -> å†…å­˜å­˜å‚¨
      case (AccessPattern.FREQUENT, _, size) if size < availableMemory * 0.3 =>
        StorageLevel.MEMORY_ONLY
      
      // é¢‘ç¹è®¿é—® + ä¸­ç­‰æ•°æ® -> åºåˆ—åŒ–å†…å­˜å­˜å‚¨  
      case (AccessPattern.FREQUENT, DataCharacteristics.SERIALIZABLE, size) 
           if size < availableMemory * 0.6 =>
        StorageLevel.MEMORY_ONLY_SER
        
      // å¶å°”è®¿é—® + å¤§æ•°æ® -> å†…å­˜+ç£ç›˜
      case (AccessPattern.OCCASIONAL, _, _) =>
        StorageLevel.MEMORY_AND_DISK_SER
        
      // CPUå¯†é›†å‹å¤„ç† -> é¿å…åºåˆ—åŒ–å¼€é”€
      case (_, DataCharacteristics.CPU_INTENSIVE, _) =>
        StorageLevel.MEMORY_AND_DISK
        
      // å¤šå‰¯æœ¬éœ€æ±‚ -> å¸¦å‰¯æœ¬çš„å­˜å‚¨
      case (AccessPattern.FREQUENT, DataCharacteristics.FAULT_TOLERANT, _) =>
        StorageLevel.MEMORY_AND_DISK_2
        
      // é»˜è®¤ç­–ç•¥
      case _ =>
        StorageLevel.MEMORY_AND_DISK
    }
  }
  
  // 2. åˆ†å±‚ç¼“å­˜ç­–ç•¥
  def multilevelCacheStrategy(): Unit = {
    val rawData = spark.read.parquet("hdfs://raw_data/")
    val processedData = rawData.filter($"status" === "ACTIVE")
    val aggregatedData = processedData.groupBy("category").count()
    
    // L1ç¼“å­˜ï¼šçƒ­ç‚¹æ•°æ®ï¼Œå†…å­˜å­˜å‚¨
    val hotData = processedData.filter($"priority" === "HIGH")
    hotData.persist(StorageLevel.MEMORY_ONLY)
    
    // L2ç¼“å­˜ï¼šæ¸©æ•°æ®ï¼Œåºåˆ—åŒ–å†…å­˜å­˜å‚¨
    processedData.persist(StorageLevel.MEMORY_ONLY_SER)
    
    // L3ç¼“å­˜ï¼šå†·æ•°æ®ï¼Œå†…å­˜+ç£ç›˜å­˜å‚¨
    rawData.persist(StorageLevel.MEMORY_AND_DISK_SER)
    
    // æ‰§è¡ŒæŸ¥è¯¢
    val result1 = hotData.count()           // ä»L1ç¼“å­˜è¯»å–
    val result2 = processedData.count()     // ä»L2ç¼“å­˜è¯»å–
    val result3 = aggregatedData.collect()  // ä»L2ç¼“å­˜è®¡ç®—
    
    // åŠæ—¶æ¸…ç†ç¼“å­˜
    cleanupCache(hotData, processedData, rawData)
  }
  
  // 3. åŠ¨æ€ç¼“å­˜ç®¡ç†
  def dynamicCacheManagement(): Unit = {
    val cacheManager = new DynamicCacheManager()
    
    val datasets = List(
      ("dataset_A", spark.table("table_A")),
      ("dataset_B", spark.table("table_B")), 
      ("dataset_C", spark.table("table_C"))
    )
    
    datasets.foreach { case (name, df) =>
      // åŸºäºè®¿é—®é¢‘ç‡åŠ¨æ€è°ƒæ•´ç¼“å­˜ç­–ç•¥
      val accessFreq = cacheManager.getAccessFrequency(name)
      val storageLevel = if (accessFreq > 10) {
        StorageLevel.MEMORY_ONLY
      } else if (accessFreq > 5) {
        StorageLevel.MEMORY_ONLY_SER
      } else {
        StorageLevel.MEMORY_AND_DISK_SER
      }
      
      df.persist(storageLevel)
      cacheManager.registerCache(name, df, storageLevel)
    }
    
    // å®šæœŸæ¸…ç†ä½é¢‘è®¿é—®çš„ç¼“å­˜
    cacheManager.cleanupLowFrequencyCache()
  }
  
  // 4. ç¼“å­˜é¢„çƒ­ç­–ç•¥
  def cacheWarmupStrategy(): Unit = {
    val commonQueries = List(
      spark.table("users").filter($"status" === "ACTIVE"),
      spark.table("products").filter($"category" === "electronics"),
      spark.table("orders").filter($"order_date" >= "2023-01-01")
    )
    
    // åå°é¢„çƒ­ç¼“å­˜
    val warmupFutures = commonQueries.map { df =>
      Future {
        df.persist(StorageLevel.MEMORY_ONLY_SER)
        df.count()  // è§¦å‘ç¼“å­˜åŠ è½½
        logInfo(s"Cache warmed up for query: ${df.queryExecution.logical}")
      }
    }
    
    // ç­‰å¾…æ‰€æœ‰ç¼“å­˜é¢„çƒ­å®Œæˆ
    Future.sequence(warmupFutures).foreach { _ =>
      logInfo("All caches warmed up successfully")
    }
  }
}
```

### 5.2 ç¼“å­˜æ€§èƒ½ç›‘æ§

```scala
// ç¼“å­˜æ€§èƒ½ç›‘æ§
class CachePerformanceMonitor {
  
  val spark = SparkSession.builder().getOrCreate()
  
  // 1. ç¼“å­˜å‘½ä¸­ç‡ç›‘æ§
  def monitorCacheHitRate(): CacheMetrics = {
    val sc = spark.sparkContext
    
    // è·å–å­˜å‚¨ä¿¡æ¯
    val storageStatus = sc.getExecutorStorageStatus
    val rddInfos = sc.getPersistentRDDs
    
    val cacheMetrics = rddInfos.map { case (id, rdd) =>
      val rddInfo = sc.statusTracker.getRDDInfo(id)
      
      rddInfo.map { info =>
        val totalPartitions = info.numPartitions
        val cachedPartitions = info.numCachedPartitions
        val hitRate = if (totalPartitions > 0) {
          cachedPartitions.toDouble / totalPartitions
        } else 0.0
        
        RDDCacheMetrics(
          rddId = id,
          name = rdd.name,
          hitRate = hitRate,
          memorySize = info.memoryUsed,
          diskSize = info.diskUsed,
          partitions = totalPartitions,
          cachedPartitions = cachedPartitions
        )
      }
    }.flatten.toSeq
    
    CacheMetrics(
      totalCachedRDDs = cacheMetrics.size,
      averageHitRate = cacheMetrics.map(_.hitRate).sum / cacheMetrics.size,
      totalMemoryUsed = cacheMetrics.map(_.memorySize).sum,
      totalDiskUsed = cacheMetrics.map(_.diskSize).sum,
      rddMetrics = cacheMetrics
    )
  }
  
  // 2. ç¼“å­˜æˆæœ¬æ”¶ç›Šåˆ†æ
  def analyzeCacheCostBenefit(df: DataFrame): CacheCostBenefitAnalysis = {
    val startTime = System.currentTimeMillis()
    
    // ä¸ä½¿ç”¨ç¼“å­˜çš„æ‰§è¡Œæ—¶é—´
    val uncachedTime = measureExecutionTime {
      df.count()
      df.filter($"value" > 100).count()
      df.groupBy("category").count().collect()
    }
    
    // ä½¿ç”¨ç¼“å­˜çš„æ‰§è¡Œæ—¶é—´
    df.cache()
    val firstAccess = measureExecutionTime { df.count() }  // ç¼“å­˜åŠ è½½æ—¶é—´
    
    val cachedTime = measureExecutionTime {
      df.filter($"value" > 100).count()
      df.groupBy("category").count().collect()
    }
    
    val cacheSize = estimateDataFrameSize(df)
    val speedup = uncachedTime.toDouble / (firstAccess + cachedTime)
    val efficiency = speedup / (cacheSize / 1024 / 1024)  // æ¯MBç¼“å­˜çš„åŠ é€Ÿæ¯”
    
    CacheCostBenefitAnalysis(
      uncachedExecutionTime = uncachedTime,
      cachedExecutionTime = cachedTime,
      cacheLoadTime = firstAccess,
      cacheSize = cacheSize,
      speedupRatio = speedup,
      efficiency = efficiency,
      recommendation = if (speedup > 2.0 && efficiency > 0.1) "BENEFICIAL" else "NOT_BENEFICIAL"
    )
  }
  
  // 3. è‡ªåŠ¨ç¼“å­˜æ¸…ç†
  def automaticCacheCleanup(): Unit = {
    val cacheTracker = new CacheUsageTracker()
    
    // å®šæœŸæ£€æŸ¥ç¼“å­˜ä½¿ç”¨æƒ…å†µ
    val cleanupScheduler = Executors.newScheduledThreadPool(1)
    cleanupScheduler.scheduleAtFixedRate(
      new Runnable {
        override def run(): Unit = {
          val metrics = monitorCacheHitRate()
          
          metrics.rddMetrics.foreach { rddMetric =>
            val usage = cacheTracker.getUsageHistory(rddMetric.rddId)
            
            // æ¸…ç†ä½æ•ˆç¼“å­˜çš„æ¡ä»¶ï¼š
            // 1. å‘½ä¸­ç‡ < 20%
            // 2. æœ€è¿‘10åˆ†é’Ÿæœªè®¿é—®
            // 3. å ç”¨å†…å­˜ > 1GB
            if (rddMetric.hitRate < 0.2 && 
                usage.lastAccessTime < System.currentTimeMillis() - 10 * 60 * 1000 &&
                rddMetric.memorySize > 1024 * 1024 * 1024) {
              
              logInfo(s"Cleaning up inefficient cache: RDD ${rddMetric.rddId}")
              spark.sparkContext.getPersistentRDDs.get(rddMetric.rddId).foreach(_.unpersist(true))
              cacheTracker.removeTracking(rddMetric.rddId)
            }
          }
        }
      },
      5,    // åˆå§‹å»¶è¿Ÿ5åˆ†é’Ÿ
      5,    // æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
      TimeUnit.MINUTES
    )
  }
}
```

---

## 6. ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

### 6.1 èµ„æºéš”ç¦»ä¸é…ç½®

```scala
// ç”Ÿäº§ç¯å¢ƒèµ„æºéš”ç¦»é…ç½®
class ProductionResourceConfiguration {
  
  // 1. YARNé˜Ÿåˆ—é…ç½®
  def configureYarnQueues(): Map[String, SparkConf] = {
    
    // ETLä½œä¸šé˜Ÿåˆ—ï¼ˆæ‰¹å¤„ç†ï¼Œé«˜ååé‡ï¼‰
    val etlConf = new SparkConf()
      .set("spark.yarn.queue", "etl")
      .set("spark.executor.instances", "50")
      .set("spark.executor.cores", "4") 
      .set("spark.executor.memory", "16g")
      .set("spark.dynamicAllocation.enabled", "true")
      .set("spark.dynamicAllocation.maxExecutors", "100")
    
    // å®æ—¶åˆ†æé˜Ÿåˆ—ï¼ˆä½å»¶è¿Ÿï¼‰
    val realtimeConf = new SparkConf()
      .set("spark.yarn.queue", "realtime")
      .set("spark.executor.instances", "20")
      .set("spark.executor.cores", "2")
      .set("spark.executor.memory", "8g") 
      .set("spark.streaming.backpressure.enabled", "true")
      .set("spark.locality.wait", "1s")  // é™ä½æœ¬åœ°æ€§ç­‰å¾…æ—¶é—´
    
    // äº¤äº’å¼æŸ¥è¯¢é˜Ÿåˆ—ï¼ˆå¿«é€Ÿå“åº”ï¼‰
    val interactiveConf = new SparkConf()
      .set("spark.yarn.queue", "interactive")
      .set("spark.executor.instances", "10")
      .set("spark.executor.cores", "2")
      .set("spark.executor.memory", "4g")
      .set("spark.sql.adaptive.enabled", "true")
      .set("spark.sql.adaptive.coalescePartitions.enabled", "true")
    
    Map(
      "etl" -> etlConf,
      "realtime" -> realtimeConf, 
      "interactive" -> interactiveConf
    )
  }
  
  // 2. ç¯å¢ƒéš”ç¦»é…ç½®
  def configureEnvironmentIsolation(): Map[String, SparkConf] = {
    
    // å¼€å‘ç¯å¢ƒ
    val devConf = new SparkConf()
      .set("spark.executor.instances", "5")
      .set("spark.executor.memory", "2g")
      .set("spark.sql.shuffle.partitions", "20")
      .set("spark.serializer", "org.apache.spark.serializer.JavaSerializer")  // å¼€å‘è°ƒè¯•å‹å¥½
    
    // æµ‹è¯•ç¯å¢ƒ  
    val testConf = new SparkConf()
      .set("spark.executor.instances", "10")
      .set("spark.executor.memory", "4g")
      .set("spark.sql.shuffle.partitions", "100")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    
    // ç”Ÿäº§ç¯å¢ƒ
    val prodConf = new SparkConf()
      .set("spark.executor.instances", "50") 
      .set("spark.executor.memory", "16g")
      .set("spark.sql.shuffle.partitions", "800")
      .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
      .set("spark.sql.adaptive.enabled", "true")
      .set("spark.memory.offHeap.enabled", "true")
      .set("spark.memory.offHeap.size", "8g")
    
    Map(
      "dev" -> devConf,
      "test" -> testConf,
      "prod" -> prodConf
    )
  }
  
  // 3. å®‰å…¨é…ç½®
  def configureSecuritySettings(): SparkConf = {
    new SparkConf()
      // Kerberosè®¤è¯
      .set("spark.kerberos.keytab", "/path/to/spark.keytab")
      .set("spark.kerberos.principal", "spark/hostname@REALM.COM")
      
      // SSLåŠ å¯†
      .set("spark.ssl.enabled", "true")
      .set("spark.ssl.keyStore", "/path/to/keystore.jks")
      .set("spark.ssl.keyStorePassword", "keystore-password")
      .set("spark.ssl.trustStore", "/path/to/truststore.jks")
      .set("spark.ssl.trustStorePassword", "truststore-password")
      
      // ç½‘ç»œåŠ å¯†
      .set("spark.network.crypto.enabled", "true")
      .set("spark.io.encryption.enabled", "true")
      
      // å®¡è®¡æ—¥å¿—
      .set("spark.eventLog.enabled", "true")
      .set("spark.eventLog.dir", "hdfs://audit-logs/spark-events")
  }
}
```

### 6.2 ç›‘æ§ä¸å‘Šè­¦

```scala
// ç”Ÿäº§ç¯å¢ƒç›‘æ§ç³»ç»Ÿ
class ProductionMonitoring {
  
  val spark = SparkSession.builder().getOrCreate()
  
  // 1. ä½œä¸šå¥åº·ç›‘æ§
  def jobHealthMonitoring(): Unit = {
    val healthChecker = new SparkJobHealthChecker()
    
    spark.sparkContext.addSparkListener(new SparkListener {
      
      override def onJobStart(jobStart: SparkListenerJobStart): Unit = {
        val jobId = jobStart.jobId
        val startTime = jobStart.time
        
        // è®°å½•ä½œä¸šå¼€å§‹
        healthChecker.recordJobStart(jobId, startTime)
        
        // è®¾ç½®è¶…æ—¶ç›‘æ§
        scheduleTimeoutCheck(jobId, expectedJobDuration = 30 * 60 * 1000) // 30åˆ†é’Ÿ
      }
      
      override def onJobEnd(jobEnd: SparkListenerJobEnd): Unit = {
        val jobId = jobEnd.jobId
        val endTime = jobEnd.time
        val duration = endTime - healthChecker.getJobStartTime(jobId)
        
        // æ€§èƒ½å‘Šè­¦
        if (duration > expectedJobDuration) {
          sendAlert(AlertType.SLOW_JOB, s"Job $jobId took ${duration/1000}s to complete")
        }
        
        // å¤±è´¥å‘Šè­¦
        jobEnd.jobResult match {
          case JobSucceeded => 
            logInfo(s"Job $jobId completed successfully in ${duration}ms")
          case JobFailed(exception) =>
            sendAlert(AlertType.JOB_FAILURE, s"Job $jobId failed: ${exception.getMessage}")
        }
      }
      
      override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
        val taskInfo = taskEnd.taskInfo
        val taskMetrics = taskEnd.taskMetrics
        
        // æ£€æµ‹å¼‚å¸¸ä»»åŠ¡
        if (taskInfo.duration > 5 * 60 * 1000) {  // ä»»åŠ¡è¶…è¿‡5åˆ†é’Ÿ
          logWarning(s"Slow task detected: ${taskInfo.taskId}, duration: ${taskInfo.duration}ms")
        }
        
        if (taskMetrics != null) {
          // å†…å­˜ä½¿ç”¨å‘Šè­¦
          val memoryUsage = taskMetrics.memoryBytesSpilled + taskMetrics.diskBytesSpilled
          if (memoryUsage > 1024 * 1024 * 1024) {  // 1GBæº¢å†™
            logWarning(s"High spill detected: ${memoryUsage / 1024 / 1024}MB")
          }
        }
      }
    })
  }
  
  // 2. èµ„æºä½¿ç”¨ç›‘æ§
  def resourceUsageMonitoring(): Unit = {
    val scheduler = Executors.newScheduledThreadPool(1)
    
    scheduler.scheduleAtFixedRate(new Runnable {
      override def run(): Unit = {
        try {
          val executorInfos = spark.sparkContext.statusTracker.getExecutorInfos
          
          executorInfos.foreach { executor =>
            val memoryUsed = executor.memoryUsed
            val memoryTotal = executor.maxMemory
            val memoryUtilization = memoryUsed.toDouble / memoryTotal
            
            // å†…å­˜ä½¿ç”¨å‘Šè­¦
            if (memoryUtilization > 0.9) {
              sendAlert(AlertType.HIGH_MEMORY_USAGE, 
                       s"Executor ${executor.executorId} memory usage: ${(memoryUtilization * 100).toInt}%")
            }
            
            // GCå‘Šè­¦
            val gcTime = executor.totalGCTime
            if (gcTime > lastGCTime.getOrElse(executor.executorId, 0L) + 60000) { // GCæ—¶é—´å¢é•¿è¶…è¿‡1åˆ†é’Ÿ
              sendAlert(AlertType.HIGH_GC_TIME, 
                       s"Executor ${executor.executorId} GC time increased significantly")
            }
            lastGCTime(executor.executorId) = gcTime
          }
          
          // é›†ç¾¤èµ„æºå‘Šè­¦
          val totalExecutors = executorInfos.length
          val activeStages = spark.sparkContext.statusTracker.getActiveStageIds().length
          
          if (totalExecutors < minRequiredExecutors) {
            sendAlert(AlertType.INSUFFICIENT_RESOURCES, s"Only $totalExecutors executors available")
          }
          
        } catch {
          case e: Exception =>
            logError("Error in resource monitoring", e)
        }
      }
    }, 30, 30, TimeUnit.SECONDS)  // æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
  }
  
  // 3. æ•°æ®è´¨é‡ç›‘æ§
  def dataQualityMonitoring(): Unit = {
    val qualityRules = List(
      DataQualityRule("null_check", "Check for null values in critical columns"),
      DataQualityRule("range_check", "Check if values are within expected ranges"),
      DataQualityRule("format_check", "Check data format consistency"),
      DataQualityRule("duplicate_check", "Check for duplicate records")
    )
    
    def validateDataQuality(df: DataFrame, tableName: String): DataQualityReport = {
      val totalRecords = df.count()
      val qualityResults = mutable.ArrayBuffer[QualityCheckResult]()
      
      // ç©ºå€¼æ£€æŸ¥
      val nullCounts = df.columns.map { col =>
        val nullCount = df.filter(df(col).isNull).count()
        (col, nullCount, nullCount.toDouble / totalRecords)
      }.filter(_._3 > 0.05)  // ç©ºå€¼ç‡è¶…è¿‡5%
      
      if (nullCounts.nonEmpty) {
        qualityResults += QualityCheckResult(
          rule = "null_check",
          status = "FAILED",
          message = s"High null rate in columns: ${nullCounts.map(_._1).mkString(", ")}"
        )
      }
      
      // é‡å¤è®°å½•æ£€æŸ¥
      val distinctRecords = df.distinct().count()
      val duplicateRate = 1.0 - (distinctRecords.toDouble / totalRecords)
      
      if (duplicateRate > 0.01) {  // é‡å¤ç‡è¶…è¿‡1%
        qualityResults += QualityCheckResult(
          rule = "duplicate_check",
          status = "FAILED", 
          message = s"Duplicate rate: ${(duplicateRate * 100).toInt}%"
        )
      }
      
      DataQualityReport(
        tableName = tableName,
        totalRecords = totalRecords,
        checkResults = qualityResults.toSeq,
        overallScore = calculateQualityScore(qualityResults.toSeq)
      )
    }
    
    // å¯¹å…³é”®è¡¨è¿›è¡Œè´¨é‡æ£€æŸ¥
    val criticalTables = List("users", "orders", "products")
    criticalTables.foreach { tableName =>
      val df = spark.table(tableName)
      val report = validateDataQuality(df, tableName)
      
      if (report.overallScore < 0.8) {  // è´¨é‡åˆ†æ•°ä½äº80%
        sendAlert(AlertType.DATA_QUALITY_ISSUE, 
                 s"Data quality issue in $tableName: ${report.overallScore}")
      }
    }
  }
  
  // 4. å‘Šè­¦ç³»ç»Ÿé›†æˆ
  def sendAlert(alertType: AlertType, message: String): Unit = {
    val alert = Alert(
      id = UUID.randomUUID().toString,
      timestamp = Timestamp.valueOf(LocalDateTime.now()),
      alertType = alertType,
      severity = determineSeverity(alertType),
      message = message,
      environment = getCurrentEnvironment()
    )
    
    // å‘é€åˆ°ä¸åŒæ¸ é“
    alertType.severity match {
      case Severity.CRITICAL => 
        sendToSlack(alert)
        sendToPagerDuty(alert)
        sendToEmail(alert)
        
      case Severity.HIGH =>
        sendToSlack(alert)
        sendToEmail(alert)
        
      case Severity.MEDIUM =>
        sendToSlack(alert)
        
      case Severity.LOW =>
        logWarning(alert.message)
    }
    
    // å­˜å‚¨å‘Šè­¦å†å²
    storeAlert(alert)
  }
}
```

### 6.3 æ€§èƒ½åŸºå‡†ä¸å›å½’æµ‹è¯•

```scala
// æ€§èƒ½åŸºå‡†ä¸å›å½’æµ‹è¯•æ¡†æ¶
class PerformanceRegressionTesting {
  
  val spark = SparkSession.builder().getOrCreate()
  
  // 1. æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶
  case class BenchmarkSuite(
    name: String,
    benchmarks: Seq[Benchmark]
  )
  
  case class Benchmark(
    name: String,
    setupFunction: () => DataFrame,
    testFunction: DataFrame => Any,
    expectedDuration: Long,  // é¢„æœŸæ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    tolerance: Double = 0.2  // å®¹å¿åº¦ï¼ˆ20%ï¼‰
  )
  
  def createBenchmarkSuite(): BenchmarkSuite = {
    BenchmarkSuite(
      name = "Core Performance Suite",
      benchmarks = Seq(
        
        // åŸºç¡€æŸ¥è¯¢æ€§èƒ½
        Benchmark(
          name = "simple_aggregation",
          setupFunction = () => createTestData(10000000),
          testFunction = df => df.groupBy("category").count().collect(),
          expectedDuration = 30000  // 30ç§’
        ),
        
        // Joinæ€§èƒ½æµ‹è¯•
        Benchmark(
          name = "large_join",
          setupFunction = () => {
            val left = createTestData(5000000)
            val right = createTestData(5000000) 
            left.createOrReplaceTempView("left")
            right.createOrReplaceTempView("right")
            spark.sql("SELECT * FROM left JOIN right ON left.id = right.id")
          },
          testFunction = df => df.count(),
          expectedDuration = 60000  // 1åˆ†é’Ÿ
        ),
        
        // çª—å£å‡½æ•°æ€§èƒ½
        Benchmark(
          name = "window_function",
          setupFunction = () => createTimeSeries(1000000),
          testFunction = df => df.withColumn("row_number", 
            row_number().over(Window.partitionBy("category").orderBy("timestamp"))).collect(),
          expectedDuration = 45000  // 45ç§’
        )
      )
    )
  }
  
  // 2. åŸºå‡†æµ‹è¯•æ‰§è¡Œå™¨
  def runBenchmarkSuite(suite: BenchmarkSuite): BenchmarkReport = {
    val results = suite.benchmarks.map { benchmark =>
      logInfo(s"Running benchmark: ${benchmark.name}")
      
      val setupStartTime = System.currentTimeMillis()
      val testData = benchmark.setupFunction()
      val setupDuration = System.currentTimeMillis() - setupStartTime
      
      // é¢„çƒ­JVM
      try {
        benchmark.testFunction(testData)
      } catch {
        case _: Exception => // å¿½ç•¥é¢„çƒ­é˜¶æ®µçš„å¼‚å¸¸
      }
      
      // æ­£å¼æµ‹è¯•ï¼ˆå¤šæ¬¡è¿è¡Œå–å¹³å‡å€¼ï¼‰
      val runTimes = (1 to 3).map { _ =>
        val startTime = System.currentTimeMillis()
        try {
          benchmark.testFunction(testData)
          val endTime = System.currentTimeMillis()
          Some(endTime - startTime)
        } catch {
          case e: Exception =>
            logError(s"Benchmark ${benchmark.name} failed", e)
            None
        }
      }.flatten
      
      val avgDuration = if (runTimes.nonEmpty) runTimes.sum / runTimes.length else Long.MaxValue
      val deviation = (avgDuration - benchmark.expectedDuration).toDouble / benchmark.expectedDuration
      
      BenchmarkResult(
        name = benchmark.name,
        expectedDuration = benchmark.expectedDuration,
        actualDuration = avgDuration,
        setupDuration = setupDuration,
        deviation = deviation,
        passed = math.abs(deviation) <= benchmark.tolerance,
        error = if (runTimes.isEmpty) Some("Test failed to execute") else None
      )
    }
    
    BenchmarkReport(
      suiteNam = suite.name,
      timestamp = System.currentTimeMillis(),
      sparkVersion = spark.version,
      results = results,
      overallPassed = results.forall(_.passed)
    )
  }
  
  // 3. æ€§èƒ½å›å½’æ£€æµ‹
  def detectPerformanceRegression(
      currentReport: BenchmarkReport,
      baselineReport: BenchmarkReport
  ): RegressionAnalysis = {
    
    val regressions = currentReport.results.zip(baselineReport.results).map {
      case (current, baseline) =>
        val performanceChange = (current.actualDuration - baseline.actualDuration).toDouble / baseline.actualDuration
        
        RegressionResult(
          benchmarkName = current.name,
          baselineDuration = baseline.actualDuration,
          currentDuration = current.actualDuration,
          performanceChange = performanceChange,
          isRegression = performanceChange > 0.1,  // æ€§èƒ½ä¸‹é™è¶…è¿‡10%è®¤ä¸ºæ˜¯å›å½’
          severity = classifyRegressionSeverity(performanceChange)
        )
    }
    
    val significantRegressions = regressions.filter(_.isRegression)
    
    RegressionAnalysis(
      totalBenchmarks = regressions.length,
      regressionsFound = significantRegressions.length,
      regressions = significantRegressions,
      overallHealthy = significantRegressions.isEmpty,
      recommendation = generateRegressionRecommendation(significantRegressions)
    )
  }
  
  // 4. CI/CDé›†æˆ
  def integrateContinuousPerformanceTesting(): Unit = {
    // åœ¨æ„å»ºæµæ°´çº¿ä¸­é›†æˆæ€§èƒ½æµ‹è¯•
    val suite = createBenchmarkSuite()
    val currentReport = runBenchmarkSuite(suite)
    
    // ä¸å†å²åŸºå‡†å¯¹æ¯”
    val baselineReport = loadBaselineReport()
    val regressionAnalysis = detectPerformanceRegression(currentReport, baselineReport)
    
    // ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
    generatePerformanceReport(currentReport, regressionAnalysis)
    
    // å¦‚æœå‘ç°ä¸¥é‡æ€§èƒ½å›å½’ï¼Œé˜»æ­¢éƒ¨ç½²
    if (regressionAnalysis.regressions.exists(_.severity == Severity.CRITICAL)) {
      throw new RuntimeException("Critical performance regression detected, blocking deployment")
    }
    
    // æ›´æ–°åŸºå‡†
    if (regressionAnalysis.overallHealthy) {
      updateBaseline(currentReport)
    }
  }
}
```

---

## æ€»ç»“

æœ¬ç« å…¨é¢ä»‹ç»äº†Sparkæ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µï¼š

1. **è°ƒä¼˜æ–¹æ³•è®º**ï¼šç³»ç»ŸåŒ–çš„æ€§èƒ½åˆ†ææ¡†æ¶ã€è¯Šæ–­å·¥å…·ã€åŸºå‡†æµ‹è¯•
2. **èµ„æºé…ç½®**ï¼šExecutorã€Driverã€å­˜å‚¨ç³»ç»Ÿçš„ä¼˜åŒ–é…ç½®ç­–ç•¥
3. **ä»£ç ä¼˜åŒ–**ï¼šSQLæŸ¥è¯¢ã€DataFrame APIã€RDDæ“ä½œçš„ä¼˜åŒ–æŠ€å·§
4. **æ•°æ®å€¾æ–œ**ï¼šæ£€æµ‹å·¥å…·ã€è§£å†³æ–¹æ¡ˆã€Joinä¼˜åŒ–ç­–ç•¥
5. **ç¼“å­˜ç­–ç•¥**ï¼šæ™ºèƒ½ç¼“å­˜é€‰æ‹©ã€æ€§èƒ½ç›‘æ§ã€åŠ¨æ€ç®¡ç†
6. **ç”Ÿäº§å®è·µ**ï¼šèµ„æºéš”ç¦»ã€ç›‘æ§å‘Šè­¦ã€æ€§èƒ½å›å½’æµ‹è¯•

è¿™äº›å®è·µç»éªŒæ¥è‡ªçœŸå®çš„ç”Ÿäº§ç¯å¢ƒï¼Œå¯ä»¥ç›´æ¥åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­ï¼Œæ˜¾è‘—æå‡Sparkåº”ç”¨çš„æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

ä¸‹ä¸€èŠ‚æˆ‘ä»¬å°†ç»“åˆé¡¹ç›®ä»£ç åˆ†æå®é™…çš„Sparkåº”ç”¨æ¨¡å¼ã€‚

---

## æ¨èé˜…è¯»
- [Sparkå®˜æ–¹è°ƒä¼˜æŒ‡å—](https://spark.apache.org/docs/latest/tuning.html)
- ã€ŠSparkæ€§èƒ½è°ƒä¼˜å®æˆ˜ã€‹
- [Databricksæ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ](https://docs.databricks.com/optimizations/index.html)
```

```markdown
# é¡¹ç›®ä»£ç åˆ†æï¼šå®é™…Sparkåº”ç”¨æ¨¡å¼æ·±åº¦å­¦ä¹ ç¬”è®°

## ç›®å½•
1. [é¡¹ç›®æ¶æ„åˆ†æ](#1-é¡¹ç›®æ¶æ„åˆ†æ)
2. [Sparkä»»åŠ¡ç±»å‹ä¸é…ç½®](#2-sparkä»»åŠ¡ç±»å‹ä¸é…ç½®)
3. [å·¥ä½œæµå¼•æ“å®ç°](#3-å·¥ä½œæµå¼•æ“å®ç°)
4. [ä»»åŠ¡è°ƒåº¦ä¸æ‰§è¡Œ](#4-ä»»åŠ¡è°ƒåº¦ä¸æ‰§è¡Œ)
5. [æœ€ä½³å®è·µåº”ç”¨](#5-æœ€ä½³å®è·µåº”ç”¨)
6. [æ¶æ„æ”¹è¿›å»ºè®®](#6-æ¶æ„æ”¹è¿›å»ºè®®)

---

## 1. é¡¹ç›®æ¶æ„åˆ†æ

### 1.1 Data Studioæ•´ä½“æ¶æ„

æ ¹æ®é¡¹ç›®ç»“æ„åˆ†æï¼ŒData Studioæ˜¯ä¸€ä¸ªä¼ä¸šçº§æ•°æ®å¹³å°ï¼Œæ”¯æŒå¤šç§è®¡ç®—å¼•æ“å’Œä»»åŠ¡ç±»å‹ï¼š

```mermaid
graph TB
    subgraph "Data Studioå¹³å°æ¶æ„"
        subgraph "å‰ç«¯å±‚ (studio-web)"
            WebUI[Webç•Œé¢<br/>ä»»åŠ¡ç®¡ç†/ç›‘æ§]
            RestAPI[REST API<br/>ä»»åŠ¡æäº¤/çŠ¶æ€æŸ¥è¯¢]
        end
        
        subgraph "æ ¸å¿ƒæœåŠ¡å±‚ (studio-core)"
            WorkflowEngine[å·¥ä½œæµå¼•æ“<br/>DAGæ‰§è¡Œç®¡ç†]
            TaskScheduler[ä»»åŠ¡è°ƒåº¦å™¨<br/>ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†]
            ResourceManager[èµ„æºç®¡ç†å™¨<br/>é›†ç¾¤èµ„æºåˆ†é…]
            TemplateService[æ¨¡æ¿æœåŠ¡<br/>ä»»åŠ¡æ¨¡æ¿ç®¡ç†]
        end
        
        subgraph "å®¢æˆ·ç«¯å±‚ (studio-client)"
            SparkClient[Sparkå®¢æˆ·ç«¯<br/>ä»»åŠ¡é…ç½®ä¸æäº¤]
            ConfigManager[é…ç½®ç®¡ç†<br/>ä»»åŠ¡å‚æ•°ç®¡ç†]
            DataIntegration[æ•°æ®é›†æˆ<br/>æ•°æ®æºè¿æ¥å™¨]
        end
        
        subgraph "è®¡ç®—å¼•æ“å±‚"
            SparkEngine[Apache Spark<br/>æ‰¹å¤„ç†/æµå¤„ç†/SQL]
            PrestoEngine[Presto<br/>äº¤äº’å¼æŸ¥è¯¢]
            FlinkEngine[Apache Flink<br/>ä½å»¶è¿Ÿæµå¤„ç†]
            JupyterEngine[Jupyter<br/>äº¤äº’å¼åˆ†æ]
        end
        
        subgraph "å­˜å‚¨å±‚"
            HDFS[HDFS<br/>åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿ]
            S3[Object Storage<br/>S3/äº‘å­˜å‚¨]
            Database[å…³ç³»æ•°æ®åº“<br/>å…ƒæ•°æ®å­˜å‚¨]
            Cache[ç¼“å­˜å±‚<br/>Redis/å†…å­˜ç¼“å­˜]
        end
        
        WebUI --> RestAPI
        RestAPI --> WorkflowEngine
        WorkflowEngine --> TaskScheduler
        TaskScheduler --> SparkClient
        SparkClient --> SparkEngine
        SparkEngine --> HDFS
        SparkEngine --> S3
    end
    
    style WorkflowEngine fill:#e3f2fd
    style SparkEngine fill:#e8f5e8
    style SparkClient fill:#fff3e0
    style HDFS fill:#f3e5f5
```

### 1.2 æ ¸å¿ƒæ¨¡å—åˆ†æ

åŸºäºé¡¹ç›®ä»£ç ï¼Œæˆ‘ä»¬æ·±å…¥åˆ†æå„ä¸ªæ ¸å¿ƒæ¨¡å—ï¼š

```java
// é¡¹ç›®æ ¸å¿ƒæ¨¡å—åˆ†æ
/**
 * studio-web: WebæœåŠ¡å±‚
 * - æä¾›REST APIæ¥å£
 * - ç”¨æˆ·ç•Œé¢äº¤äº’
 * - ä»»åŠ¡ç®¡ç†å‰ç«¯
 */

/**
 * studio-core: æ ¸å¿ƒä¸šåŠ¡é€»è¾‘
 * - å·¥ä½œæµæ‰§è¡Œå¼•æ“
 * - ä»»åŠ¡è°ƒåº¦ä¸ç›‘æ§
 * - æ¨¡æ¿ç®¡ç†æœåŠ¡
 * - æƒé™å’Œå®‰å…¨ç®¡ç†
 */

/**
 * studio-client: å®¢æˆ·ç«¯SDK
 * - å„ç§è®¡ç®—å¼•æ“çš„é…ç½®ç±»
 * - æ•°æ®æºè¿æ¥å™¨
 * - ä»»åŠ¡æäº¤å®¢æˆ·ç«¯
 * - å…¬å…±å·¥å…·ç±»
 */
```

---

## 2. Sparkä»»åŠ¡ç±»å‹ä¸é…ç½®

### 2.1 æ”¯æŒçš„Sparkä»»åŠ¡ç±»å‹

ä»é¡¹ç›®ä»£ç å¯ä»¥çœ‹å‡ºï¼Œæ”¯æŒä¸‰ç§ä¸»è¦çš„Sparkä»»åŠ¡ç±»å‹ï¼š

```java
// æ¥è‡ª TaskExeTypeEnum.java
public enum TaskExeTypeEnum {
    SPARK_JAR(1, "spark jar"),      // Scala/Java Sparkåº”ç”¨
    SPARK_SQL(2, "spark SQL"),      // SQLæŸ¥è¯¢ä»»åŠ¡  
    PYTHON_SPARK(3, "python spark"), // PySparkåº”ç”¨
    // ... å…¶ä»–ä»»åŠ¡ç±»å‹
}
```

#### 1. **Spark JARä»»åŠ¡é…ç½®åˆ†æ**

```java
// æ¥è‡ª SparkJarConfig.java
@Data
public class SparkJarConfig {
    private String sparkVersion;           // Sparkç‰ˆæœ¬
    private ResourceConfig mainResource;   // ä¸»JARåŒ…é…ç½®
    private String mainClass;              // å…¥å£ç±»
    private String sparkProperties;        // Sparkå‚æ•°é…ç½®
    private String programArgument;        // ç¨‹åºå‚æ•°
    private List<ResourceConfig> otherResources; // ä¾èµ–JARåŒ…
    private Boolean isStreamingTask;       // æ˜¯å¦æµå¤„ç†ä»»åŠ¡
    private YarnQueueInfo yarnQueueInfo;   // YARNé˜Ÿåˆ—ä¿¡æ¯
    private List<CampaignConfig> campaignConfigList; // æ´»åŠ¨é…ç½®
    private Boolean enableCampaignConfig;  // æ˜¯å¦å¯ç”¨æ´»åŠ¨é…ç½®
}

// å®é™…åº”ç”¨åœºæ™¯ç¤ºä¾‹
public class SparkJarTaskExample {
    
    public SparkJarConfig createEtlTaskConfig() {
        return SparkJarConfig.builder()
            .sparkVersion("3.2.0")
            .mainClass("com.shopee.di.etl.DataProcessingJob")
            .sparkProperties("""
                --conf spark.executor.memory=8g
                --conf spark.executor.cores=4  
                --conf spark.executor.instances=20
                --conf spark.sql.adaptive.enabled=true
                --conf spark.sql.adaptive.coalescePartitions.enabled=true
                --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
                """)
            .programArgument("--input hdfs://input/data/ --output hdfs://output/processed/")
            .isStreamingTask(false)
            .yarnQueueInfo(YarnQueueInfo.builder()
                .queueName("data-processing")
                .priority("NORMAL")
                .build())
            .build();
    }
}
```

#### 2. **PySparkä»»åŠ¡é…ç½®åˆ†æ**

```java
// æ¥è‡ª PythonSparkConfig.java
@Data
public class PythonSparkConfig {
    private String sparkVersion;
    private String language;               // Pythonè¯­è¨€æ ‡è¯†
    private ResourceConfig mainResource;   // ä¸»Pythonè„šæœ¬
    private String sparkProperties;
    private String programArgument;
    private List<ResourceConfig> otherResources; // ä¾èµ–æ–‡ä»¶/åŒ…
    private Boolean isStreamingTask;
    private YarnQueueInfo yarnQueueInfo;
    private List<CampaignConfig> campaignConfigList;
    private Boolean enableCampaignConfig;
}

// PySparkä»»åŠ¡é…ç½®ç¤ºä¾‹
public class PySparkTaskExample {
    
    public PythonSparkConfig createMLPipelineConfig() {
        return PythonSparkConfig.builder()
            .sparkVersion("3.2.0")
            .language("python")
            .mainResource(ResourceConfig.builder()
                .resourcePath("hdfs://scripts/ml_pipeline.py")
                .resourceType("PYTHON_FILE")
                .build())
            .sparkProperties("""
                --conf spark.executor.memory=16g
                --conf spark.executor.cores=4
                --conf spark.executor.instances=10
                --conf spark.pyspark.driver.python=/opt/python3.8/bin/python
                --conf spark.pyspark.python=/opt/python3.8/bin/python
                --conf spark.sql.execution.arrow.pyspark.enabled=true
                """)
            .programArgument("--model-path hdfs://models/ --data-path hdfs://data/")
            .isStreamingTask(false)
            .build();
    }
}
```

#### 3. **Spark SQLä»»åŠ¡é…ç½®åˆ†æ**

```java
// æ¥è‡ª SparkSQLConfig.java  
@Data
public class SparkSQLConfig {
    private String sparkVersion = StringConstant.DEFAULT_SPARK_VERSION_3;
    private String sparkProperties;
    private String programArgument;
    private List<ResourceConfig> sparkSqlResources;  // SQLè„šæœ¬èµ„æº
    private List<CampaignConfig> campaignConfigList;
    private Boolean enableCampaignConfig;
}

// Spark SQLä»»åŠ¡ç¤ºä¾‹
public class SparkSQLTaskExample {
    
    public SparkSQLConfig createReportingTaskConfig() {
        return SparkSQLConfig.builder()
            .sparkVersion("3.2.0")
            .sparkProperties("""
                --conf spark.executor.memory=12g
                --conf spark.sql.shuffle.partitions=800
                --conf spark.sql.adaptive.enabled=true
                --conf spark.sql.adaptive.skewJoin.enabled=true
                --conf spark.sql.adaptive.coalescePartitions.enabled=true
                """)
            .sparkSqlResources(Arrays.asList(
                ResourceConfig.builder()
                    .resourcePath("hdfs://sql/daily_report.sql")
                    .resourceType("SQL_FILE")
                    .build()
            ))
            .build();
    }
}
```

### 2.2 èµ„æºç®¡ç†ä¸é…ç½®

```java
// æ¥è‡ª ResourceConfig.java (æ¨æ–­)
@Data
public class ResourceConfig {
    private String resourceId;
    private String resourcePath;    // èµ„æºè·¯å¾„ï¼ˆHDFS/S3/æœ¬åœ°ï¼‰
    private String resourceType;    // èµ„æºç±»å‹ï¼ˆJAR/PYTHON/SQLç­‰ï¼‰
    private String resourceVersion; // èµ„æºç‰ˆæœ¬
    private Long resourceSize;      // èµ„æºå¤§å°
    private String checksum;        // æ ¡éªŒå’Œ
}

// èµ„æºç®¡ç†æœ€ä½³å®è·µ
public class ResourceManagement {
    
    // 1. ç‰ˆæœ¬åŒ–èµ„æºç®¡ç†
    public ResourceConfig createVersionedResource(String path, String version) {
        return ResourceConfig.builder()
            .resourcePath(path)
            .resourceVersion(version)
            .resourceType(detectResourceType(path))
            .checksum(calculateChecksum(path))
            .build();
    }
    
    // 2. ä¾èµ–è§£æ
    public List<ResourceConfig> resolveDependencies(ResourceConfig mainResource) {
        List<ResourceConfig> dependencies = new ArrayList<>();
        
        // è§£æJARåŒ…ä¾èµ–
        if ("JAR".equals(mainResource.getResourceType())) {
            dependencies.addAll(parseJarDependencies(mainResource.getResourcePath()));
        }
        
        // è§£æPythonåŒ…ä¾èµ–
        if ("PYTHON_FILE".equals(mainResource.getResourceType())) {
            dependencies.addAll(parsePythonDependencies(mainResource.getResourcePath()));
        }
        
        return dependencies;
    }
}
```

---

## 3. å·¥ä½œæµå¼•æ“å®ç°

### 3.1 å·¥ä½œæµæ‰§è¡Œæ ¸å¿ƒé€»è¾‘

```java
// æ¥è‡ª WorkflowExecutionServiceImpl.java çš„æ ¸å¿ƒé€»è¾‘åˆ†æ
@Service
public class WorkflowExecutionServiceImpl implements WorkflowExecutionService {
    
    @Override
    @Transactional(rollbackFor = Exception.class)
    public Long submit(Long workflowId, String operator, String operatorEmail, Set<Long> selectedIds) 
        throws Exception {
        
        // 1. å·¥ä½œæµéªŒè¯ä¸è·å–
        AssetDO assetDO = assetMapper.getById(workflowId);
        validateWorkflow(assetDO);
        
        // 2. è·å–æ‰€æœ‰ä»»åŠ¡èŠ‚ç‚¹
        List<AssetDO> allNodes = assetSearchService.getAllWorkflowTaskNodes(workflowId);
        filterExecutableNodes(allNodes, selectedIds);
        
        // 3. æ„å»ºæ‰§è¡Œè®¡åˆ’
        ExecutionDO executionDO = buildExecutionPlan(assetDO, operator, operatorEmail);
        
        // 4. åˆ›å»ºä»»åŠ¡å®ä¾‹
        List<TaskDO> taskDOS = buildTaskDOList(allNodes, executionDO, variableMap);
        
        // 5. æäº¤åˆå§‹ä»»åŠ¡ï¼ˆæ— ä¸Šæ¸¸ä¾èµ–çš„ä»»åŠ¡ï¼‰
        Set<Long> nodesWithoutUpStream = ExecutionUtils.getNodesWithoutUpStream(
            allIds, executionDO.getRelations());
        
        submitInitialTasks(taskDOS, nodesWithoutUpStream);
        
        // 6. æ›´æ–°æ‰§è¡ŒçŠ¶æ€
        executionService.updateExecutionStatus(executionDO.getId(), ExecutionStatusEnum.RUNNING);
        
        return executionDO.getId();
    }
    
    // ä»»åŠ¡é…ç½®æ„å»ºæ ¸å¿ƒé€»è¾‘
    private ExecutionInfo buildExecutionInfo(AssetDO assetDO, AssetDetailDO assetDetailDO, 
                                           Map<String, Object> variableMap) {
        ExecutionInfo executionInfo = new ExecutionInfo();
        AssetTypeEnum assetTypeEnum = AssetTypeEnum.getByCode(assetDO.getAssetType());
        
        switch (assetTypeEnum) {
            case SPARK_JAR:
                return buildSparkJarExecutionInfo(assetDetailDO, variableMap);
            case PYTHON_SPARK:
                return buildPySparkExecutionInfo(assetDetailDO, variableMap);
            case SPARK_SQL:
                return buildSparkSQLExecutionInfo(assetDetailDO, variableMap);
            default:
                throw new UnsupportedOperationException("Unsupported task type: " + assetTypeEnum);
        }
    }
    
    private ExecutionInfo buildSparkJarExecutionInfo(AssetDetailDO assetDetailDO, 
                                                   Map<String, Object> variableMap) {
        SparkJarConfig config = JsonUtils.parseObject(
            assetDetailDO.getSpecificConfiguration(), SparkJarConfig.class);
        
        ExecutionInfo executionInfo = new ExecutionInfo();
        
        // æ„å»ºSparké…ç½®
        String sparkConfig = Optional.ofNullable(config.getSparkProperties()).orElse("") 
                           + " --class " + config.getMainClass();
        executionInfo.setConfig(sparkConfig);
        
        // è®¾ç½®ç‰ˆæœ¬å’Œèµ„æº
        executionInfo.setSparkVersion(config.getSparkVersion());
        executionInfo.setMainResource(buildMainResource(config.getMainResource(), assetDO));
        executionInfo.setAttachedResources(buildAttachedResource(config.getOtherResources(), assetDO));
        
        // å¤„ç†ç¨‹åºå‚æ•°å’Œå˜é‡æ›¿æ¢
        if (StringUtils.isNotBlank(config.getProgramArgument())) {
            Map<String, Object> taskVariableMap = fillTaskVariables(variableMap, assetDetailDO, assetDO);
            executionInfo.setProgramArguments(
                StringReplaceUtils.replacedVariable(config.getProgramArgument(), taskVariableMap));
        }
        
        return executionInfo;
    }
}
```

### 3.2 ä»»åŠ¡ä¾èµ–å…³ç³»ç®¡ç†

```java
// å·¥ä½œæµä¾èµ–å…³ç³»ç®¡ç†
public class WorkflowDependencyManager {
    
    // 1. ä¾èµ–å…³ç³»è§£æ
    public Map<Long, Set<Long>> parseDependencies(String relationJson) {
        List<WorkflowLink> relations = JsonUtils.toList(relationJson, WorkflowLink.class);
        Map<Long, Set<Long>> dependencies = new HashMap<>();
        
        for (WorkflowLink link : relations) {
            Long targetId = link.getTo().getId();
            Long sourceId = link.getFrom().getId();
            
            dependencies.computeIfAbsent(targetId, k -> new HashSet<>()).add(sourceId);
        }
        
        return dependencies;
    }
    
    // 2. æ‹“æ‰‘æ’åºå®ç°
    public List<Long> topologicalSort(Set<Long> taskIds, Map<Long, Set<Long>> dependencies) {
        List<Long> result = new ArrayList<>();
        Set<Long> visited = new HashSet<>();
        Set<Long> visiting = new HashSet<>();
        
        for (Long taskId : taskIds) {
            if (!visited.contains(taskId)) {
                if (!dfs(taskId, dependencies, visited, visiting, result)) {
                    throw new RuntimeException("Circular dependency detected in workflow");
                }
            }
        }
        
        Collections.reverse(result);
        return result;
    }
    
    // 3. è·å–å¯æ‰§è¡Œä»»åŠ¡
    public Set<Long> getExecutableTasks(Set<Long> allTasks, Map<Long, Set<Long>> dependencies, 
                                       Set<Long> completedTasks) {
        return allTasks.stream()
            .filter(taskId -> !completedTasks.contains(taskId))
            .filter(taskId -> {
                Set<Long> deps = dependencies.getOrDefault(taskId, Collections.emptySet());
                return completedTasks.containsAll(deps);  // æ‰€æœ‰ä¾èµ–éƒ½å·²å®Œæˆ
            })
            .collect(Collectors.toSet());
    }
}
```

### 3.3 åŠ¨æ€è°ƒåº¦å®ç°

```mermaid
sequenceDiagram
    participant UI as Web UI
    participant WE as WorkflowEngine
    participant TS as TaskScheduler
    participant SE as SchedulerEngine
    participant Executor as SparkExecutor
    
    UI->>WE: æäº¤å·¥ä½œæµ
    WE->>WE: è§£æDAGä¾èµ–
    WE->>TS: æäº¤åˆå§‹ä»»åŠ¡
    
    loop ä»»åŠ¡æ‰§è¡Œå¾ªç¯
        TS->>SE: æäº¤ä»»åŠ¡åˆ°è°ƒåº¦å™¨
        SE->>Executor: å¯åŠ¨Sparkä»»åŠ¡
        Executor->>SE: ä»»åŠ¡æ‰§è¡Œå®Œæˆ
        SE->>TS: æ›´æ–°ä»»åŠ¡çŠ¶æ€
        TS->>WE: è§¦å‘ä¸‹æ¸¸ä»»åŠ¡
        WE->>WE: æ£€æŸ¥ä¾èµ–å…³ç³»
        WE->>TS: æäº¤æ–°çš„å¯æ‰§è¡Œä»»åŠ¡
    end
    
    TS->>UI: å·¥ä½œæµæ‰§è¡Œå®Œæˆ
```

---

## 4. ä»»åŠ¡è°ƒåº¦ä¸æ‰§è¡Œ

### 4.1 ä»»åŠ¡ç”Ÿå‘½å‘¨æœŸç®¡ç†

```java
// ä»»åŠ¡çŠ¶æ€ç®¡ç†
public class TaskLifecycleManager {
    
    public enum TaskStatus {
        PENDING,    // å¾…æ‰§è¡Œ
        RUNNING,    // æ‰§è¡Œä¸­
        SUCCESS,    // æˆåŠŸ
        FAILED,     // å¤±è´¥
        CANCELLED,  // å–æ¶ˆ
        RETRY       // é‡è¯•
    }
    
    // ä»»åŠ¡çŠ¶æ€è½¬æ¢
    public boolean transitionTaskState(Long taskId, TaskStatus fromStatus, TaskStatus toStatus) {
        TaskDO task = taskMapper.selectById(taskId);
        
        // éªŒè¯çŠ¶æ€è½¬æ¢çš„åˆæ³•æ€§
        if (!isValidTransition(task.getTaskStatus(), fromStatus, toStatus)) {
            throw new IllegalStateException(
                String.format("Invalid state transition: %s -> %s for task %d", 
                             fromStatus, toStatus, taskId));
        }
        
        // æ›´æ–°ä»»åŠ¡çŠ¶æ€
        TaskDO updateTask = new TaskDO();
        updateTask.setId(taskId);
        updateTask.setTaskStatus(toStatus.getCode());
        updateTask.setUpdateTime(System.currentTimeMillis());
        
        if (toStatus == TaskStatus.RUNNING) {
            updateTask.setStartTime(System.currentTimeMillis());
        } else if (toStatus == TaskStatus.SUCCESS || toStatus == TaskStatus.FAILED) {
            updateTask.setEndTime(System.currentTimeMillis());
        }
        
        int updated = taskMapper.updateByPrimaryKey(updateTask);
        
        // çŠ¶æ€è½¬æ¢æˆåŠŸåçš„åç»­å¤„ç†
        if (updated > 0) {
            handleStateTransition(taskId, toStatus);
        }
        
        return updated > 0;
    }
    
    // çŠ¶æ€è½¬æ¢åå¤„ç†
    private void handleStateTransition(Long taskId, TaskStatus newStatus) {
        switch (newStatus) {
            case SUCCESS:
                handleTaskSuccess(taskId);
                break;
            case FAILED:
                handleTaskFailure(taskId);
                break;
            case CANCELLED:
                handleTaskCancellation(taskId);
                break;
        }
    }
    
    private void handleTaskSuccess(Long taskId) {
        // 1. æ›´æ–°å·¥ä½œæµçŠ¶æ€
        updateWorkflowProgress(taskId);
        
        // 2. è§¦å‘ä¸‹æ¸¸ä»»åŠ¡
        triggerDownstreamTasks(taskId);
        
        // 3. æ¸…ç†ä¸´æ—¶èµ„æº
        cleanupTaskResources(taskId);
        
        // 4. å‘é€æˆåŠŸé€šçŸ¥
        sendSuccessNotification(taskId);
    }
}
```

### 4.2 é”™è¯¯å¤„ç†ä¸é‡è¯•æœºåˆ¶

```java
// ä»»åŠ¡é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
public class TaskErrorHandler {
    
    @Value("${task.retry.maxAttempts:3}")
    private int maxRetryAttempts;
    
    @Value("${task.retry.backoffMultiplier:2}")
    private int backoffMultiplier;
    
    public void handleTaskFailure(TaskDO task, Exception exception) {
        logError(String.format("Task %d failed: %s", task.getId(), exception.getMessage()), exception);
        
        // åˆ¤æ–­æ˜¯å¦å¯ä»¥é‡è¯•
        if (shouldRetry(task, exception)) {
            scheduleRetry(task);
        } else {
            markTaskAsFinalFailure(task, exception);
        }
    }
    
    private boolean shouldRetry(TaskDO task, Exception exception) {
        // 1. æ£€æŸ¥é‡è¯•æ¬¡æ•°
        if (task.getRetryCount() >= maxRetryAttempts) {
            return false;
        }
        
        // 2. æ£€æŸ¥å¼‚å¸¸ç±»å‹
        if (isNonRetryableException(exception)) {
            return false;
        }
        
        // 3. æ£€æŸ¥ä»»åŠ¡ç±»å‹
        if (isStreamingTask(task)) {
            return false;  // æµä»»åŠ¡é€šå¸¸ä¸é‡è¯•
        }
        
        return true;
    }
    
    private void scheduleRetry(TaskDO task) {
        int retryCount = task.getRetryCount() + 1;
        long delay = calculateRetryDelay(retryCount);
        
        // æ›´æ–°é‡è¯•ä¿¡æ¯
        TaskDO retryTask = new TaskDO();
        retryTask.setId(task.getId());
        retryTask.setRetryCount(retryCount);
        retryTask.setTaskStatus(TaskStatus.RETRY.getCode());
        retryTask.setNextRetryTime(System.currentTimeMillis() + delay);
        
        taskMapper.updateByPrimaryKey(retryTask);
        
        // è°ƒåº¦é‡è¯•
        scheduleTaskWithDelay(task.getId(), delay);
        
        logInfo(String.format("Task %d scheduled for retry %d after %d ms", 
                             task.getId(), retryCount, delay));
    }
    
    private long calculateRetryDelay(int retryCount) {
        // æŒ‡æ•°é€€é¿ç­–ç•¥ï¼š1min, 2min, 4min, 8min...
        long baseDelay = 60 * 1000;  // 1åˆ†é’Ÿ
        return baseDelay * (long) Math.pow(backoffMultiplier, retryCount - 1);
    }
    
    private boolean isNonRetryableException(Exception exception) {
        // ä»¥ä¸‹å¼‚å¸¸ç±»å‹ä¸è¿›è¡Œé‡è¯•
        return exception instanceof IllegalArgumentException ||
               exception instanceof SecurityException ||
               exception instanceof ClassNotFoundException ||
               exception.getMessage().contains("OutOfMemoryError");
    }
}
```

### 4.3 èµ„æºè°ƒåº¦ä¼˜åŒ–

```java
// èµ„æºè°ƒåº¦ä¼˜åŒ–
public class ResourceScheduler {
    
    // 1. æ™ºèƒ½èµ„æºåˆ†é…
    public ResourceAllocation allocateResources(TaskDO task, ClusterStatus clusterStatus) {
        AssetTypeEnum taskType = AssetTypeEnum.getByCode(task.getAssetType());
        
        switch (taskType) {
            case SPARK_JAR:
                return allocateSparkJarResources(task, clusterStatus);
            case PYTHON_SPARK:
                return allocatePySparkResources(task, clusterStatus);
            case SPARK_SQL:
                return allocateSparkSQLResources(task, clusterStatus);
            default:
                return getDefaultResourceAllocation();
        }
    }
    
    private ResourceAllocation allocateSparkJarResources(TaskDO task, ClusterStatus clusterStatus) {
        SparkJarConfig config = parseSparkJarConfig(task);
        
        // åŸºäºå†å²æ‰§è¡Œæ•°æ®é¢„ä¼°èµ„æºéœ€æ±‚
        TaskHistoryMetrics history = getTaskHistory(task.getFileId());
        
        int executorInstances = calculateExecutorInstances(
            config, history, clusterStatus.getAvailableCores());
        String executorMemory = calculateExecutorMemory(
            config, history, clusterStatus.getAvailableMemory());
        
        return ResourceAllocation.builder()
            .executorInstances(executorInstances)
            .executorMemory(executorMemory)
            .executorCores(getOptimalCoreCount(taskType))
            .queueName(selectOptimalQueue(task, clusterStatus))
            .priority(calculateTaskPriority(task))
            .build();
    }
    
    // 2. åŠ¨æ€èµ„æºè°ƒæ•´
    public void adjustResourcesBasedOnProgress(Long taskId) {
        TaskDO task = taskMapper.selectById(taskId);
        TaskMetrics currentMetrics = getTaskMetrics(taskId);
        
        // æ£€æŸ¥èµ„æºä½¿ç”¨æ•ˆç‡
        double memoryUtilization = currentMetrics.getMemoryUtilization();
        double cpuUtilization = currentMetrics.getCpuUtilization();
        
        ResourceAdjustment adjustment = new ResourceAdjustment();
        
        // å†…å­˜ä½¿ç”¨ç‡è¿‡ä½ï¼Œå‡å°‘å†…å­˜åˆ†é…
        if (memoryUtilization < 0.3 && task.getExecutionDuration() > 10 * 60 * 1000) {
            adjustment.setMemoryScale(0.7);  // å‡å°‘30%å†…å­˜
        }
        
        // CPUä½¿ç”¨ç‡è¿‡ä½ï¼Œå‡å°‘æ ¸å¿ƒæ•°
        if (cpuUtilization < 0.4 && task.getExecutionDuration() > 5 * 60 * 1000) {
            adjustment.setCoreScale(0.8);   // å‡å°‘20%æ ¸å¿ƒ
        }
        
        // åº”ç”¨èµ„æºè°ƒæ•´
        if (adjustment.hasChanges()) {
            applyResourceAdjustment(taskId, adjustment);
        }
    }
    
    // 3. é˜Ÿåˆ—è´Ÿè½½å‡è¡¡
    public String selectOptimalQueue(TaskDO task, ClusterStatus clusterStatus) {
        Map<String, QueueStatus> queueStatuses = clusterStatus.getQueueStatuses();
        
        // åŸºäºä»»åŠ¡ç±»å‹çš„é˜Ÿåˆ—äº²å’Œæ€§
        List<String> preferredQueues = getPreferredQueues(task.getAssetType());
        
        // é€‰æ‹©è´Ÿè½½æœ€è½»çš„é˜Ÿåˆ—
        return preferredQueues.stream()
            .filter(queueName -> queueStatuses.containsKey(queueName))
            .min(Comparator.comparingDouble(queueName -> 
                queueStatuses.get(queueName).getLoadFactor()))
            .orElse("default");
    }
}
```

---

## 5. æœ€ä½³å®è·µåº”ç”¨

### 5.1 é…ç½®æ¨¡æ¿åŒ–

åŸºäºé¡¹ç›®ä¸­çš„æ¨¡æ¿æœåŠ¡ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æœ€ä½³å®è·µçš„åº”ç”¨ï¼š

```java
// æ¥è‡ª PySparkTemplateTaskService.java
@Service
public class PySparkTemplateTaskService extends AbstractTemplateTaskService {
    
    @Override
    public void checkTemplateForSync(AssetDO template, AssetDetailDO templateDetail) throws Exception {
        super.checkTemplateForSync(template, templateDetail);
        
        // æ£€æŸ¥PySparké…ç½®çš„å®Œæ•´æ€§
        if (StringUtils.isEmpty(templateDetail.getSpecificConfiguration())) {
            throw new DataStudioException(DataStudioExceptionEnum.TASK_CONFIG_ILLEGAL, 
                "The template is missing Spark configuration. Please add it in saving.");
        }
        
        PythonSparkConfig pySparkConfig = JsonUtils.parseObject(
            templateDetail.getSpecificConfiguration(), PythonSparkConfig.class);
        
        // éªŒè¯å¿…éœ€çš„é…ç½®é¡¹
        if (StringUtils.isBlank(pySparkConfig.getSparkVersion()) || 
            StringUtils.isBlank(pySparkConfig.getLanguage()) || 
            Objects.isNull(pySparkConfig.getMainResource())) {
            throw new DataStudioException(DataStudioExceptionEnum.TASK_CONFIG_ILLEGAL, 
                "The template's Spark configuration is missing mandatory argument(s).");
        }
        
        // æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨
        checkResources(pySparkConfig.getMainResource(), pySparkConfig.getOtherResources());
        
        // æ£€æŸ¥æ¨¡æ¿å˜é‡
        checkPropertyWithVariable(template, templateDetail);
    }
    
    // é…ç½®æ¨¡æ¿æœ€ä½³å®è·µ
    public static class SparkConfigTemplates {
        
        // ETLä»»åŠ¡æ¨¡æ¿
        public static final String ETL_TEMPLATE = """
            {
              "sparkVersion": "3.2.0",
              "language": "python",
              "sparkProperties": "--conf spark.executor.memory=8g --conf spark.executor.cores=4 --conf spark.executor.instances=20 --conf spark.sql.adaptive.enabled=true --conf spark.sql.adaptive.coalescePartitions.enabled=true --conf spark.serializer=org.apache.spark.serializer.KryoSerializer",
              "isStreamingTask": false,
              "yarnQueueInfo": {
                "queueName": "etl",
                "priority": "NORMAL"
              }
            }
            """;
        
        // æœºå™¨å­¦ä¹ ä»»åŠ¡æ¨¡æ¿
        public static final String ML_TEMPLATE = """
            {
              "sparkVersion": "3.2.0",
              "language": "python",
              "sparkProperties": "--conf spark.executor.memory=16g --conf spark.executor.cores=4 --conf spark.executor.instances=10 --conf spark.sql.execution.arrow.pyspark.enabled=true --conf spark.python.worker.memory=2g",
              "isStreamingTask": false,
              "yarnQueueInfo": {
                "queueName": "ml",
                "priority": "HIGH"
              }
            }
            """;
        
        // æµå¤„ç†ä»»åŠ¡æ¨¡æ¿
        public static final String STREAMING_TEMPLATE = """
            {
              "sparkVersion": "3.2.0",
              "language": "python",
              "sparkProperties": "--conf spark.executor.memory=4g --conf spark.executor.cores=2 --conf spark.executor.instances=5 --conf spark.streaming.backpressure.enabled=true --conf spark.streaming.receiver.maxRate=10000",
              "isStreamingTask": true,
              "yarnQueueInfo": {
                "queueName": "streaming",
                "priority": "HIGH"
              }
            }
            """;
    }
}
```

### 5.2 å˜é‡æ›¿æ¢æœºåˆ¶

```java
// ä»»åŠ¡å˜é‡æ›¿æ¢æœºåˆ¶çš„å®ç°
public class VariableReplacementService {
    
    // å˜é‡å¡«å……é€»è¾‘
    public Map<String, Object> fillTaskVariables(
            Map<String, Object> workflowVariables,
            AssetDetailDO taskDetail,
            AssetDO taskAsset) {
        
        Map<String, Object> taskVariables = new HashMap<>(workflowVariables);
        
        // 1. ç³»ç»Ÿå˜é‡
        taskVariables.put("TASK_ID", taskAsset.getId());
        taskVariables.put("EXECUTION_DATE", DateUtils.formatDate(new Date(), "yyyy-MM-dd"));
        taskVariables.put("EXECUTION_TIMESTAMP", System.currentTimeMillis());
        taskVariables.put("USER", getCurrentUser());
        
        // 2. ç¯å¢ƒå˜é‡  
        taskVariables.put("ENV", getCurrentEnvironment());
        taskVariables.put("CLUSTER", getCurrentCluster());
        
        // 3. ä»»åŠ¡ç‰¹å®šå˜é‡
        if (taskDetail.getTaskVariables() != null) {
            Map<String, Object> taskSpecificVars = JsonUtils.parseObject(
                taskDetail.getTaskVariables(), new TypeReference<Map<String, Object>>() {});
            taskVariables.putAll(taskSpecificVars);
        }
        
        return taskVariables;
    }
    
    // å˜é‡æ›¿æ¢çš„å®é™…åº”ç”¨ç¤ºä¾‹
    public String replaceVariables(String template, Map<String, Object> variables) {
        String result = template;
        
        for (Map.Entry<String, Object> entry : variables.entrySet()) {
            String placeholder = "${" + entry.getKey() + "}";
            String value = String.valueOf(entry.getValue());
            result = result.replace(placeholder, value);
        }
        
        return result;
    }
    
    // å®é™…ä½¿ç”¨åœºæ™¯
    public void demonstrateVariableUsage() {
        // åŸå§‹é…ç½®æ¨¡æ¿
        String configTemplate = """
            --input hdfs://data/${ENV}/${EXECUTION_DATE}/
            --output hdfs://output/${ENV}/${TASK_ID}/
            --user ${USER}
            --batch-size ${BATCH_SIZE}
            """;
        
        Map<String, Object> variables = Map.of(
            "ENV", "production",
            "EXECUTION_DATE", "2023-12-01",
            "TASK_ID", "12345",
            "USER", "data_engineer",
            "BATCH_SIZE", "10000"
        );
        
        String finalConfig = replaceVariables(configTemplate, variables);
        // ç»“æœï¼š
        // --input hdfs://data/production/2023-12-01/
        // --output hdfs://output/production/12345/
        // --user data_engineer
        // --batch-size 10000
    }
}
```

### 5.3 ç›‘æ§ä¸æ—¥å¿—

```java
// ä»»åŠ¡ç›‘æ§å’Œæ—¥å¿—æ”¶é›†
public class TaskMonitoringService {
    
    // 1. ä»»åŠ¡æ‰§è¡ŒæŒ‡æ ‡æ”¶é›†
    public void collectTaskMetrics(Long taskId) {
        TaskDO task = taskMapper.selectById(taskId);
        
        if (TaskStatus.RUNNING.getCode().equals(task.getTaskStatus())) {
            TaskMetrics metrics = new TaskMetrics();
            
            // æ”¶é›†Sparkåº”ç”¨æŒ‡æ ‡
            if (task.getScheduleInstanceCode() != null) {
                SparkApplicationInfo sparkApp = getSparkApplicationInfo(task.getScheduleInstanceCode());
                
                metrics.setApplicationId(sparkApp.getApplicationId());
                metrics.setDriverMemoryUsage(sparkApp.getDriverMemoryUsage());
                metrics.setExecutorCount(sparkApp.getExecutorCount());
                metrics.setTotalCores(sparkApp.getTotalCores());
                metrics.setShuffleReadBytes(sparkApp.getShuffleReadBytes());
                metrics.setShuffleWriteBytes(sparkApp.getShuffleWriteBytes());
                metrics.setGcTime(sparkApp.getGcTime());
            }
            
            // å­˜å‚¨æŒ‡æ ‡
            taskMetricsMapper.insert(metrics);
            
            // æ£€æŸ¥å¼‚å¸¸æƒ…å†µ
            checkAnomalousMetrics(task, metrics);
        }
    }
    
    private void checkAnomalousMetrics(TaskDO task, TaskMetrics metrics) {
        List<Anomaly> anomalies = new ArrayList<>();
        
        // 1. æ£€æŸ¥å†…å­˜ä½¿ç”¨å¼‚å¸¸
        if (metrics.getDriverMemoryUsage() > 0.9) {
            anomalies.add(new Anomaly("HIGH_DRIVER_MEMORY", 
                "Driver memory usage > 90%: " + metrics.getDriverMemoryUsage()));
        }
        
        // 2. æ£€æŸ¥GCæ—¶é—´å¼‚å¸¸
        if (metrics.getGcTime() > task.getExecutionDuration() * 0.1) {
            anomalies.add(new Anomaly("HIGH_GC_TIME", 
                "GC time > 10% of execution time"));
        }
        
        // 3. æ£€æŸ¥Shuffleå¼‚å¸¸
        long shuffleTotal = metrics.getShuffleReadBytes() + metrics.getShuffleWriteBytes();
        if (shuffleTotal > 100L * 1024 * 1024 * 1024) {  // 100GB
            anomalies.add(new Anomaly("HIGH_SHUFFLE_VOLUME", 
                "Shuffle volume > 100GB: " + (shuffleTotal / 1024 / 1024 / 1024) + "GB"));
        }
        
        // å‘é€å‘Šè­¦
        if (!anomalies.isEmpty()) {
            sendAnomalyAlert(task, anomalies);
        }
    }
    
    // 2. æ—¥å¿—èšåˆå’Œåˆ†æ
    public void aggregateTaskLogs(Long taskId) {
        TaskDO task = taskMapper.selectById(taskId);
        
        // æ”¶é›†å„ç§æ—¥å¿—æº
        List<LogEntry> driverLogs = collectDriverLogs(task.getScheduleInstanceCode());
        List<LogEntry> executorLogs = collectExecutorLogs(task.getScheduleInstanceCode());
        List<LogEntry> yarnLogs = collectYarnLogs(task.getScheduleInstanceCode());
        
        // åˆå¹¶å’Œåˆ†ææ—¥å¿—
        List<LogEntry> allLogs = new ArrayList<>();
        allLogs.addAll(driverLogs);
        allLogs.addAll(executorLogs);
        allLogs.addAll(yarnLogs);
        
        // æ—¥å¿—åˆ†æ
        LogAnalysisResult analysis = analyzeLogEntries(allLogs);
        
        // å­˜å‚¨åˆ†æç»“æœ
        TaskLogAnalysis logAnalysis = new TaskLogAnalysis();
        logAnalysis.setTaskId(taskId);
        logAnalysis.setErrorCount(analysis.getErrorCount());
        logAnalysis.setWarningCount(analysis.getWarningCount());
        logAnalysis.setKeyErrors(JsonUtils.toJsonString(analysis.getKeyErrors()));
        logAnalysis.setPerformanceIssues(JsonUtils.toJsonString(analysis.getPerformanceIssues()));
        
        taskLogAnalysisMapper.insert(logAnalysis);
    }
}
```

---

## 6. æ¶æ„æ”¹è¿›å»ºè®®

åŸºäºå¯¹é¡¹ç›®ä»£ç çš„åˆ†æå’ŒSparkæœ€ä½³å®è·µï¼Œæå‡ºä»¥ä¸‹æ”¹è¿›å»ºè®®ï¼š

### 6.1 é…ç½®ç®¡ç†ä¼˜åŒ–

```java
// å»ºè®®ï¼šåŠ¨æ€é…ç½®ç®¡ç†
public class DynamicConfigurationManager {
    
    // 1. åŸºäºæœºå™¨å­¦ä¹ çš„èµ„æºé¢„æµ‹
    public ResourceRecommendation recommendResources(TaskProfile taskProfile) {
        MLModel resourceModel = loadResourcePredictionModel();
        
        PredictionInput input = PredictionInput.builder()
            .dataSize(taskProfile.getInputDataSize())
            .complexity(taskProfile.getComplexityScore())
            .historyMetrics(taskProfile.getHistoricalMetrics())
            .build();
        
        ResourceRecommendation recommendation = resourceModel.predict(input);
        
        return recommendation;
    }
    
    // 2. è‡ªé€‚åº”é…ç½®è°ƒæ•´
    public SparkConfig adaptConfiguration(SparkConfig baseConfig, TaskMetrics realTimeMetrics) {
        SparkConfig adaptedConfig = baseConfig.clone();
        
        // åŸºäºå®æ—¶æŒ‡æ ‡è°ƒæ•´é…ç½®
        if (realTimeMetrics.getMemoryPressure() > 0.8) {
            adaptedConfig.setExecutorMemory(
                increaseMemory(adaptedConfig.getExecutorMemory(), 1.2));
        }
        
        if (realTimeMetrics.getCpuUtilization() < 0.3) {
            adaptedConfig.setExecutorCores(
                Math.max(1, adaptedConfig.getExecutorCores() - 1));
        }
        
        return adaptedConfig;
    }
}
```

### 6.2 æ™ºèƒ½è°ƒåº¦ä¼˜åŒ–

```java
// å»ºè®®ï¼šæ™ºèƒ½ä»»åŠ¡è°ƒåº¦
public class IntelligentTaskScheduler {
    
    // 1. åŸºäºå†å²æ•°æ®çš„æ‰§è¡Œæ—¶é—´é¢„æµ‹
    public long predictExecutionTime(TaskDO task) {
        TaskHistoryAnalyzer analyzer = new TaskHistoryAnalyzer();
        
        List<TaskExecutionHistory> history = taskHistoryMapper.selectByTaskType(
            task.getAssetType(), task.getFileId());
        
        if (history.isEmpty()) {
            return getDefaultExecutionTime(task.getAssetType());
        }
        
        return analyzer.predictExecutionTime(history, task);
    }
    
    // 2. æ™ºèƒ½èµ„æºåˆ†é…
    public ResourceAllocation optimizeResourceAllocation(List<TaskDO> pendingTasks, 
                                                       ClusterStatus clusterStatus) {
        // ä½¿ç”¨æ•´æ•°çº¿æ€§è§„åˆ’æ±‚è§£æœ€ä¼˜èµ„æºåˆ†é…
        OptimizationModel model = createResourceOptimizationModel(pendingTasks, clusterStatus);
        OptimizationSolution solution = solver.solve(model);
        
        return solution.getResourceAllocation();
    }
    
    // 3. åŠ¨æ€ä¼˜å…ˆçº§è°ƒæ•´
    public void adjustTaskPriorities() {
        List<TaskDO> runningTasks = taskMapper.selectRunningTasks();
        
        for (TaskDO task : runningTasks) {
            TaskMetrics metrics = getTaskMetrics(task.getId());
            
            // åŸºäºSLAå’Œèµ„æºä½¿ç”¨æ•ˆç‡è°ƒæ•´ä¼˜å…ˆçº§
            int newPriority = calculateDynamicPriority(task, metrics);
            
            if (newPriority != task.getPriority()) {
                updateTaskPriority(task.getId(), newPriority);
            }
        }
    }
}
```

### 6.3 æ•…éšœé¢„æµ‹ä¸è‡ªæ„ˆ

```java
// å»ºè®®ï¼šæ•…éšœé¢„æµ‹å’Œè‡ªæ„ˆæœºåˆ¶
public class TaskFailurePredictionService {
    
    // 1. æ•…éšœé¢„æµ‹æ¨¡å‹
    public FailurePrediction predictFailure(Long taskId) {
        TaskDO task = taskMapper.selectById(taskId);
        TaskMetrics currentMetrics = getTaskMetrics(taskId);
        
        // ç‰¹å¾å·¥ç¨‹
        FailurePredictionFeatures features = extractFeatures(task, currentMetrics);
        
        // æ¨¡å‹é¢„æµ‹
        MLModel failureModel = loadFailurePredictionModel();
        double failureProbability = failureModel.predict(features);
        
        return FailurePrediction.builder()
            .taskId(taskId)
            .failureProbability(failureProbability)
            .riskLevel(classifyRiskLevel(failureProbability))
            .recommendations(generateRecommendations(features, failureProbability))
            .build();
    }
    
    // 2. è‡ªåŠ¨æ•…éšœä¿®å¤
    public void autoHealTask(Long taskId, FailureType failureType) {
        switch (failureType) {
            case OUT_OF_MEMORY:
                increaseTaskMemory(taskId, 1.5);
                restartTask(taskId);
                break;
                
            case DATA_SKEW:
                enableAdaptiveQueryExecution(taskId);
                adjustPartitionCount(taskId);
                restartTask(taskId);
                break;
                
            case EXECUTOR_LOST:
                increaseExecutorCount(taskId, 2);
                enableDynamicAllocation(taskId);
                restartTask(taskId);
                break;
                
            default:
                scheduleManualIntervention(taskId, failureType);
        }
    }
    
    // 3. é¢„é˜²æ€§ç»´æŠ¤
    public void performPreventiveMaintenance() {
        List<TaskDO> activeTasks = taskMapper.selectActiveTasks();
        
        for (TaskDO task : activeTasks) {
            FailurePrediction prediction = predictFailure(task.getId());
            
            if (prediction.getRiskLevel() == RiskLevel.HIGH) {
                // é¢„é˜²æ€§å¹²é¢„
                applyPreventiveMeasures(task, prediction.getRecommendations());
                
                // å‘é€é¢„è­¦
                sendPreventiveAlert(task, prediction);
            }
        }
    }
}
```

### 6.4 æ€§èƒ½æŒç»­ä¼˜åŒ–

```java
// å»ºè®®ï¼šæŒç»­æ€§èƒ½ä¼˜åŒ–æ¡†æ¶
public class ContinuousPerformanceOptimizer {
    
    // 1. æ€§èƒ½åŸºå‡†å»ºç«‹å’Œæ›´æ–°
    public void updatePerformanceBaseline() {
        List<TaskExecutionHistory> recentExecutions = 
            taskHistoryMapper.selectRecentExecutions(30); // æœ€è¿‘30å¤©
        
        Map<String, PerformanceBaseline> baselines = 
            recentExecutions.stream()
                .collect(groupingBy(
                    TaskExecutionHistory::getTaskType,
                    mapping(identity(), 
                           collectingAndThen(toList(), this::calculateBaseline))
                ));
        
        // æ›´æ–°åŸºå‡†æ•°æ®
        baselines.forEach(this::savePerformanceBaseline);
    }
    
    // 2. A/Bæµ‹è¯•æ¡†æ¶
    public void runConfigurationABTest(String configurationName, 
                                     SparkConfig configA, 
                                     SparkConfig configB) {
        ABTestExperiment experiment = ABTestExperiment.builder()
            .name(configurationName)
            .configurationA(configA)
            .configurationB(configB)
            .trafficSplitRatio(0.5)  // 50/50åˆ†æµ
            .duration(Duration.ofDays(7))  // è¿è¡Œ7å¤©
            .build();
        
        // å¯åŠ¨å®éªŒ
        abTestManager.startExperiment(experiment);
        
        // è‡ªåŠ¨æ”¶é›†æŒ‡æ ‡
        scheduleMetricsCollection(experiment);
        
        // å®éªŒç»“æŸåè‡ªåŠ¨åˆ†æ
        scheduleExperimentAnalysis(experiment);
    }
    
    // 3. è‡ªåŠ¨æ€§èƒ½è°ƒä¼˜
    public void autoTuneTaskConfiguration(Long taskId) {
        TaskDO task = taskMapper.selectById(taskId);
        TaskPerformanceHistory history = getTaskPerformanceHistory(taskId);
        
        // ä½¿ç”¨é—ä¼ ç®—æ³•æˆ–è´å¶æ–¯ä¼˜åŒ–
        ConfigurationOptimizer optimizer = new BayesianOptimizer();
        
        OptimizationResult result = optimizer.optimize(
            task.getCurrentConfiguration(),
            history.getPerformanceMetrics(),
            generateSearchSpace(task.getAssetType())
        );
        
        if (result.getImprovement() > 0.1) {  // 10%ä»¥ä¸Šæ€§èƒ½æå‡
            // åº”ç”¨ä¼˜åŒ–é…ç½®
            applyOptimizedConfiguration(taskId, result.getOptimalConfiguration());
            
            // è®°å½•ä¼˜åŒ–å†å²
            recordOptimizationResult(taskId, result);
        }
    }
}
```

---

## æ€»ç»“

é€šè¿‡å¯¹Data Studioé¡¹ç›®çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼š

### é¡¹ç›®äº®ç‚¹ï¼š
1. **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
2. **å¤šå¼•æ“æ”¯æŒ**ï¼šç»Ÿä¸€çš„æ¥å£æ”¯æŒSparkã€Prestoã€Flinkç­‰å¤šç§å¼•æ“
3. **å®Œæ•´çš„å·¥ä½œæµç®¡ç†**ï¼šDAGä¾èµ–è§£æã€åŠ¨æ€è°ƒåº¦ã€é”™è¯¯å¤„ç†
4. **æ¨¡æ¿åŒ–é…ç½®**ï¼šé€šè¿‡æ¨¡æ¿ç®€åŒ–ä»»åŠ¡é…ç½®ï¼Œæé«˜å¼€å‘æ•ˆç‡
5. **çµæ´»çš„å˜é‡æ›¿æ¢**ï¼šæ”¯æŒç¯å¢ƒå˜é‡ã€ä»»åŠ¡å˜é‡ç­‰åŠ¨æ€é…ç½®

### æ”¹è¿›å»ºè®®ï¼š
1. **æ™ºèƒ½åŒ–é…ç½®ç®¡ç†**ï¼šåŸºäºå†å²æ•°æ®å’Œæœºå™¨å­¦ä¹ çš„èµ„æºé¢„æµ‹
2. **æ•…éšœé¢„æµ‹ä¸è‡ªæ„ˆ**ï¼šä¸»åŠ¨çš„æ•…éšœæ£€æµ‹å’Œè‡ªåŠ¨ä¿®å¤æœºåˆ¶
3. **æŒç»­æ€§èƒ½ä¼˜åŒ–**ï¼šA/Bæµ‹è¯•å’Œè‡ªåŠ¨è°ƒä¼˜æ¡†æ¶
4. **æ›´ç²¾ç»†çš„èµ„æºè°ƒåº¦**ï¼šåŸºäºå®æ—¶è´Ÿè½½çš„åŠ¨æ€èµ„æºåˆ†é…
5. **å¢å¼ºçš„ç›‘æ§ä½“ç³»**ï¼šå®æ—¶æŒ‡æ ‡æ”¶é›†å’Œå¼‚å¸¸æ£€æµ‹

è¿™ä¸ªé¡¹ç›®ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå¾ˆå¥½çš„ä¼ä¸šçº§æ•°æ®å¹³å°å®ç°å‚è€ƒï¼Œç»“åˆSparkçš„ç†è®ºçŸ¥è¯†ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£å¦‚ä½•åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ„å»ºå’Œä¼˜åŒ–å¤§æ•°æ®å¤„ç†ç³»ç»Ÿã€‚

---

## å­¦ä¹ æ”¶è·

é€šè¿‡è¿™ä¸ƒä¸ªç« èŠ‚çš„æ·±åº¦å­¦ä¹ ï¼Œæˆ‘ä»¬å…¨é¢æŒæ¡äº†ï¼š

1. **ç†è®ºåŸºç¡€**ï¼šRDDã€DataFrameã€SQLä¼˜åŒ–å™¨çš„æ ¸å¿ƒåŸç†
2. **æ‰§è¡Œå¼•æ“**ï¼šDAGè°ƒåº¦ã€Stageåˆ’åˆ†ã€Taskæ‰§è¡Œæœºåˆ¶  
3. **ç³»ç»Ÿä¼˜åŒ–**ï¼šå†…å­˜ç®¡ç†ã€Shuffleä¼˜åŒ–ã€æ€§èƒ½è°ƒä¼˜
4. **æµå¤„ç†æŠ€æœ¯**ï¼šStreamingã€çŠ¶æ€ç®¡ç†ã€å®¹é”™æœºåˆ¶
5. **ç”Ÿäº§å®è·µ**ï¼šç›‘æ§ã€è°ƒä¼˜ã€æœ€ä½³å®è·µ
6. **å®é™…åº”ç”¨**ï¼šä¼ä¸šçº§å¹³å°çš„æ¶æ„è®¾è®¡å’Œå®ç°

è¿™äº›çŸ¥è¯†å½¢æˆäº†å®Œæ•´çš„SparkæŠ€æœ¯æ ˆå­¦ä¹ ä½“ç³»ï¼Œæ—¢æœ‰æ·±åº¦çš„ç†è®ºç†è§£ï¼Œåˆæœ‰ä¸°å¯Œçš„å®è·µç»éªŒï¼Œä¸ºå®é™…é¡¹ç›®å¼€å‘æä¾›äº†å¼ºæœ‰åŠ›çš„æ”¯æ’‘ã€‚

---

## æ¨èé˜…è¯»
- [Apache Sparkå®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/)
- ã€ŠSparkæƒå¨æŒ‡å—ã€‹
- ã€ŠLearning Spark 2.0ã€‹
- [Databricksåšå®¢æŠ€æœ¯æ–‡ç« ](https://databricks.com/blog)
```

```markdown
# Sparkæ·±åº¦å­¦ä¹ ç¬”è®° - å®Œæ•´å­¦ä¹ èµ„æ–™

æ¬¢è¿ä½¿ç”¨è¿™å¥—å…¨é¢æ·±å…¥çš„Apache Sparkå­¦ä¹ èµ„æ–™ï¼è¿™æ˜¯ä¸€ä¸ªä»ç†è®ºåˆ°å®è·µã€ä»åŸºç¡€åˆ°é«˜çº§çš„å®Œæ•´SparkæŠ€æœ¯æ ˆå­¦ä¹ ä½“ç³»ã€‚

## ğŸ“š å­¦ä¹ èµ„æ–™ç›®å½•

### [01. Sparkæ ¸å¿ƒç†è®ºä¸æ¶æ„](./spark-learning-notes-01-core-architecture.md)
- **å­¦ä¹ ç›®æ ‡**: æŒæ¡SparkåŸºç¡€æ¦‚å¿µå’Œæ•´ä½“æ¶æ„
- **æ ¸å¿ƒå†…å®¹**: 
  - Sparkè®¾è®¡æ€æƒ³ä¸å®šä½
  - Driver-Executoræ¶æ„è¯¦è§£
  - RDDæ ¸å¿ƒç†è®ºï¼ˆäº”å¤§ç‰¹æ€§ã€ä¾èµ–å…³ç³»ã€è¡€ç¼˜å®¹é”™ï¼‰
  - DataFrameä¸Dataseté«˜çº§æŠ½è±¡
  - å®é™…ä»£ç ç¤ºä¾‹åˆ†æ

### [02. Sparkæ‰§è¡Œå¼•æ“æ·±åº¦å‰–æ](./spark-learning-notes-02-execution-engine.md)
- **å­¦ä¹ ç›®æ ‡**: æ·±å…¥ç†è§£Sparkä½œä¸šæ‰§è¡Œæœºåˆ¶
- **æ ¸å¿ƒå†…å®¹**:
  - DAGè°ƒåº¦å™¨å·¥ä½œåŸç†
  - Stageåˆ’åˆ†ç®—æ³•ï¼ˆçª„ä¾èµ–vså®½ä¾èµ–ï¼‰
  - Taskè°ƒåº¦ä¸æ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–
  - äº‹ä»¶é©±åŠ¨è°ƒåº¦æœºåˆ¶
  - æ‰§è¡Œæµç¨‹å®ä¾‹åˆ†æ

### [03. Spark SQLä¸Catalystä¼˜åŒ–å™¨](./spark-learning-notes-03-catalyst-optimizer.md)
- **å­¦ä¹ ç›®æ ‡**: æŒæ¡Spark SQLå†…æ ¸å’ŒæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯
- **æ ¸å¿ƒå†…å®¹**:
  - SQLè§£æä¸åˆ†ææµç¨‹
  - Catalystä¼˜åŒ–å™¨æ¶æ„ï¼ˆåŸºäºè§„åˆ™çš„ä¼˜åŒ–ï¼‰
  - Tungstenæ‰§è¡Œå¼•æ“ï¼ˆä»£ç ç”Ÿæˆã€å‘é‡åŒ–ï¼‰
  - å¸¸ç”¨ä¼˜åŒ–è§„åˆ™è¯¦è§£
  - æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜å®è·µ

### [04. Sparkå†…å­˜ç®¡ç†ä¸Shuffleæœºåˆ¶](./spark-learning-notes-04-memory-shuffle.md)
- **å­¦ä¹ ç›®æ ‡**: ç²¾é€šSparkåº•å±‚å†…å­˜ç®¡ç†å’Œæ•°æ®ä¼ è¾“æœºåˆ¶
- **æ ¸å¿ƒå†…å®¹**:
  - ç»Ÿä¸€å†…å­˜ç®¡ç†æœºåˆ¶
  - åŠ¨æ€å†…å­˜å…±äº«ä¸é©±é€ç­–ç•¥
  - Shuffleæ¼”è¿›å†ç¨‹ï¼ˆHashâ†’Sortâ†’Unsafeï¼‰
  - BlockManagerå­˜å‚¨ç³»ç»Ÿ
  - å†…å­˜è°ƒä¼˜ä¸Shuffleä¼˜åŒ–

### [05. Spark Streamingä¸æµå¤„ç†](./spark-learning-notes-05-streaming.md)
- **å­¦ä¹ ç›®æ ‡**: æŒæ¡Sparkæµå¤„ç†æŠ€æœ¯å’Œå®æ—¶æ•°æ®å¤„ç†
- **æ ¸å¿ƒå†…å®¹**:
  - DStream API vs Structured Streaming
  - äº‹ä»¶æ—¶é—´å¤„ç†ä¸æ°´ä½çº¿æœºåˆ¶
  - æµæ‰¹ä¸€ä½“åŒ–æ¶æ„è®¾è®¡
  - çŠ¶æ€ç®¡ç†ä¸å®¹é”™æœºåˆ¶
  - æµå¤„ç†æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### [06. Sparkæ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µ](./spark-learning-notes-06-performance-tuning.md)
- **å­¦ä¹ ç›®æ ‡**: æŒæ¡ç”Ÿäº§ç¯å¢ƒSparkæ€§èƒ½ä¼˜åŒ–æŠ€èƒ½
- **æ ¸å¿ƒå†…å®¹**:
  - ç³»ç»ŸåŒ–æ€§èƒ½è°ƒä¼˜æ–¹æ³•è®º
  - èµ„æºé…ç½®ä¼˜åŒ–ï¼ˆCPUã€å†…å­˜ã€å­˜å‚¨ï¼‰
  - ä»£ç å±‚é¢ä¼˜åŒ–æŠ€å·§
  - æ•°æ®å€¾æ–œæ£€æµ‹ä¸è§£å†³æ–¹æ¡ˆ
  - ç¼“å­˜ç­–ç•¥ä¸ç›‘æ§å‘Šè­¦

### [07. é¡¹ç›®å®è·µï¼šData Studioå¹³å°åˆ†æ](./spark-learning-notes-07-project-analysis.md)
- **å­¦ä¹ ç›®æ ‡**: ç†è§£ä¼ä¸šçº§Sparkå¹³å°çš„å®é™…åº”ç”¨æ¨¡å¼
- **æ ¸å¿ƒå†…å®¹**:
  - Data Studioæ¶æ„åˆ†æ
  - å¤šç§Sparkä»»åŠ¡ç±»å‹é…ç½®
  - å·¥ä½œæµå¼•æ“å®ç°æœºåˆ¶
  - ä»»åŠ¡è°ƒåº¦ä¸ç”Ÿå‘½å‘¨æœŸç®¡ç†
  - æ¶æ„æ”¹è¿›å»ºè®®ä¸æœªæ¥æ–¹å‘

## ğŸ¯ å­¦ä¹ è·¯å¾„å»ºè®®

### åˆå­¦è€…è·¯å¾„ (2-3å‘¨)
1. **ç¬¬1å‘¨**: å­¦ä¹ ç¬”è®°01-02ï¼ŒæŒæ¡åŸºç¡€æ¦‚å¿µå’Œæ‰§è¡ŒåŸç†
2. **ç¬¬2å‘¨**: å­¦ä¹ ç¬”è®°03-04ï¼Œç†è§£SQLä¼˜åŒ–å’Œå†…å­˜ç®¡ç†
3. **ç¬¬3å‘¨**: å­¦ä¹ ç¬”è®°05ï¼Œäº†è§£æµå¤„ç†åŸºç¡€

### è¿›é˜¶è·¯å¾„ (4-6å‘¨)
1. **å‰3å‘¨**: æŒ‰é¡ºåºå­¦ä¹ ç¬”è®°01-05ï¼Œå»ºç«‹å®Œæ•´çŸ¥è¯†ä½“ç³»
2. **ç¬¬4å‘¨**: é‡ç‚¹å­¦ä¹ ç¬”è®°06ï¼ŒæŒæ¡æ€§èƒ½è°ƒä¼˜æŠ€èƒ½
3. **ç¬¬5-6å‘¨**: å­¦ä¹ ç¬”è®°07ï¼Œç»“åˆå®é™…é¡¹ç›®æ·±åŒ–ç†è§£

### ä¸“å®¶è·¯å¾„ (æŒç»­å­¦ä¹ )
1. **ç†è®ºæ·±åŒ–**: åå¤é˜…è¯»æ ¸å¿ƒç« èŠ‚ï¼Œæ·±å…¥ç†è§£åŸç†
2. **å®è·µåº”ç”¨**: ç»“åˆå®é™…é¡¹ç›®åº”ç”¨æ‰€å­¦çŸ¥è¯†
3. **æŠ€æœ¯è·Ÿè¸ª**: å…³æ³¨Sparkæ–°ç‰ˆæœ¬ç‰¹æ€§å’Œç¤¾åŒºå‘å±•

## ğŸ› ï¸ å®è·µå»ºè®®

### ç¯å¢ƒå‡†å¤‡
- **æœ¬åœ°å¼€å‘**: æ­å»ºå•æœºSparkç¯å¢ƒè¿›è¡Œå®éªŒ
- **é›†ç¾¤å®è·µ**: åœ¨Hadoop/K8sé›†ç¾¤ä¸Šéƒ¨ç½²Spark
- **äº‘å¹³å°**: ä½¿ç”¨Databricks/EMRç­‰æ‰˜ç®¡æœåŠ¡

### å®æˆ˜é¡¹ç›®
1. **æ‰¹å¤„ç†ETL**: å®ç°å¤§è§„æ¨¡æ•°æ®æ¸…æ´—å’Œè½¬æ¢
2. **å®æ—¶åˆ†æ**: æ„å»ºæµå¤„ç†æ•°æ®ç®¡é“
3. **æœºå™¨å­¦ä¹ **: ä½¿ç”¨Spark MLlibè¿›è¡Œå¤§è§„æ¨¡æœºå™¨å­¦ä¹ 
4. **æ•°æ®æ¹–**: ç»“åˆDelta Lake/Icebergæ„å»ºç°ä»£æ•°æ®æ¹–

## ğŸ“– å­¦ä¹ ç‰¹è‰²

### ç†è®ºä¸å®è·µå¹¶é‡
- **æ·±åº¦ç†è®º**: æ¯ä¸ªæ¦‚å¿µéƒ½æ·±å…¥åˆ°æºç å’Œå®ç°åŸç†
- **ä¸°å¯Œç¤ºä¾‹**: æä¾›å®Œæ•´å¯è¿è¡Œçš„ä»£ç ç¤ºä¾‹
- **å®é™…åº”ç”¨**: ç»“åˆçœŸå®é¡¹ç›®æ¡ˆä¾‹åˆ†æ

### ä½“ç³»åŒ–å­¦ä¹ 
- **å¾ªåºæ¸è¿›**: ä»åŸºç¡€åˆ°é«˜çº§ï¼Œé€»è¾‘æ¸…æ™°
- **ç›¸äº’å…³è”**: å„ç« èŠ‚å†…å®¹ç›¸äº’å‘¼åº”ï¼Œå½¢æˆå®Œæ•´ä½“ç³»
- **å®ç”¨å¯¼å‘**: å…³æ³¨ç”Ÿäº§ç¯å¢ƒå®é™…éœ€æ±‚

### å¯è§†åŒ–å›¾è¡¨
- **æ¶æ„å›¾**: Mermaidå›¾è¡¨æ¸…æ™°å±•ç¤ºç³»ç»Ÿæ¶æ„
- **æµç¨‹å›¾**: è¯¦ç»†æè¿°æ‰§è¡Œæµç¨‹å’Œæ•°æ®æµå‘
- **å¯¹æ¯”å›¾**: ç›´è§‚å¯¹æ¯”ä¸åŒæŠ€æœ¯æ–¹æ¡ˆ

## ğŸ“ å­¦ä¹ æˆæœ

å®Œæˆæœ¬å¥—å­¦ä¹ èµ„æ–™åï¼Œä½ å°†èƒ½å¤Ÿï¼š

1. **æ·±å…¥ç†è§£Sparkæ ¸å¿ƒåŸç†**ï¼Œå…·å¤‡è§£å†³å¤æ‚é—®é¢˜çš„èƒ½åŠ›
2. **ç†Ÿç»ƒè¿›è¡Œæ€§èƒ½è°ƒä¼˜**ï¼Œå°†Sparkåº”ç”¨æ€§èƒ½æå‡åˆ°ç”Ÿäº§çº§åˆ«
3. **è®¾è®¡å¤§è§„æ¨¡æ•°æ®å¤„ç†æ¶æ„**ï¼Œæ„å»ºä¼ä¸šçº§æ•°æ®å¹³å°
4. **å¤„ç†å„ç§å®é™…é—®é¢˜**ï¼ŒåŒ…æ‹¬æ•°æ®å€¾æ–œã€å†…å­˜ç®¡ç†ã€æ•…éšœæ’æŸ¥ç­‰
5. **è·Ÿä¸ŠæŠ€æœ¯å‘å±•è¶‹åŠ¿**ï¼Œå…·å¤‡æŒç»­å­¦ä¹ å’ŒæŠ€æœ¯æ¼”è¿›çš„èƒ½åŠ›

## ğŸ”— æ¨èèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Apache Sparkå®˜æ–¹æ–‡æ¡£](https://spark.apache.org/docs/latest/)
- [Spark SQLæŒ‡å—](https://spark.apache.org/docs/latest/sql-programming-guide.html)
- [Structured StreamingæŒ‡å—](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)

### ç»å…¸ä¹¦ç±
- ã€ŠSparkæƒå¨æŒ‡å—ã€‹- Matei Zahariaç­‰
- ã€ŠLearning Spark 2.0ã€‹- Jules S. Damjiç­‰
- ã€ŠSparkæ€§èƒ½ä¼˜åŒ–æŒ‡å—ã€‹- Holden Karauç­‰

### åœ¨çº¿èµ„æº
- [Databricksåšå®¢](https://databricks.com/blog) - æœ€æ–°æŠ€æœ¯æ–‡ç« å’Œæœ€ä½³å®è·µ
- [Spark Summitè§†é¢‘](https://databricks.com/sparkaisummit) - æŠ€æœ¯å¤§ä¼šæ¼”è®²
- [GitHub Sparkæºç ](https://github.com/apache/spark) - æ·±å…¥ç†è§£å®ç°åŸç†

## ğŸ’¡ å­¦ä¹ å»ºè®®

1. **åŠ¨æ‰‹å®è·µ**: ç†è®ºå­¦ä¹ å¿…é¡»ç»“åˆä»£ç å®è·µ
2. **æ·±å…¥æ€è€ƒ**: ä¸ä»…è¦çŸ¥é“"æ€ä¹ˆåš"ï¼Œæ›´è¦ç†è§£"ä¸ºä»€ä¹ˆè¿™ä¹ˆåš"
3. **é¡¹ç›®åº”ç”¨**: å°†æ‰€å­¦çŸ¥è¯†åº”ç”¨åˆ°å®é™…é¡¹ç›®ä¸­
4. **æŒç»­å­¦ä¹ **: å…³æ³¨Sparkç”Ÿæ€ç³»ç»Ÿçš„å‘å±•å’Œæ–°ç‰¹æ€§
5. **ç¤¾åŒºå‚ä¸**: åŠ å…¥Sparkç¤¾åŒºï¼Œä¸å…¶ä»–å¼€å‘è€…äº¤æµå­¦ä¹ 

---

**å¼€å§‹ä½ çš„Sparkæ·±åº¦å­¦ä¹ ä¹‹æ—…å§ï¼** ğŸš€

å¦‚æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿äº¤æµè®¨è®ºã€‚ç¥å­¦ä¹ æ„‰å¿«ï¼
```

## ğŸ‰ Sparkæ·±åº¦å­¦ä¹ ç¬”è®°åˆ›å»ºå®Œæˆï¼

æˆ‘å·²ç»ä¸ºä½ åˆ›å»ºäº†ä¸€å¥—å…¨é¢æ·±å…¥çš„Sparkå­¦ä¹ èµ„æ–™ï¼ŒåŒ…å«7ä¸ªæ ¸å¿ƒä¸»é¢˜çš„è¯¦ç»†å­¦ä¹ ç¬”è®°ï¼š

### ğŸ“‹ å·²å®Œæˆçš„å­¦ä¹ ç¬”è®°

1. **[Sparkæ ¸å¿ƒç†è®ºä¸æ¶æ„](docs/spark-learning-notes-01-core-architecture.md)** âœ…
   - RDDç†è®ºã€DataFrame/Datasetã€æ¶æ„è®¾è®¡

2. **[Sparkæ‰§è¡Œå¼•æ“è¯¦è§£](docs/spark-learning-notes-02-execution-engine.md)** âœ…  
   - DAGè°ƒåº¦ã€Stageåˆ’åˆ†ã€Taskæ‰§è¡Œæœºåˆ¶

3. **[Spark SQLä¸Catalystä¼˜åŒ–å™¨](docs/spark-learning-notes-03-catalyst-optimizer.md)** âœ…
   - æŸ¥è¯¢ä¼˜åŒ–ã€ä»£ç ç”Ÿæˆã€Tungstenå¼•æ“

4. **[Sparkå†…å­˜ç®¡ç†ä¸Shuffleæœºåˆ¶](docs/spark-learning-notes-04-memory-shuffle.md)** âœ…
   - ç»Ÿä¸€å†…å­˜ç®¡ç†ã€Shuffleä¼˜åŒ–ã€å­˜å‚¨ç³»ç»Ÿ

5. **[Spark Streamingä¸æµå¤„ç†](docs/spark-learning-notes-05-streaming.md)** âœ…
   - æµæ‰¹ä¸€ä½“åŒ–ã€çŠ¶æ€ç®¡ç†ã€å®¹é”™æœºåˆ¶

6. **[Sparkæ€§èƒ½è°ƒä¼˜ä¸æœ€ä½³å®è·µ](docs/spark-learning-notes-06-performance-tuning.md)** âœ…
   - æ€§èƒ½è¯Šæ–­ã€èµ„æºé…ç½®ã€ä»£ç ä¼˜åŒ–

7. **[é¡¹ç›®ä»£ç åˆ†æï¼šå®é™…åº”ç”¨æ¨¡å¼](docs/spark-learning-notes-07-project-analysis.md)** âœ…
   - ä¼ä¸šçº§å¹³å°æ¶æ„ã€å®é™…åº”ç”¨æ¡ˆä¾‹åˆ†æ

8. **[å­¦ä¹ èµ„æ–™ç›®å½•å¯¼èˆª](docs/README.md)** âœ…
   - å®Œæ•´å­¦ä¹ è·¯å¾„æŒ‡å¯¼

## ğŸŒŸ å­¦ä¹ èµ„æ–™ç‰¹è‰²

### ğŸ“– å†…å®¹å…¨é¢
- **ç†è®ºæ·±åº¦**: æ·±å…¥æºç çº§åˆ«çš„åŸç†è§£æ
- **å®è·µä¸°å¯Œ**: ç»“åˆä½ çš„é¡¹ç›®ä»£ç å®é™…åº”ç”¨
- **ä½“ç³»å®Œæ•´**: ä»åŸºç¡€æ¦‚å¿µåˆ°é«˜çº§ä¼˜åŒ–çš„å®Œæ•´çŸ¥è¯†ä½“ç³»

### ğŸ¯ å®ç”¨å¯¼å‘  
- **ç”Ÿäº§çº§å®è·µ**: åŸºäºçœŸå®ç”Ÿäº§ç¯å¢ƒçš„ç»éªŒæ€»ç»“
- **é—®é¢˜è§£å†³**: æ¶µç›–æ•°æ®å€¾æ–œã€å†…å­˜ç®¡ç†ç­‰å®é™…é—®é¢˜
- **æ€§èƒ½ä¼˜åŒ–**: ç³»ç»ŸåŒ–çš„è°ƒä¼˜æ–¹æ³•è®ºå’Œå·¥å…·

### ğŸ“Š å¯è§†åŒ–ä¸°å¯Œ
- **æ¶æ„å›¾è¡¨**: ä½¿ç”¨Mermaidåˆ›å»ºæ¸…æ™°çš„æ¶æ„å›¾
- **æµç¨‹å›¾**: è¯¦ç»†çš„æ‰§è¡Œæµç¨‹å’Œæ•°æ®æµå‘
- **ä»£ç ç¤ºä¾‹**: å®Œæ•´å¯è¿è¡Œçš„ç¤ºä¾‹ä»£ç 

## ğŸš€ å¦‚ä½•ä½¿ç”¨è¿™å¥—å­¦ä¹ èµ„æ–™

### åˆå­¦è€… (2-3å‘¨)
1. ä»**æ ¸å¿ƒæ¶æ„**å¼€å§‹ï¼Œå»ºç«‹æ•´ä½“æ¦‚å¿µ
2. å­¦ä¹ **æ‰§è¡Œå¼•æ“**ï¼Œç†è§£ä½œä¸šè¿è¡ŒåŸç†  
3. äº†è§£**æµå¤„ç†**åŸºç¡€ï¼ŒæŒæ¡ç°ä»£æ•°æ®å¤„ç†

### è¿›é˜¶è€… (4-6å‘¨)
1. æ·±å…¥**SQLä¼˜åŒ–**å’Œ**å†…å­˜ç®¡ç†**
2. æŒæ¡**æ€§èƒ½è°ƒä¼˜**çš„ç³»ç»Ÿæ–¹æ³•
3. åˆ†æ**é¡¹ç›®å®è·µ**ï¼Œç†è§£ä¼ä¸šçº§åº”ç”¨

### ä¸“å®¶çº§ (æŒç»­å­¦ä¹ )
- ä½œä¸ºå‚è€ƒæ‰‹å†Œï¼Œè§£å†³å…·ä½“æŠ€æœ¯é—®é¢˜
- è·Ÿè¸ªSparkæ–°ç‰¹æ€§ï¼ŒæŒç»­æ›´æ–°çŸ¥è¯†
- æŒ‡å¯¼å›¢é˜Ÿå­¦ä¹ å’ŒæŠ€æœ¯åˆ†äº«

è¿™å¥—å­¦ä¹ èµ„æ–™å°†å¸®åŠ©ä½ ä»Sparkåˆå­¦è€…æˆé•¿ä¸ºèƒ½å¤Ÿè§£å†³å¤æ‚ç”Ÿäº§é—®é¢˜çš„ä¸“å®¶ã€‚æ¯ä¸ªç¬”è®°éƒ½å¯ä»¥ç‹¬ç«‹å­¦ä¹ ï¼Œä¹Ÿå¯ä»¥ä½œä¸ºå®Œæ•´ä½“ç³»æ¥æŒæ¡ã€‚

å¼€å§‹ä½ çš„Sparkæ·±åº¦å­¦ä¹ ä¹‹æ—…å§ï¼ğŸ“

